{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"body\">\n",
    "<div class=\"main-title\">\n",
    "Reinforcement Learning Glossary\n",
    "</div>\n",
    "\n",
    "To start understanding RL, it's useful to define a few things...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"body\">\n",
    "<div class=\"title\">\n",
    "Agent\n",
    "</div>\n",
    "\n",
    "An agent is something can interact with its enviroment to affect it - is has *agency* over its environment. E.g. a robot, a character in a video game or a computer system controlling traffic light signals in a city.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"body\">\n",
    "<div class=\"title\">\n",
    "Environment\n",
    "</div>\n",
    "\n",
    "The environment is what the agent can interact with. For a robot that might be a real environment like a room full of objects. It may be a simulation or a digital enviroment. Or it could be something more abstract like a set of traffic lights.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"body\">\n",
    "<div class=\"title\">\n",
    "State\n",
    "</div>\n",
    "\n",
    "At any point in time, the environment has a ***state*** which defines everything in the environment (perhaps as well as the agent's own state). This could include everything from all object positions, to the temperature, to information about other agents in the environment.\n",
    "\n",
    "At time $t$, a state $s_t$ represented completely by $k$ variables is a k-vector;\n",
    "$s_t = \\begin{bmatrix}s^1_t \\\\ \\vdots \\\\ s^k_t\\end{bmatrix}$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"body\">\n",
    "<div class=\"title\">\n",
    "Observation\n",
    "</div>\n",
    "An agent may not be able to know (or even sense) the whole state. In this case, what it can directly observenis called an **observation**.\n",
    "\n",
    "E.g. We as humans do not know the state of our entire environment (the universe). We can only observe a small part of it at a time.\n",
    "\n",
    "E.g. A robot might be able to perform some task better if it could hear. But if it is not equipped with a microphone, then these useful sounds would be missing from its information about the state.\n",
    "\n",
    "In the case where the agent cannot know the complete state, but only an observation, the problem is known as **partially observable**.\n",
    "\n",
    "At time $t$, an observation $o_t$ represented by $k$ variables (where k is less than the number of variables that represent complete state) is a k-vector:\n",
    "\n",
    "$o_t = \\begin{bmatrix}o^1_t \\\\ \\vdots \\\\ o^k_t\\end{bmatrix}$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"body\">\n",
    "<div class=\"title\">\n",
    "Actions\n",
    "</div>\n",
    "\n",
    "Actions are things that the agent can choose to do to affect the environment. The set of all possible actions is called the **action space**.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"body\">\n",
    "<div class=\"title\">\n",
    "Policy\n",
    "</div>\n",
    "\n",
    "You probably know that in government, politicians have to follow certain *policies* that determine how they should respond to a situation. \n",
    "\n",
    "In RL, a **policy** means almost exactly the same thing: a policy defines what an agent should do when it finds itself in a certain state.\n",
    "\n",
    "Mathematically, a policy is a distribution over possible actions $a_t$, conditioned on the state $s_t$, at time $t$. We use the symbol $\\pi$ to represent the policy: $\\pi(a_t|s_t)$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"body\">\n",
    "<div class=\"title\">\n",
    "Reward\n",
    "</div>\n",
    "\n",
    "In RL, we train our agents by giving them *rewards* when they do the right thing. These rewards are not *actual* rewards like a bar of gold or a treat. Rather they are just what we try to make our agents get the maximum amount of by writing such code. Later we will discuss how to define and codify rewards for our agents.\n",
    "\n",
    "Reward for taking action $a_t$ in state $s_t$ at time $t$ is denoted as $r(s_t, a_t)$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"body\">\n",
    "<div class=\"title\">\n",
    "Trajectory\n",
    "</div>\n",
    "\n",
    "A list of states that the agent encounters and the corresponding actions that it takes and rewards it receives over one episode/lifetime/game.\n",
    "A trajectory until time T is denoted as $\\tau = (s_1, a_1, r_2 ... ,r_T, s_T, a_T)$ Note that action t corresponds to receiving reward t+1.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@import url('https://fonts.googleapis.com/css?family=Montserrat&display=swap');\n",
       "#notebook-container {\n",
       "\tbackground-color: #ff822e;\n",
       "}\n",
       ".CodeMirror {\n",
       "\tbackground-color: white;\n",
       "border-radius: 10px;\n",
       "}\n",
       ".body {\n",
       "\tbackground-color: black;\n",
       "\tcolor: #ff822e;\n",
       "\tpadding: 10px;\n",
       "\tborder-radius: 10px;\n",
       "\tfont-size: 17px;\n",
       "\tfont-family: montserrat;\n",
       "}\n",
       "\n",
       ".main-title {\n",
       "\tfont-size: 50px;\n",
       "\tfont-weight: 1000;\n",
       "\tline-height: 1.0;\n",
       "}\n",
       "\n",
       ".title {\n",
       "\tfont-size: 30px;\n",
       "\tline-height: 1.0;\n",
       "\tfont-weight: 1000;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IGNORE THIS IT'S JUST FOR STYLING\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"./style.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
