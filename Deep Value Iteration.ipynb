{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to RL \n",
    "\n",
    "## Problem setup: We want to make an AI that is able to complete a simple video game.\n",
    "\n",
    "### What is the game we are going to start with?\n",
    "In this game, we want our agent (character) to move through the 2D world and reach the goal. At each timestep our agent can to either move up, down, left or right. The agent cannot move into obstacles, and when it reaches the goal, the game ends.\n",
    "\n",
    "# insert video of game being played\n",
    "\n",
    "We are going to use an environment that we built, called Griddy, that works in exactly the same way as other environments provided as part of openAI gym. \n",
    "\n",
    "\n",
    "The main ideas are:\n",
    "<ul>\n",
    "<li>we need to create our environment</li>\n",
    "<li>we need to initialise it by calling `env.reset()`</li>\n",
    "<li>we can increment the simulation by one timestep by calling `env.step(action)`</li>\n",
    "</ul>\n",
    "\n",
    "Check out [openAI gym's docs](http://gym.openai.com/docs/) to see how the environments work in general and in more detail.\n",
    "\n",
    "Let's set up our simulation to train our agent in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from GriddyEnv import GriddyEnv # make sure you: pip3 install GriddyEnv\n",
    "\n",
    "# SET UP THE ENVIRONMENT\n",
    "env = GriddyEnv()    # create the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once we have an agent in the game, what do we do?\n",
    "\n",
    "Our agent has no idea of how to win the game. It simply observes states that change based on it's actions and receives a reward signal for doing so.\n",
    "So the agent has to learn about the game for itself. Just like a baby learns to interact with it's world by playing with it, our agent has to try random actions to figure out when and why it receives negative or positive rewards.\n",
    "\n",
    "A function which tells the agent what to do in a given state is called a **policy**\n",
    "\n",
    "We need our agent to understand what actions might lead it to achieving high rewards, but it doesn't know anything about how to complete the game yet. So let's set up our environment and implement a random policy that takes in a state and returns a random action for the agent to take.\n",
    "\n",
    "![](./images/policy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Space\n",
    "The action space consists of 4 unique actions: 0, 1, 2, 3<br>\n",
    "0 - Move left<br>\n",
    "1 - Move right<br>\n",
    "2 - Move up<br>\n",
    "3 - Move down<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation Space\n",
    "Has shape (3, 4, 4). Our grid world is 4x4<br>\n",
    "Each of the 3 channels is a binary mask for the location of different objects within the environment.<br>\n",
    "Channel 0 - Goal<br>\n",
    "Channel 1 - Wall<br>\n",
    "Channel 2 - Agent<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualise agent function\n",
    "def visualise_agent(policy, n=5):\n",
    "    try:\n",
    "        for trial_i in range(n):\n",
    "            observation = env.reset()\n",
    "            done=False\n",
    "            t=0\n",
    "            while not done:\n",
    "                env.render()\n",
    "                policy_action = policy(observation)\n",
    "                observation, reward, done, info = env.step(policy_action)\n",
    "                time.sleep(0.5)\n",
    "                t+=1\n",
    "            env.render()\n",
    "            time.sleep(1.5)\n",
    "            print(\"Episode {} finished after {} timesteps\".format(trial_i, t))\n",
    "        env.close()\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENT A RANDOM POLICY\n",
    "def random_policy(state):\n",
    "    #action = #fill this in\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## How do we know if we are doing well?\n",
    "\n",
    "When our agent takes this action and moves into a new state, the environment returns it a reward. The reward when it reaches the goal is +1, and 0 everywhere else. The reward that the agent receives at any point can be considered as what it feels in that moment - like pain or pleasure.\n",
    "\n",
    "**However**, the reward doesn't tell the agent how good that move actually was, only whether it sensed anything, and how good or bad that sensation was.\n",
    "\n",
    "E.g.\n",
    "- Our agent might not receive any reward for stepping toward the goal, even though this might be a good move.\n",
    "- A robot might receive a negative reward as it's battery depletes, but still make good progress towards its goal.\n",
    "- A chess playing agent might receive a positive reward for taking an opponent's piece, but make a bad move in doing so by exposing its king to an attack eventually causing it to lose the game.\n",
    "\n",
    "What we really want to know is not the instantaneous reward, but \"How good is the position I'm in right now?\", that is, what amount of reward can our agent get from this point onwards.\n",
    "This future reward is also known as the return.\n",
    "\n",
    "![](./images/undiscounted_return.png)\n",
    "\n",
    "#### Is getting a reward now as good as getting the same reward later?\n",
    "- What if the reward is removed from the game in the next timestep?\n",
    "- Would you rather be rich now or later?\n",
    "- What if a larger reward is introduced and you don't have enough energy to reach both?\n",
    "- What about inflation?\n",
    "\n",
    "It's better to get rewards sooner rather than later.\n",
    "\n",
    "![](./images/decay.png)\n",
    "\n",
    "We can encode this into our goal by using a **discount factor**, $\\gamma \\in [0, 1]$ ($\\gamma$ between 0 and 1). This makes our agent value more immediate rewards more than those which can be reached further in the future. This makes the goal become:\n",
    "\n",
    "![](./images/discounted_return.png)\n",
    "\n",
    "\n",
    "The value of these return values can be defined recursively as shown below.\n",
    "\n",
    "![](./images/recursive_return.png)\n",
    "\n",
    "Because of this, we can calculate the returns by having our agent play one run-through of the game and then *backing-up* through that trajectory, step-by-step, looking forward at what the future reward was from that point.\n",
    "\n",
    "The back up procedure is a way that we can determine the returns for each state that we visited in an episode. The return of a terminal state is always zero, but the terminal state is not the goal, it is the state which our agent transitions into once the episode has finished. We do backup by looking at the final timestep before our agent went into the terminal state - here it is easy to calculate the return. It is simply the reward that we received for moving into this state, because the expected return from the next state (terminal state) is always zero. Then using the recursive expression of returns (above), we can calculate the return for the timestep before that. This can be done recursively until we reach our initial state. At this point we know what all of the returns were for the whole episode.\n",
    "\n",
    "![](./images/backup.png)\n",
    "\n",
    "### So how good *is* each state?\n",
    "In general, the goal of reinforcement learning is to maximise the **expected** future reward. That is, to maximise the expected return from a the current state onwards. The measure of this, is called the *value* of the state (or the state value). A function that predicts this value is called a **state-value function** or **value function**.\n",
    "\n",
    "![](./images/value_def.png)\n",
    "\n",
    "If we had a way to estimate this, then we could look ahead to the state that each action would take us to and take the action which results in us landing in the state with best value. \n",
    "\n",
    "![](./images/follow_values.png)\n",
    "\n",
    "Values will not be updated for states that aren't visited to during an episode.\n",
    "\n",
    "If we initialise the value for each state as zero, and then average the return for each state over many episodes, that average return will converge to the true value of the state for this policy. This process of iteratively updating the value function is called **value iteration**.\n",
    "\n",
    "![](./images/update_values.png)\n",
    "\n",
    "Value iteration is a type of **value based** method. Notice that to learn an optimal policy, we never have to represent it explicitly. There is no function which represents the policy. Instead we just look-ahead and choose the action that maximises the value of the next state.\n",
    "\n",
    "Now that our agent is exploring the environment, let's implement value iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1\n",
    "i_episode=0\n",
    "discount_factor=0.8\n",
    "value_table = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_Gs(episode_mem, discount_factor=0.95):\n",
    "    return episode_mem\n",
    "\n",
    "def update_value_table(value_table, episode_mem, alpha=0.5):\n",
    "    return value_table, v_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(policy, n_episodes=100):\n",
    "    global epsilon\n",
    "    global value_table\n",
    "    global i_episode\n",
    "    try:\n",
    "        for _ in range(n_episodes):\n",
    "            observation = env.reset()\n",
    "            episode_mem = []\n",
    "            done=False\n",
    "            t=0\n",
    "            while not done:\n",
    "                env.render()\n",
    "                time.sleep(0.05)\n",
    "                action = policy(observation)\n",
    "                new_observation, reward, done, info = env.step(action)\n",
    "                episode_mem.append({'observation':observation,\n",
    "                                    'action':action,\n",
    "                                    'reward':reward,\n",
    "                                    'new_observation':new_observation,\n",
    "                                    'done':done})\n",
    "                observation=new_observation\n",
    "                t+=1\n",
    "                epsilon*=0.999\n",
    "            episode_mem = calculate_Gs(episode_mem, discount_factor)\n",
    "            value_table, v_delta = update_value_table(value_table, episode_mem)\n",
    "            i_episode+=1\n",
    "            print(\"Episode {} finished after {} timesteps. Eplislon={}. V_Delta={}\".format(i_episode, t, epsilon, v_delta))\n",
    "            env.render()\n",
    "            time.sleep(1)\n",
    "        env.close()\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(random_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we use the values that we know to perform well?\n",
    "\n",
    "Now that our agent is capable of exploring and learning about it's environment, we need to make it take advantage of what it knows so that it can perform well.\n",
    "Our random policy has helped us to estimate the values of each state, which means we have some idea of how good each state is. Think about how we could use this knowledge to make our agent perform well before reading the next paragraphs.\n",
    "\n",
    "In this simple version of the game, we know exactly what actions will lead us to what states. That means we have a perfect **model** of the environment. A model is a function that tells us how the state will change when we take certain actions. E.g. we know that if the agent tries to move up into an empty space, then that's where it will end up.\n",
    "\n",
    "Because we know exactly what states we can end up in by taking an action, we can just look at the value of the states and choose the action which leads us to the state with the greatest value. So we just move into the best state that we can reach at any point.\n",
    "A policy that always takes the action that it expects to end up in the best, currently reachable state is called a **greedy policy**.\n",
    "\n",
    "Let's implement a greedy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transition model\n",
    "def transition(state, action):\n",
    "    state = np.copy(state)\n",
    "    agent_pos = list(zip(*np.where(state[2] == 1)))[0]\n",
    "    new_agent_pos = np.array(agent_pos)\n",
    "    if action==0:\n",
    "        new_agent_pos[1]-=1\n",
    "    elif action==1:\n",
    "        new_agent_pos[1]+=1\n",
    "    elif action==2:\n",
    "        new_agent_pos[0]-=1\n",
    "    elif action==3:\n",
    "        new_agent_pos[0]+=1    \n",
    "    new_agent_pos = np.clip(new_agent_pos, 0, 3)\n",
    "\n",
    "    state[2, agent_pos[0], agent_pos[1]] = 0 #moved from this position so it is empty\n",
    "    \n",
    "    state[2, new_agent_pos[0], new_agent_pos[1]] = 1 #moved to this position\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### implement greedy policy\n",
    "#greedy policy\n",
    "def greedy_policy(state):\n",
    "    return policy_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why not just act greedily all the time?\n",
    "\n",
    "If we act greedily all the time then we will move into the state with the best value. But remember that these values are only estimates based on our agent's experience with the game, which means that they might not be correct. So if we want to make sure that our agent will do well by always choosing the next action greedily, we need to make sure that it has good estimates for the values of those states. This brings us to a core challenge in reinforcement learning: **the exploration vs exploitation dilemma**. Our agent can either exploit what it knows by using it's current knowledge to choose the best action, or it can explore more and improve it's knowledge perhaps learning that some actions are even worse than what it does currently.\n",
    "\n",
    "## An epsilon-greedy policy\n",
    "We can combine our random policy and our greedy policy to make an improved policy that both explores its environment and exploits its current knowledge. An $\\epsilon$-greedy (epsilon-greedy) policy is one which exploits what it knows most of the time, but with probability $\\epsilon$ will instead select a random action to try.\n",
    "\n",
    "## Do we need to keep exploring once we are confident in the values of states?\n",
    "\n",
    "As our agent explores more, it becomes more confident in predicting how valuable any state is. Once it knows a lot, it should start to explore less and exploit what it knows more. That means that we should decrease epsilon over time.\n",
    "\n",
    "Let's implement it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state):\n",
    "    epsilon = 0.05\n",
    "    if random.random() < epsilon:\n",
    "        return random_policy(state)\n",
    "    else:\n",
    "        return greedy_policy(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How can we find an optimal policy?\n",
    "\n",
    "An optimal policy would take the best possible action in any state. Because of this, the optimal value function would give the maximum possible values for any state.\n",
    "\n",
    "In the first line below, the maximum state-value of a state is equivalent to the maximum state-action value when taking the best action in that state. Following this, we can derive a recursive definition of the optimal value function.\n",
    "\n",
    "In the last step, we even remove the policy from the equation entirely! This means that value iteration never needs to explicitly represent a policy in terms of a function that takes in a state and returns a distribution over actions.\n",
    "Instead, value iteration uses a **model**, $p(s', r | s, a)$, to look one step ahead, and take the action, $a$, that most likely leads it to the next state that has the best state-value function.\n",
    "\n",
    "A **model** defines how the state changes. It is also known as the transition dynamics of the environment. In our case the model is really simple: we are certain that taking the action to move right will move our agent one space to the right as long as there are no obstacles. There is no randomness in our environment (e.g. no wind that might push us into a different cell when we try to move right). That is, our environment is deterministic, not stochastic.\n",
    "\n",
    "![](./images/bellman_op_v.png)\n",
    "\n",
    "![](./images/backup_v.png)\n",
    "\n",
    "![](./images/update_rule_v.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement value iteration to find optimal value function\n",
    "def update_value_table(episode_mem, value_table, discount_factor=0.95, alpha=0.5):\n",
    "    return value_table, v_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Code solution with visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gym\n",
    "from GriddyEnv import GriddyEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(3*4*4, 100)\n",
    "        self.fc2 = torch.nn.Linear(100, 100)\n",
    "        self.fc3 = torch.nn.Linear(100, 1)\n",
    "    def forward(self, obs):\n",
    "        obs = obs.view(-1, 3*4*4)\n",
    "        x = F.relu(self.fc1(obs))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    def create_optimizer(self, lr=0.001):\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the transition model aka our model of the environment. Given state and action it predicts next state\n",
    "def transition(state, action):\n",
    "    state = np.copy(state)\n",
    "    agent_pos = list(zip(*np.where(state[2] == 1)))[0]\n",
    "    new_agent_pos = np.array(agent_pos)\n",
    "    if action==0:\n",
    "        new_agent_pos[1]-=1\n",
    "    elif action==1:\n",
    "        new_agent_pos[1]+=1\n",
    "    elif action==2:\n",
    "        new_agent_pos[0]-=1\n",
    "    elif action==3:\n",
    "        new_agent_pos[0]+=1    \n",
    "    new_agent_pos = np.clip(new_agent_pos, 0, 3)\n",
    "\n",
    "    state[2, agent_pos[0], agent_pos[1]] = 0 #moved from this position so it is empty\n",
    "    state[2, new_agent_pos[0], new_agent_pos[1]] = 1 #moved to this position\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_greedy_policy(value_network):\n",
    "    def greedy_policy(state, return_action_vals=False):\n",
    "        action_values=[]\n",
    "        for test_action in range(4): #for each action\n",
    "            new_state = transition(state, test_action)\n",
    "            action_values.append(value_network(torch.tensor(new_state).double()).item())\n",
    "        policy_action = np.argmax(action_values)\n",
    "        if return_action_vals: return action_values\n",
    "        return policy_action\n",
    "    return greedy_policy\n",
    "\n",
    "def create_stochastic_policy(value_network):\n",
    "    def stochastic_policy(state, return_action_vals=False):\n",
    "        action_values=[]\n",
    "        for test_action in range(4): #for each action\n",
    "            new_state = transition(state, test_action)\n",
    "            action_values.append(value_network(torch.tensor(new_state).double()).item())\n",
    "        action_probs = F.softmax(torch.tensor(action_values), dim=-1)\n",
    "        policy_action = torch.distributions.Categorical(action_probs).sample().item()\n",
    "        if return_action_vals: return action_values\n",
    "        return policy_action\n",
    "    return stochastic_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_epsilon_greedy_policy(greedy_policy):\n",
    "    def epsilon_greedy_policy(state):\n",
    "        action = env.action_space.sample() if np.random.rand()<epsilon else greedy_policy(state)\n",
    "        return action\n",
    "    return epsilon_greedy_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_value_network(episode_mem, value_network, discount_factor=0.95, alpha=0.1):\n",
    "    all_diffs=[]\n",
    "    for i, mem in reversed(list(enumerate(episode_mem))): #start from terminal state\n",
    "        if i==len(episode_mem)-1: #if terminal state, G=reward\n",
    "            calculated_new_v = episode_mem[i]['reward']\n",
    "        else:\n",
    "            calculated_new_v = mem['reward']+(discount_factor*np.max(greedy_policy(mem['new_observation'], return_action_vals=True)))\n",
    "        predicted_old_v = value_network(torch.tensor(mem['new_observation']).double())\n",
    "        all_diffs.append(abs(calculated_new_v-predicted_old_v.item()))\n",
    "        label_new_v = predicted_old_v.item() + alpha*(calculated_new_v-predicted_old_v.item())\n",
    "        cost = F.mse_loss(predicted_old_v, torch.tensor([[label_new_v]]).double())\n",
    "        cost.backward()\n",
    "        value_network.optimizer.step()\n",
    "        value_network.optimizer.zero_grad()\n",
    "    return np.mean(all_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_table_viz(value_network, any_obs=None):\n",
    "    values = np.zeros((4, 4))\n",
    "    base_st = np.zeros((3, 4, 4), dtype=np.int64)\n",
    "    if np.all(any_obs==None):\n",
    "        base_st[0, 3, 3]=1\n",
    "    else:\n",
    "        base_st[0] = any_obs[0]\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            test_st = np.copy(base_st)\n",
    "            test_st[2, i, j] = 1\n",
    "            \n",
    "            val = value_network(torch.tensor(test_st).double()).item()\n",
    "            values[i, j] = val\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_agent(policy, value_network=None, n=5):\n",
    "    try:\n",
    "        for trial_i in range(n):\n",
    "            observation = env.reset(random_goal=True)\n",
    "            done=False\n",
    "            t=0\n",
    "            while not done:\n",
    "                if value_network: env.render(value_table_viz(value_network, observation))\n",
    "                else: env.render()\n",
    "                policy_action = policy(observation)\n",
    "                observation, reward, done, info = env.step(policy_action)\n",
    "                time.sleep(0.5)\n",
    "                t+=1\n",
    "            env.render()\n",
    "            time.sleep(1.5)\n",
    "            print(\"Episode {} finished after {} timesteps\".format(trial_i, t))\n",
    "        env.close()\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPER-PARAMS\n",
    "epsilon = 1\n",
    "i_episode=0\n",
    "discount_factor=0.9\n",
    "learning_rate=0.3\n",
    "\n",
    "env = GriddyEnv(4, 4)\n",
    "value_network = ValueNetwork().double()\n",
    "value_network.create_optimizer(0.001)\n",
    "greedy_policy = create_greedy_policy(value_network)\n",
    "stochastic_policy = create_stochastic_policy(value_network)\n",
    "epsilon_greedy_policy = create_epsilon_greedy_policy(stochastic_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(policy, n_episodes=100):\n",
    "    global epsilon\n",
    "    global value_network\n",
    "    global i_episode\n",
    "    try:\n",
    "        for _ in range(n_episodes):\n",
    "            observation = env.reset(True)\n",
    "            episode_mem = []\n",
    "            done=False\n",
    "            t=0\n",
    "            while not done:\n",
    "                env.render()\n",
    "                time.sleep(0.05)\n",
    "                action = policy(observation)\n",
    "                new_observation, reward, done, info = env.step(action)\n",
    "                episode_mem.append({'observation':observation,\n",
    "                                    'action':action,\n",
    "                                    'reward':reward,\n",
    "                                    'new_observation':new_observation,\n",
    "                                    'done':done})\n",
    "                observation=new_observation\n",
    "                t+=1\n",
    "            epsilon*=0.995\n",
    "            v_delta = update_value_network(episode_mem, value_network, discount_factor, learning_rate)\n",
    "            i_episode+=1\n",
    "            print(\"Episode {} finished after {} timesteps. Eplislon={}. V_Delta={}\".format(i_episode, t, epsilon, v_delta))#, end='\\r')\n",
    "            #print(value_table_viz(value_table))\n",
    "            #print()\n",
    "            env.render(value_table_viz(value_network, observation))\n",
    "            time.sleep(2)\n",
    "        env.close()\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished after 47 timesteps. Eplislon=0.9540649618417361. V_Delta=0.028973087408575415\n",
      "Episode 2 finished after 3 timesteps. Eplislon=0.9512056281970315. V_Delta=0.32335138908216127\n",
      "Episode 3 finished after 46 timesteps. Eplislon=0.9084203817511969. V_Delta=0.028031647602332\n",
      "Episode 4 finished after 3 timesteps. Eplislon=0.9056978449586682. V_Delta=0.28029125389121107\n",
      "Episode 5 finished after 67 timesteps. Eplislon=0.8469758853683546. V_Delta=0.023133618437263567\n",
      "Episode 6 finished after 24 timesteps. Eplislon=0.8268805241487632. V_Delta=0.04711592920882321\n",
      "Episode 7 finished after 3 timesteps. Eplislon=0.8244023623910088. V_Delta=0.23078495639099317\n",
      "Episode 8 finished after 8 timesteps. Eplislon=0.8178301806491574. V_Delta=0.09658891314478028\n",
      "Episode 9 finished after 14 timesteps. Eplislon=0.8064546837933355. V_Delta=0.05986019012335729\n",
      "Episode 10 finished after 38 timesteps. Eplislon=0.7763695993260584. V_Delta=0.03253713124161693\n",
      "Episode 11 finished after 5 timesteps. Eplislon=0.7724955072656065. V_Delta=0.13238402571323854\n",
      "Episode 12 finished after 12 timesteps. Eplislon=0.7632763763146613. V_Delta=0.0715987238091108\n",
      "Episode 13 finished after 8 timesteps. Eplislon=0.7571914943525904. V_Delta=0.06519292700947993\n",
      "Episode 14 finished after 6 timesteps. Eplislon=0.7526596881264136. V_Delta=0.10119376787188127\n",
      "Episode 15 finished after 11 timesteps. Eplislon=0.7444217038990517. V_Delta=0.05741884281301928\n",
      "Episode 16 finished after 108 timesteps. Eplislon=0.6681773581762521. V_Delta=0.027341898315783996\n",
      "Episode 17 finished after 3 timesteps. Eplislon=0.6661748299656206. V_Delta=0.11747167246897598\n",
      "Episode 18 finished after 92 timesteps. Eplislon=0.6075935243162931. V_Delta=0.032159693533426834\n",
      "Episode 19 finished after 325 timesteps. Eplislon=0.4389315614456469. V_Delta=0.02360250592612145\n",
      "Episode 20 finished after 3 timesteps. Eplislon=0.4376160831170627. V_Delta=0.13968846460320336\n",
      "Episode 21 finished after 36 timesteps. Eplislon=0.42213450329202495. V_Delta=0.036947479235474305\n",
      "Episode 22 finished after 17 timesteps. Eplislon=0.41501534097911913. V_Delta=0.06258387573090649\n",
      "Episode 23 finished after 10 timesteps. Eplislon=0.41088381354487985. V_Delta=0.08458860048137534\n",
      "Episode 24 finished after 294 timesteps. Eplislon=0.306177005589209. V_Delta=0.012679162749260713\n",
      "Episode 25 finished after 13 timesteps. Eplislon=0.3022204989748846. V_Delta=0.06651432670318144\n",
      "Episode 26 finished after 13 timesteps. Eplislon=0.29831511946776773. V_Delta=0.0435334943767243\n",
      "Episode 27 finished after 26 timesteps. Eplislon=0.2906551075963786. V_Delta=0.02679626524101502\n",
      "Episode 28 finished after 98 timesteps. Eplislon=0.26350921379006. V_Delta=0.030400495191540155\n",
      "Episode 29 finished after 299 timesteps. Eplislon=0.19537850619842073. V_Delta=0.02055497833371111\n",
      "Episode 30 finished after 3 timesteps. Eplislon=0.19479295661996557. V_Delta=0.21311836328311254\n",
      "Episode 31 finished after 70 timesteps. Eplislon=0.18161738795277457. V_Delta=0.04206858279615004\n",
      "Episode 32 finished after 84 timesteps. Eplislon=0.1669776852449974. V_Delta=0.03163650776524216\n",
      "Episode 33 finished after 19 timesteps. Eplislon=0.16383350125341073. V_Delta=0.0635080810577559\n",
      "Episode 34 finished after 66 timesteps. Eplislon=0.1533645326385287. V_Delta=0.03334993501295978\n",
      "Episode 35 finished after 47 timesteps. Eplislon=0.14631972697965345. V_Delta=0.03629125631841748\n",
      "Episode 36 finished after 60 timesteps. Eplislon=0.13779459277533324. V_Delta=0.03276800079906723\n",
      "Episode 37 finished after 120 timesteps. Eplislon=0.12220550295922675. V_Delta=0.019792609933342767\n",
      "Episode 38 finished after 7 timesteps. Eplislon=0.12135262648115634. V_Delta=0.138105445295092\n",
      "Episode 39 finished after 51 timesteps. Eplislon=0.11531587000406265. V_Delta=0.037117318962605644\n",
      "Episode 40 finished after 17 timesteps. Eplislon=0.11337110503126219. V_Delta=0.051971790152863226\n",
      "Episode 41 finished after 115 timesteps. Eplislon=0.10104935034078859. V_Delta=0.027665666764713097\n",
      "Episode 42 finished after 159 timesteps. Eplislon=0.08618787113701235. V_Delta=0.017944163172866002\n",
      "Episode 43 finished after 89 timesteps. Eplislon=0.07884508138038125. V_Delta=0.02425192954809954\n",
      "Episode 44 finished after 51 timesteps. Eplislon=0.07492288727950691. V_Delta=0.03359548713300221\n",
      "Episode 45 finished after 6 timesteps. Eplislon=0.0744744723018047. V_Delta=0.09513223112286408\n",
      "Episode 46 finished after 33 timesteps. Eplislon=0.07205573393440176. V_Delta=0.04435349069581597\n",
      "Episode 47 finished after 22 timesteps. Eplislon=0.07048704222174904. V_Delta=0.04693776172126982\n",
      "Episode 48 finished after 4 timesteps. Eplislon=0.07020551669323769. V_Delta=0.08013823156641064\n",
      "Episode 49 finished after 35 timesteps. Eplislon=0.06778964004958347. V_Delta=0.046662596637335185\n",
      "Episode 50 finished after 38 timesteps. Eplislon=0.06526072294130288. V_Delta=0.04481257176144527\n",
      "Episode 51 finished after 11 timesteps. Eplislon=0.06454643358219696. V_Delta=0.0874708527274492\n",
      "Episode 52 finished after 95 timesteps. Eplislon=0.058693989825242855. V_Delta=0.02900239460801023\n",
      "Episode 53 finished after 41 timesteps. Eplislon=0.05633504553635086. V_Delta=0.04631427324027045\n",
      "Episode 54 finished after 1 timesteps. Eplislon=0.05627871049081451. V_Delta=0.3126590721360233\n",
      "Episode 55 finished after 3 timesteps. Eplislon=0.056110043139194835. V_Delta=0.15078084520312063\n",
      "Episode 56 finished after 10 timesteps. Eplislon=0.05555146093830795. V_Delta=0.05763978828079201\n",
      "Episode 57 finished after 7 timesteps. Eplislon=0.0551637653500615. V_Delta=0.048869724628602444\n",
      "Episode 58 finished after 72 timesteps. Eplislon=0.05132973885076838. V_Delta=0.04465384905158481\n",
      "Episode 59 finished after 27 timesteps. Eplislon=0.04996170339735633. V_Delta=0.03045595145021356\n",
      "Episode 60 finished after 28 timesteps. Eplislon=0.04858149856964864. V_Delta=0.03587625155148643\n",
      "Episode 61 finished after 2 timesteps. Eplislon=0.048484384154007916. V_Delta=0.08215164506191408\n",
      "Episode 62 finished after 7 timesteps. Eplislon=0.048146009941739586. V_Delta=0.027700019818457742\n",
      "Episode 63 finished after 18 timesteps. Eplislon=0.047286708962080405. V_Delta=0.02402663196409187\n",
      "Episode 64 finished after 49 timesteps. Eplislon=0.045024408111757654. V_Delta=0.02507737959822651\n",
      "Episode 65 finished after 69 timesteps. Eplislon=0.042021030627625605. V_Delta=0.024289881109602684\n",
      "Episode 66 finished after 96 timesteps. Eplislon=0.03817276068432637. V_Delta=0.03441778170947524\n",
      "Episode 67 finished after 12 timesteps. Eplislon=0.03771719857917758. V_Delta=0.04856410290342387\n",
      "Episode 68 finished after 8 timesteps. Eplislon=0.037416514962579334. V_Delta=0.10183544653931872\n",
      "Episode 69 finished after 34 timesteps. Eplislon=0.03616512194318716. V_Delta=0.0341525425956709\n",
      "Episode 70 finished after 47 timesteps. Eplislon=0.03450387568672859. V_Delta=0.02782248912951978\n",
      "Episode 71 finished after 14 timesteps. Eplislon=0.03402394875486053. V_Delta=0.03736909361050904\n",
      "Episode 72 finished after 29 timesteps. Eplislon=0.03305094444473369. V_Delta=0.039649031839274214\n",
      "Episode 73 finished after 13 timesteps. Eplislon=0.03262385071163769. V_Delta=0.062299106510001215\n",
      "Episode 74 finished after 22 timesteps. Eplislon=0.03191361210255273. V_Delta=0.0421365646187937\n",
      "Episode 75 finished after 83 timesteps. Eplislon=0.02937050979584611. V_Delta=0.028118570356505834\n",
      "Episode 76 finished after 16 timesteps. Eplislon=0.028904089706128858. V_Delta=0.04787278106467708\n",
      "Episode 77 finished after 12 timesteps. Eplislon=0.028559141954960843. V_Delta=0.03575600925349779\n",
      "Episode 78 finished after 44 timesteps. Eplislon=0.027329182266073036. V_Delta=0.0278487624394302\n",
      "Episode 79 finished after 86 timesteps. Eplislon=0.025076020979810997. V_Delta=0.028686718658176416\n",
      "Episode 80 finished after 21 timesteps. Eplislon=0.02455465730210389. V_Delta=0.033622566237155066\n",
      "Episode 81 finished after 6 timesteps. Eplislon=0.024407697187425827. V_Delta=0.06328488557740776\n",
      "Episode 82 finished after 2 timesteps. Eplislon=0.02435890620074816. V_Delta=0.10529459295195198\n",
      "Episode 83 finished after 5 timesteps. Eplislon=0.024237355015339135. V_Delta=0.08972066307661586\n",
      "Episode 84 finished after 6 timesteps. Eplislon=0.02409229396118864. V_Delta=0.047962540289796275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 85 finished after 11 timesteps. Eplislon=0.023828599836494265. V_Delta=0.04837481084412207\n",
      "Episode 86 finished after 1 timesteps. Eplislon=0.023804771236657772. V_Delta=0.017234349616007005\n",
      "Episode 87 finished after 6 timesteps. Eplislon=0.02366229920506788. V_Delta=0.04730059640168649\n",
      "Episode 88 finished after 2 timesteps. Eplislon=0.02361499826895695. V_Delta=0.16172554721995713\n",
      "Episode 89 finished after 16 timesteps. Eplislon=0.02323997891492321. V_Delta=0.030573894485043523\n",
      "Episode 90 finished after 16 timesteps. Eplislon=0.022870915077561477. V_Delta=0.03249268044585116\n",
      "Episode 91 finished after 188 timesteps. Eplislon=0.018949394085873532. V_Delta=0.020438825696640628\n",
      "Episode 92 finished after 25 timesteps. Eplislon=0.018481300707052454. V_Delta=0.04700467124601612\n",
      "Episode 93 finished after 91 timesteps. Eplislon=0.016872986615415676. V_Delta=0.01682884346605989\n",
      "Episode 94 finished after 35 timesteps. Eplislon=0.016292361955234244. V_Delta=0.02936684724203369\n",
      "Episode 95 finished after 33 timesteps. Eplislon=0.015763228149531714. V_Delta=0.02762274818947501\n",
      "Episode 96 finished after 24 timesteps. Eplislon=0.015389229586970627. V_Delta=0.030382453477533055\n",
      "Episode 97 finished after 4 timesteps. Eplislon=0.015327764942458738. V_Delta=0.07651949748248862\n",
      "Episode 98 finished after 5 timesteps. Eplislon=0.015251279242194844. V_Delta=0.08660290488939189\n",
      "Episode 99 finished after 67 timesteps. Eplislon=0.014262445042858061. V_Delta=0.021327915044037974\n",
      "Episode 100 finished after 17 timesteps. Eplislon=0.014021913505049215. V_Delta=0.05106932577984107\n"
     ]
    }
   ],
   "source": [
    "train(epsilon_greedy_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current estimates of value\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.51701049, 0.57998881, 0.62474069, 0.67338781],\n",
       "       [0.58888181, 0.6550619 , 0.72840154, 0.80980877],\n",
       "       [0.65598463, 0.72898751, 0.80999937, 0.8999999 ],\n",
       "       [0.70749033, 0.80901724, 0.89982683, 0.99999997]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Current estimates of value')\n",
    "value_table_viz(value_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished after 4 timesteps\n",
      "Episode 1 finished after 3 timesteps\n",
      "Episode 2 finished after 1 timesteps\n"
     ]
    }
   ],
   "source": [
    "visualise_agent(greedy_policy, value_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook!\n",
    "\n",
    "Next you might want to check out:\n",
    "- [Policy Gradients]()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
