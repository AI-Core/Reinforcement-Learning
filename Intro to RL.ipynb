{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to RL \n",
    "\n",
    "## Problem setup: We want to make an AI that is able to complete a simple video game.\n",
    "\n",
    "### What is the game we are going to start with?\n",
    "In this game, we want our agent (character) to move through the 2D world and reach the goal. At each timestep our agent can to either move up, down, left or right. The agent cannot move into obstacles, and when it reaches the goal, the game ends.\n",
    "\n",
    "![](./images/griddy.gif)\n",
    "\n",
    "We are going to use an environment that we built, called Griddy, that works in exactly the same way as other environments provided as part of openAI gym. \n",
    "\n",
    "\n",
    "The main ideas are:\n",
    "<ul>\n",
    "<li>we need to create our environment</li>\n",
    "<li>we need to initialise it by calling `env.reset()`</li>\n",
    "<li>we can increment the simulation by one timestep by calling `env.step(action)`</li>\n",
    "</ul>\n",
    "\n",
    "Check out [openAI gym's docs](http://gym.openai.com/docs/) to see how the environments work in general and in more detail.\n",
    "\n",
    "Let's set up our simulation to train our agent in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-c5be1fa42853>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-c5be1fa42853>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    env =     # create the environment\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from GriddyEnv import GriddyEnv # make sure you: pip3 install GriddyEnv\n",
    "\n",
    "# SET UP THE ENVIRONMENT\n",
    "env =     # create the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once we have an agent in the game, what do we do?\n",
    "\n",
    "Our agent has no idea of how to win the game. \n",
    "It simply observes states, and takes actions.\n",
    "As a result of these actions, the agent will see the environment change to a new state and also receive some sensation. This sensation, which may be good or bad, is called a **reward**.\n",
    "and then  that change based on it's actions and receives a reward signal for doing so.\n",
    "\n",
    "This continuous interaction between the agent and our environment sets up the framework for a **reinforcement learning problem**, in an agent-environment loop as shown below.\n",
    "\n",
    "![](images/agent-env-loop.png)\n",
    "\n",
    "So without any prior knowledge, the agent has to learn about the game for itself. Just like a baby learns to interact with it's world by playing with it, our agent has to try random actions in the environment to figure out what causes it to receive negative or positive rewards.\n",
    "\n",
    "A function which tells the agent what actions to take from a given state is called a **policy**.\n",
    "\n",
    "![](./images/policy.png)\n",
    "\n",
    "Policies can be deterministic or stochastic (have randomness).\n",
    "\n",
    "Mathematically, a policy is a probability distribution over actions, conditioned on the state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our RL problem\n",
    "\n",
    "In our case:\n",
    "\n",
    "### Action Space\n",
    "The action space consists of 4 unique actions: 0, 1, 2, 3<br>\n",
    "0 - Move left<br>\n",
    "1 - Move right<br>\n",
    "2 - Move up<br>\n",
    "3 - Move down<br>\n",
    "\n",
    "### Observation Space\n",
    "Has shape (3, 4, 4). Our grid world is 4x4<br>\n",
    "Each of the 3 channels is a binary mask for the location of different objects within the environment.<br>\n",
    "Channel 0 - Goal<br>\n",
    "Channel 1 - Wall<br>\n",
    "Channel 2 - Agent<br>\n",
    "This is what our environment returns us on a state transition.\n",
    "\n",
    "I've simplified the code by including lines that convert this tensor state representation to an integer. This better represents the size of our environment and makes visualisation easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop our agent into the environment, implement a random policy, and then watch it act for a few episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualise agent function\n",
    "def visualise_agent(policy, n=5):\n",
    "    try:\n",
    "        for trial_i in range(n):\n",
    "            observation = env.reset()\n",
    "            done=False\n",
    "            t = 0\n",
    "            while not done:\n",
    "                env.render()\n",
    "                agent_pos = list(zip(*np.where(observation[2] == 1)))[0]\n",
    "                state = 4 * agent_pos[0] + agent_pos[1]\n",
    "                policy_action = policy(state)\n",
    "#                 print('policy_action:', policy_action)\n",
    "                observation, reward, done, info = env.step(policy_action)\n",
    "                time.sleep(0.1)\n",
    "                t += 1\n",
    "            env.render()\n",
    "            time.sleep(1.5)\n",
    "            print(\"Episode {} finished after {} timesteps\".format(trial_i, t))\n",
    "        env.close()\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENT A RANDOM POLICY\n",
    "def random_policy(state):\n",
    "    action = \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_agent(random_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we know if we are doing well?\n",
    "\n",
    "When our agent takes this action and moves into a new state, the environment returns it a reward. The reward when it reaches the goal is +1, and 0 everywhere else. The reward that the agent receives at any point can be considered as what it feels in that moment - like pain or pleasure.\n",
    "\n",
    "**However**, the reward doesn't tell the agent how good that move actually was, only whether it sensed anything, and how good or bad that sensation was at that particular moment.\n",
    "\n",
    "E.g.\n",
    "- Our agent might not receive any reward for stepping toward the goal, even though this might be a good move.\n",
    "- A robot might receive a negative reward as it's battery depletes, but still make good progress towards its goal.\n",
    "- A chess playing agent might receive a positive reward for taking an opponent's piece, but make a bad move in doing so by exposing its king to an attack eventually causing it to lose the game.\n",
    "\n",
    "What we really want to know is not the instantaneous reward, but \"How good is the position I'm in right now?\", that is, what amount of reward can our agent get from this point onwards.\n",
    "This future reward is also known as the return.\n",
    "\n",
    "![](./images/undiscounted_return.jpg)\n",
    "\n",
    "#### Is getting a reward now as good as getting the same reward later?\n",
    "- What if the reward is removed from the game in the next timestep?\n",
    "- Would you rather be rich now or later?\n",
    "- What if a larger reward is introduced and you don't have enough energy to reach both?\n",
    "- What about inflation?\n",
    "\n",
    "It's better to get rewards sooner rather than later.\n",
    "\n",
    "![](./images/decay.jpg)\n",
    "\n",
    "We can encode this into our goal by using a **discount factor**, $\\gamma \\in [0, 1]$ ($\\gamma$ between 0 and 1). \n",
    "The discount factor is the coefficient of a reward $t$ timesteps in the future, raised to the power of $t$. Because it is less than 1, raising to the power reduces its value. As such, this coefficient weights rewards further away in the future by a lesser number than those nearby in time.\n",
    "\n",
    "This makes our agent value more immediate rewards more than those which can be reached further in the future.\n",
    "This makes the goal become:\n",
    "\n",
    "![](./images/discounted_return.png)\n",
    "\n",
    "This is called the **discounted return**. From here on, when we say return, we mean discounted return unless otherwise specified, because we rarely use the undiscounted version.\n",
    "\n",
    "Once an agent has played a whole game it can calculate the return for each state it visited by simply adding up the discounted reward it achieved from there on.\n",
    "\n",
    "The value of these return values can also be defined recursively as shown below.\n",
    "\n",
    "![](./images/recursive_return.png)\n",
    " \n",
    "Because of this recursive relationship, we can calculate the experienced returns for each visited state in a single pass through the trajectory of that episode. We do this by working backwards, firstly calculating the  return from the terminal state (always zero), and then recursively calculating the return from the previous timestep by discounting it and adding the reward from that timestep.\n",
    "This process is called ***backup***.\n",
    "\n",
    "The return of a terminal state is always zero, because from there the episode will have terminated and hence the agent will not be able to attain any further reward.\n",
    "\n",
    "![](./images/backup.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value functions - so how good is each state?\n",
    "\n",
    "Now we know how to calculate the return that we experienced during any single episode. \n",
    "But this is just a single sample estimate. \n",
    "In general, the goal of reinforcement learning is to maximise the **expected** future reward. \n",
    "That is, to maximise the average return from a the current state onwards. \n",
    "This quantity is defined as the **state-value** of a state, commonly just referred to as the  or **value** of a state. \n",
    "A function that returns this value given a state is called a value function.\n",
    "\n",
    "![](./images/value_def.jpg)\n",
    "\n",
    "**Note that a value function must correspond to some policy.** If we follow a bad policy then states will have lower values than if we follow a good policy. If we change the policy, then the value function will change\n",
    "\n",
    "### The Bellman equation for $V(S)$\n",
    "\n",
    "Using the recursive definition of the return, we can express the value function recursively.\n",
    "\n",
    "The Bellman equations are those which express value functions recursively.\n",
    "\n",
    "![](./images/value-bellman.jpg)\n",
    "\n",
    "#### Recovering the Bellman optimality question for $V(S)$\n",
    "\n",
    "![](./images/value-optim-deriv.jpg)\n",
    "\n",
    "\n",
    "### The Bellman Optimality equation for $V(S)$\n",
    "\n",
    "For a value function to be better than some other, it must have values greater than or equal to those for the other value function for all states.\n",
    "\n",
    "If we are acting optimally at any given state, then the action we take should consider all actions and then take the one with the best expected return. This return is calculated using the value for the next state.\n",
    "\n",
    "![](./images/value-optim.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we use these tools, and improve our agent's performance?\n",
    "\n",
    "### Method 1: Dynamic Programming (DP) methods for computing value functions and improving policies\n",
    "\n",
    "The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect **model** of the environment.\n",
    "\n",
    "A model tells us how the environment will change when we take certain actions. It may be stochastic or deterministic. A model can allow us to **simulate** the progression of the environment, taking any action from any state.\n",
    "\n",
    "![](./images/model.jpg)\n",
    "\n",
    "**A model will also be referred to as a transition function.**\n",
    "\n",
    "Fortunately (by our design) in this simple version of the game, we do know exactly what actions will lead us to what states. That means we have a perfect **model** of the environment. A model is a function that tells us how the state will change when we take certain actions. E.g. we know that if the agent tries to move up into an empty space, then that's where it will end up.\n",
    "\n",
    "Let's run the cell below to define our transition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSITION FUNCTION/ENVIRONMENT MODEL\n",
    "def transition(state, action):\n",
    "    LEFT = 0\n",
    "    DOWN = 1\n",
    "    RIGHT = 2\n",
    "    UP = 3\n",
    "    \n",
    "    GOAL = 15\n",
    "    nrow = 4\n",
    "    ncol = 4\n",
    "    \n",
    "    row = state // 4 # nearest factor of nrows\n",
    "    col = state % 4 # remainder of ncols\n",
    "#     print('state:', state, 'row:', row, 'col:', col)\n",
    "    if action == LEFT:\n",
    "        state = (row, max(col-1, 0))\n",
    "    if action == DOWN:\n",
    "        state = (min(row+1, nrow - 1), col)\n",
    "    if action == RIGHT:\n",
    "        state = (row, min(col+1, ncol - 1))\n",
    "    if action == UP:\n",
    "        state = (max(row-1, 0), col)\n",
    "    state = nrow * state[0] + state[1] # convert back to integer\n",
    "    reward = 0\n",
    "    if state == GOAL:\n",
    "        reward = 1\n",
    "    return state, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a model, we can look ahead to the successor states reachable from our current state.\n",
    "If we also had a way to estimate the value function, then we could look ahead to the state that each action would take us to and take the action which results in the best expected return.\n",
    "\n",
    "Acting greedily with respect to a value function for an optimal policy will produce optimal behaviour.\n",
    "\n",
    "![](./images/follow_values.png)\n",
    "\n",
    "#### Computing value functions using dynamic programming\n",
    "## Algorithm 1: Policy iteration\n",
    "\n",
    "### Step 1: Policy evaluation step\n",
    "\n",
    "Policy evaluation is the process of approximately evaluating the value function for our current policy.\n",
    "\n",
    "How can we do this using our environment model?\n",
    "\n",
    "The Bellman equations define the value of any state recursively, as a function of it successor state.\n",
    "We know that we can use our model to simulate the next states that our agent will move into by taking any given action, and that it defines what rewards we might receive.\n",
    "Given this, along with the fact that our value table already contains estimates\n",
    "\n",
    "# policy eval\n",
    "![](./images/policy-evaluation.jpg)\n",
    "\n",
    "### Step 2: Policy improvement step\n",
    "\n",
    "Policy improvement is done by setting the new policy to be greedy with respect to the value function. This means that the new policy will consider all actions,and then take the action that leads to the greatest expected return, based on our current value function.\n",
    "\n",
    "# policy improvement\n",
    "![](./images/policy-improvement.jpg)\n",
    "\n",
    "### Full policy iteration algorithm\n",
    "\n",
    "The full policy iteration algorithm iterates between policy evaluation and policy improvement. This alternatively improves the policy by making it greedy with respect to the value function, and then improves the value function by minimising the Bellman error.\n",
    "\n",
    "Let's put these two steps together to produce the full policy iteration algorithm.\n",
    "\n",
    "![](./images/policy-iteration.jpg)\n",
    "\n",
    "### Does this converge to an optimal policy?\n",
    "\n",
    "For a policy $\\pi'$ to better than some other policy $\\pi$?, it must be such that $v_{\\pi'}(s) \\geq v_{\\pi}(s)$ for all states.\n",
    "\n",
    "![](./images/convergence.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_value_table(num_states=16):\n",
    "    value_table = {} # start off with empty map\n",
    "    for s in range(num_states): # for each state\n",
    "        value_table[s] = np.random.rand()\n",
    "#         value_table[s] = 0\n",
    "    return value_table\n",
    "\n",
    "def initialise_deterministic_policy(num_states=16, num_actions=4):\n",
    "    policy = {} # start off with empty map\n",
    "    for s in range(num_states):\n",
    "        action_probs = # initialise probabilities randomly\n",
    "        action_probs /= # normalise probs\n",
    "        policy[s] =\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTING VALUE FUNCTION USING DYNAMIC PROGRAMMING\n",
    "def policy_evaluation(policy, value_table, discount_factor, error_threshold=0.01, num_states=16):\n",
    "    print()\n",
    "    print(value_table)\n",
    "    new_value_table = {} # init new value table to be filled in and returned\n",
    "    converged = False # initially we have not found a converged value function for this policy\n",
    "    k = 0 # sweep index\n",
    "    while not converged: # until the value function converges\n",
    "        print('sweep ', k)\n",
    "        k += 1 # increment sweep counter\n",
    "        \n",
    "        worst_delta = 0 # difference between previous values and iterated values\n",
    "        for state in range(num_states): # loop over each state\n",
    "            action =  # get the action according to current policy\n",
    "            new_state, reward =  # use model to simulate next state and reward\n",
    "            new_val =  # compute new value\n",
    "#             print(new_val)\n",
    "#             print(value_table[state])\n",
    "            new_value_table[state] =  # update value table\n",
    "            delta =  # find the absolute diff between new val and old val for this state\n",
    "#             print('delta:', delta)\n",
    "\n",
    "            # CHECK WORST VALUE FUNCTION ERROR\n",
    "            if delta > worst_delta: # is this state the one for which our value table is most wrong?\n",
    "                worst_delta = delta # update worst error for this sweep\n",
    "                print('worst_delta:', worst_delta)\n",
    "                \n",
    "        # CHECK CONVERGED\n",
    "        if worst_delta < error_threshold: # once the values stop changing\n",
    "            converged = # we have found the value function\n",
    "            print('Converged on value function')\n",
    "        value_table =  # update value table (took me ages to realise i was missing this line and debug )\n",
    "    return new_value_table # return converged value table evaluated for this policy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVE POLICY\n",
    "def policy_improvement(value_table, discount_factor): # set a greedy policy which will always be better than the previous\n",
    "    new_policy = {} # initialise empty new policy to be filled and returned\n",
    "    action_space = range(4) #  \n",
    "    \n",
    "    for  # loop over each state\n",
    "        best_value =  # initialise best value as negative infinity\n",
    "        best_action =  # no best action found yet\n",
    "        for : # set the policy as greedy with respect to the value function\n",
    "            new_state, reward = # use model to simulate next state and reward\n",
    "            value =  # update value of state\n",
    "            \n",
    "            # CHECK MAX VALUE\n",
    "            if value > best_value: # checking all actions, which gives this state the best value?\n",
    "                best_value = value # update best value\n",
    "                best_action = action # update best action found so far\n",
    "                \n",
    "        # SET GREEDY POLICY\n",
    "        new_policy[state] =  # update new policy to take best action found when it sees this state\n",
    "    return new_policy\n",
    "\n",
    "def check_stable_policy(old_policy, new_policy):\n",
    "    stable = True\n",
    "    # CHECK STABLE POLICY\n",
    "    return stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# POLICY ITERATION ALGORITHM\n",
    "def policy_iteration(discount_factor=0.9):\n",
    "    value_table = # initiailise value table\n",
    "    policy = # initialise deterministic policy\n",
    "    policy_stable = # initialise false\n",
    "    policy_idx = 0 # outer to check how many policies we've tried\n",
    "    while not policy_stable: # until convergence\n",
    "        \n",
    "        # POLICY ITERATION\n",
    "        print('Evaluating policy ', policy_idx)\n",
    "        value_table =  # converge on value function\n",
    "        \n",
    "        # POLICY IMPROVEMENT\n",
    "        print('Iterating policy ', policy_idx)\n",
    "        new_policy =  # get greedy policy using converged value function\n",
    "        \n",
    "        # CHECK CONVERGENCE\n",
    "        if check_stable_policy(policy, new_policy): # compare policies\n",
    "            policy_stable = True # when they have the same greedy action for each state in the state space\n",
    "            print('Policy now stable - optimal policy found')\n",
    "            \n",
    "        policy =  # update policy\n",
    "        \n",
    "    print('Optimal policy:', policy)\n",
    "    return policy\n",
    "        \n",
    "optimal_policy = policy_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_map_to_func(policy_map): # turn our dict into a function\n",
    "    def policy(k):\n",
    "        return policy_map[k]    \n",
    "    return policy\n",
    "\n",
    "optimal_policy_func = policy_map_to_func(optimal_policy)\n",
    "visualise_agent(optimal_policy_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the output policy is optimal, but it is not the only optimal policy. For this map it's easy to see that there could be alternatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 2: Value iteration - forget the explicit policy\n",
    "\n",
    "Value iteration is very similar to policy iteration - but we only perform a single sweep over the state space when updating our value function, instead of repeating this until our approximate value function converges for the current policy.\n",
    "\n",
    "The policy evaluation phase requires us to sweep over the entire state space repeatedly. If the state space is large, then this can be computationally expensive.\n",
    "\n",
    "It also turns out that it's not necessary.\n",
    "\n",
    "Every time we perform a sweep, the value function gets closer to the true value function for the current policy. \n",
    "It can be seen that improved policies are found by greedily following value functions that result from truncated policy evaluations.\n",
    "\n",
    "Additionally, if we are greedily following the value function rather than querying the policy each timestep then we don't need to represent the policy explicitly. \n",
    " \n",
    "\n",
    "Notice that in this case, to learn an optimal policy, we never have to represent it explicitly. There is no function which represents the policy. Instead we just look-ahead and choose the action that maximises the value of the next state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where does value iteration come from?\n",
    "\n",
    "In the first line below, the maximum state-value of a state is equivalent to the maximum action-value when taking the best action in that state. Following this, we can derive a recursive definition of the optimal value function.\n",
    "\n",
    "In the last step, we even remove the policy from the equation entirely! This means that value iteration never needs to explicitly represent a policy in terms of a function that takes in a state and returns a distribution over actions.\n",
    "Instead, value iteration uses a **model**, $p(s', r | s, a)$, to look one step ahead, and take the action, $a$, that most likely leads it to the next state that has the best state-value function.\n",
    "\n",
    "A **model** defines how the state changes. It is also known as the transition dynamics of the environment. In our case the model is really simple: we are certain that taking the action to move right will move our agent one space to the right as long as there are no obstacles. There is no randomness in our environment (e.g. no wind that might push us into a different cell when we try to move right). That is, our environment is deterministic, not stochastic.\n",
    "\n",
    "![](./images/bellman_op_v.png)\n",
    "\n",
    "![](./images/backup_v.png)\n",
    "\n",
    "![](./images/update_rule_v.png)  \n",
    "\n",
    "Let's now implement value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(num_states=16, num_actions=4, error_threshold=0.01, discount_factor=0.9):\n",
    "    converged = False\n",
    "    value_table = initialise_value_table()\n",
    "    while not converged:\n",
    "        new_value_table = {}\n",
    "        \n",
    "        # POLICY EVALUATION WITH CONSTANT IMPLICIT POLICY UPDATES\n",
    "        worst_delta = 0\n",
    "        for state in range(num_states): # sweep once over state space\n",
    "            \n",
    "            # FIND BEST ACTION FOR THIS STATE BASED ON CURRENT VALUE TABLE\n",
    "            best_action = None\n",
    "            best_value = -float('inf')\n",
    "            for action in range(num_actions): # max over actions\n",
    "                new_state, reward = # simulate next state\n",
    "                new_val = # compute new value\n",
    "                if new_val > best_value:\n",
    "                    best_action = action\n",
    "                    best_value = new_val\n",
    "            \n",
    "            # CHECK ERROR\n",
    "            delta = # check value change for each state\n",
    "            worst_delta = max(delta, worst_delta)\n",
    "            \n",
    "            new_value_table[state] = best_value # update value table greedily\n",
    "\n",
    "        value_table = new_value_table\n",
    "        \n",
    "        # NOTE THAT THERE IS NO POLICY IMPROVEMENT STEP - IT'S DONE IMPLICITLY BY MAXIMISING VALUES OVER ACTIONS\n",
    "        \n",
    "        # CHECK CONVERGENCE\n",
    "        if worst_delta < : # check if worst delta below threshold\n",
    "            print('Converged')\n",
    "            converged = True\n",
    "    \n",
    "    # RETURN DETERMINISTIC POLICY - at this point, the algorithm has completed its job\n",
    "    # now the value function should be optimal, and hence correspond to an optimal policy which we can extract\n",
    "    output_policy = {}\n",
    "    for state in range(num_states):    \n",
    "        best_action = None\n",
    "        best_value = -float('inf')\n",
    "        for action in range(num_actions): # max over actions\n",
    "            new_state, reward = # \n",
    "            new_val = # new val\n",
    "            if new_val > best_value:\n",
    "                best_action = action\n",
    "                best_value = new_val\n",
    "        output_policy[state] = best_action\n",
    "    return output_policy\n",
    "\n",
    "optimal_policy = value_iteration()\n",
    "optimal_policy_func = policy_map_to_func(optimal_policy)\n",
    "visualise_agent(optimal_policy_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Value iteration and policy iteration are types of **value based** method - they use a value or Q function to find an optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(policy, n_episodes=100):\n",
    "#     global value_table\n",
    "#     global i_episode\n",
    "#     try:\n",
    "#         for _ in range(n_episodes):\n",
    "#             observation = env.reset()\n",
    "#             episode_mem = []\n",
    "#             done=False\n",
    "#             t = 0\n",
    "#             while not done:\n",
    "#                 env.render()\n",
    "#                 time.sleep(0.05)\n",
    "#                 action = policy(observation)\n",
    "#                 new_observation, reward, done, info = env.step(action)\n",
    "#                 episode_mem.append({'observation':observation,\n",
    "#                                     'action':action,\n",
    "#                                     'reward':reward,\n",
    "#                                     'new_observation':new_observation,\n",
    "#                                     'done':done})\n",
    "#                 observation=new_observation\n",
    "#                 t+=1\n",
    "#                 epsilon*=0.999\n",
    "#             episode_mem = calculate_Gs(episode_mem, discount_factor)\n",
    "#             value_table, v_delta = update_value_table(value_table, episode_mem)\n",
    "#             i_episode+=1\n",
    "#             print(\"Episode {} finished after {} timesteps. Eplsilon={}. V_Delta={}\".format(i_episode, t, epsilon, v_delta))\n",
    "#             env.render()\n",
    "#             time.sleep(1)\n",
    "#         env.close()\n",
    "#     except KeyboardInterrupt:\n",
    "#         env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon = 1\n",
    "# i_episode = 0\n",
    "# discount_factor = 0.8\n",
    "# value_table = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_Gs(episode_mem, discount_factor=0.95):\n",
    "#     for i, mem in reversed(list(enumerate(episode_mem))): #start from terminal state\n",
    "#         if i==len(episode_mem)-1: #if terminal state, G=reward\n",
    "#             episode_mem[i]['G']= mem['reward'] \n",
    "#         else:\n",
    "#             G = mem['reward']+discount_factor*episode_mem[i+1]['G']\n",
    "#             episode_mem[i]['G'] = G \n",
    "#     return episode_mem\n",
    "\n",
    "# def update_value_table(value_table, episode_mem):\n",
    "#     all_diffs=[]\n",
    "#     for mem in episode_mem:\n",
    "#         key = pickle.dumps(mem['new_observation'])\n",
    "#         if key not in value_table:\n",
    "#             value_table[key]=0 #initialize\n",
    "#         new_val = max(value_table[key], mem['G'])\n",
    "#         diff = abs(value_table[key]-new_val)\n",
    "#         all_diffs.append(diff)\n",
    "#         value_table[key] = new_val\n",
    "#     return value_table, np.mean(all_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(policy, n_episodes=100):\n",
    "#     global epsilon\n",
    "#     global value_table\n",
    "#     global i_episode\n",
    "#     try:\n",
    "#         for _ in range(n_episodes):\n",
    "#             observation = env.reset()\n",
    "#             episode_mem = []\n",
    "#             done=False\n",
    "#             t=0\n",
    "#             while not done:\n",
    "#                 env.render()\n",
    "#                 time.sleep(0.05)\n",
    "#                 action = policy(observation)\n",
    "#                 new_observation, reward, done, info = env.step(action)\n",
    "#                 episode_mem.append({'observation':observation,\n",
    "#                                     'action':action,\n",
    "#                                     'reward':reward,\n",
    "#                                     'new_observation':new_observation,\n",
    "#                                     'done':done})\n",
    "#                 observation=new_observation\n",
    "#                 t+=1\n",
    "#                 epsilon*=0.999\n",
    "#             episode_mem = calculate_Gs(episode_mem, discount_factor)\n",
    "#             value_table, v_delta = update_value_table(value_table, episode_mem)\n",
    "#             i_episode+=1\n",
    "#             print(\"Episode {} finished after {} timesteps. Eplsilon={}. V_Delta={}\".format(i_episode, t, epsilon, v_delta))\n",
    "#             env.render()\n",
    "#             time.sleep(1)\n",
    "#         env.close()\n",
    "#     except KeyboardInterrupt:\n",
    "#         env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train(random_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawbacks of Dynamic Programming\n",
    "#### What if we aren't fortunate enough to have a model?\n",
    "We very rarely have access to a transition model for the environment.\n",
    "\n",
    "#### Policy evaluation requires sweeping over entire states\n",
    "\n",
    "Backgammon has over $10^{20}$ states. Performing a sweep over this many states is extremely computationally expensive. \n",
    "Some of these states are very rare. Some might never actually be seen through experience.\n",
    "How can we continue to evaluate policies without needing this sweep?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Q-functions - how good is taking a certain action in a certain state?\n",
    "\n",
    "Another useful thing for our agent to know is how good it is to take a particular action, from a particular state.\n",
    "\n",
    "![](./images/q_def.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Monte-Carlo (MC) methods for computing value functions and improving policies\n",
    "\n",
    "\n",
    "Monte-Carlo methods are those based on repeated sampling to estimate a quantity. \n",
    "\n",
    "Similarly to what we did using DP, we can do MC based implementations of policy and value iteration by sampling experienced values of states, rather than simulating them with a model.\n",
    "\n",
    "\n",
    "The goal of using MC methods is to avoing the need for a model - if we don't have to look ahead from each state, then we can remove the model.\n",
    "\n",
    "However, even if we find an optimal value function, we will still need to use a model to extract an optimal policy to understand what states are reachable from others. It's no good having a chess piece next you your opponent's King if you don't know how that piece can move.\n",
    "\n",
    "The way we chose actions greedily when we had a model, was by using it to consider all possible actions and then taking the one that gave us the best expected return.\n",
    "\n",
    "As such, what would be more useful would be to use MC methods to estimate the action-value function. This tells us how good any particular action is from a certain state. If we know how good each action is, then we don't need a model - as long as we know all possible actions, we can just plug them into our q function and then take the one which for which our q function returns the largest value.\n",
    "\n",
    "#### Computing value functions using Monte-Carlo methods\n",
    "\n",
    "For this method, we will use Monte-Carlo sampling to estimate the q-value of each state by running many episodes, and then doing backup from the terminal state.\n",
    "\n",
    "The equation below shows how we can use Monte-Carlo sampling to approximate the value function for any given policy, alternatively to how we did so using the model in dynamic programming.\n",
    "\n",
    "![](./images/MC-value-calc.jpg)\n",
    "\n",
    "Here's one way that we can incrementally compute this average.\n",
    "\n",
    "![](./images/exp-avg.jpg)\n",
    "\n",
    "\n",
    "#### Improving the policy using Monte-Carlo methods\n",
    "\n",
    "As with DP, we can improve the current policy by acting greedily with respect to the latest approximation of the value function in the process of generalised policy iteration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_action_value_table(num_states=16, num_actions=4):\n",
    "    q_table = {}\n",
    "    # initialise zero action value table\n",
    "    return q_table\n",
    "\n",
    "def get_state_idx_from_observation(observation):\n",
    "    return np.argmax(observation[2].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GriddyEnv(4, 4)\n",
    "epsilon = 1\n",
    "i_episode=0\n",
    "discount_factor=0.9\n",
    "learning_rate=0.3\n",
    "value_table = {}\n",
    "\n",
    "def return_optimal_policy_from_q(q_table):\n",
    "    optimal_policy = {}\n",
    "    for # for each state\n",
    "        optimal_policy[state] = # rerturn the optimal action\n",
    "    return optimal_policy\n",
    "\n",
    "def value_table_viz(value_table):\n",
    "    values = np.zeros((4, 4))\n",
    "    base_st = np.zeros((3, 4, 4), dtype=np.int64)\n",
    "    base_st[0, 3, 3]=1\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            test_st = np.copy(base_st)\n",
    "            test_st[2, i, j] = 1\n",
    "            key = pickle.dumps(test_st)\n",
    "            if key in value_table:\n",
    "                val = value_table[key]\n",
    "            else:\n",
    "                val=0\n",
    "            values[i, j] = val\n",
    "    return values\n",
    "\n",
    "def count_empty_action_values(q_table):\n",
    "    empty = 0\n",
    "    for s, actions in q_table.items():\n",
    "        for action, q in actions.items():\n",
    "#         print()\n",
    "#         print('action', action)\n",
    "#         print('q', q)\n",
    "#         print('empty?: ', q ==0)\n",
    "            if q <= 0:\n",
    "                empty += 1\n",
    "    print(f'{empty} empty action values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have our agent update its q-table after each episode, using the experience which it gained from that episode. This experience will be a list of dictionaries with the state, observation, reward and new state at each timestep.\n",
    "\n",
    "Fill in the function below to update the q-table using the experience from an episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(episode_mem, q_table, discount_factor=0.9, learning_rate=0.1):\n",
    "    for # do backup\n",
    "        state = get_state_idx_from_observation(mem['observation'])\n",
    "        new_state = get_state_idx_from_observation(mem['new_observation'])\n",
    "        action = mem['action']\n",
    "        if idx == len(episode_mem) - 1: # if terminal state, G=reward\n",
    "            _return = # get return from final step\n",
    "        else:\n",
    "            _return = # otherwise compute recursively\n",
    "        q_table[state][action] =  # take exponential average\n",
    "        q_table[state][action] = np.round(q_table[state][action], 2) # round to 2 dp\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding an optimal policy using Monte-Carlo methods\n",
    "\n",
    "Let's implement a Monte-Carlo algorithm to find the value function for our random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_policy_evaluation(policy, n_episodes=100):\n",
    "    q_table = initialise_action_value_table()\n",
    "    try:\n",
    "        \n",
    "        # SAMPLE SOME EPISODES\n",
    "        for episode_idx in range(n_episodes):\n",
    "            observation = env.reset()\n",
    "            episode_mem = []\n",
    "            done=False\n",
    "            t = 0\n",
    "            while not done:\n",
    "                env.render()\n",
    "                time.sleep(0.05)\n",
    "                action = policy(observation)\n",
    "                new_observation, reward, done, info = env.step(action)\n",
    "                episode_mem.append({'observation':observation,\n",
    "                                    'action':action,\n",
    "                                    'reward':reward,\n",
    "                                    'new_observation':new_observation,\n",
    "                                    'done':done})\n",
    "                observation = new_observation\n",
    "                t += 1\n",
    "                \n",
    "            q_table =  # update q table this is the implicit policy update\n",
    "            print(\"Episode {} finished after {} timesteps. Eplsilon={}.\".format(episode_idx, t, epsilon))#, end='\\r')\n",
    "            env.render(value_table_viz(value_table))\n",
    "            print('q table:', q_table)\n",
    "            count_empty_action_values(q_table) \n",
    "            print()\n",
    "            \n",
    "            time.sleep(2)\n",
    "\n",
    "\n",
    "        env.close()\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()\n",
    "        \n",
    "    return q_table\n",
    "\n",
    "MC_policy_evaluation(random_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've computed the value function for this policy, let's improve the policy by acting greedily with respect to that value function.\n",
    "\n",
    "Fill in the cell to implement a greedy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(state, q_table):\n",
    "    return np.argmax(q_table[state].values()) # return action with best value for this state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_MC_control():\n",
    "    greedy_policy = policy_map_to_func(q_table)\n",
    "    while True:\n",
    "        \n",
    "        # POLICY EVALUATION\n",
    "        q_table = MC_policy_evaluation(greedy_policy)\n",
    "        \n",
    "        # POLICY IMPROVEMENT\n",
    "        \n",
    "        \n",
    "        # CHECK CONVERGENCE\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why not just act greedily all the time?\n",
    "\n",
    "If we act greedily all the time then we will move into the state with the best value. But remember that these values are only estimates based on our agent's experience with the game, which means that they might not be correct. So if we want to make sure that our agent will do well by always choosing the next action greedily, we need to make sure that it has good estimates for the values of those states. This brings us to a core challenge in reinforcement learning: **the exploration vs exploitation dilemma**. Our agent can either exploit what it knows by using it's current knowledge to choose the best action, or it can explore more and improve it's knowledge perhaps learning that some actions are even worse than what it does currently.\n",
    "\n",
    "Because we aren't using a model here, we aren't considering all possible actions. Instead we are just acting based on experience. \n",
    "If we experience a lower return from taking one action in a state rather than another, then we are not going to try the one that gave us the low return again.\n",
    "This would be more problematic in a stochastic environment - where environment transitions can vary randomly.\n",
    "When we act greedily we stop exploring, only exploiting the experience we have, even though we might have not experienced the best states.\n",
    "\n",
    "## An epsilon-greedy policy\n",
    "We can combine our random policy and our greedy policy to make an improved policy that both explores its environment and exploits its current knowledge. An $\\epsilon$-greedy (epsilon-greedy) policy is one which exploits what it knows most of the time, but with probability $\\epsilon$ will instead select a random action to try.\n",
    "\n",
    "## Do we need to keep exploring once we are confident in the values of states?\n",
    "\n",
    "As our agent explores more, it becomes more confident in predicting how valuable any state is. Once it knows a lot, it should start to explore less and exploit what it knows more. That means that we should decrease epsilon over time.\n",
    "\n",
    "Let's implement it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state):\n",
    "    epsilon = 0.05\n",
    "    if random.random() < epsilon:\n",
    "        return random_policy(state)\n",
    "    else:\n",
    "        return greedy_policy(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Code solution with visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from griddy_env import GriddyEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_from_state(state):\n",
    "    key = pickle.dumps(state)\n",
    "    if key not in value_table:\n",
    "        value_table[key]=0 #initialize\n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_value_table(episode_mem, value_table, discount_factor=0.95, learning_rate=0.1):\n",
    "    all_diffs=[]\n",
    "    for i, mem in reversed(list(enumerate(episode_mem))): #start from terminal state\n",
    "        if i==len(episode_mem)-1: #if terminal state, G=reward\n",
    "            calculated_new_v = episode_mem[i]['reward']\n",
    "        else:\n",
    "            calculated_new_v = mem['reward']+(discount_factor*np.max(greedy_policy(mem['new_observation'], return_action_vals=True)))\n",
    "        key = key_from_state(mem['new_observation'])\n",
    "        diff = abs(value_table[key]-calculated_new_v)\n",
    "        all_diffs.append(diff)\n",
    "        value_table[key] =  value_table[key] + learning_rate*(calculated_new_v-value_table[key])\n",
    "    return value_table, np.mean(all_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #This is the transition model aka our model of the environment. Given state and action it predicts next state\n",
    "# def transition(state, action):\n",
    "#     state = np.copy(state)\n",
    "#     agent_pos = list(zip(*np.where(state[2] == 1)))[0]\n",
    "#     new_agent_pos = np.array(agent_pos)\n",
    "#     if action==0:\n",
    "#         new_agent_pos[1]-=1\n",
    "#     elif action==1:\n",
    "#         new_agent_pos[1]+=1\n",
    "#     elif action==2:\n",
    "#         new_agent_pos[0]-=1\n",
    "#     elif action==3:\n",
    "#         new_agent_pos[0]+=1    \n",
    "#     new_agent_pos = np.clip(new_agent_pos, 0, 3)\n",
    "\n",
    "#     state[2, agent_pos[0], agent_pos[1]] = 0 #moved from this position so it is empty\n",
    "#     state[2, new_agent_pos[0], new_agent_pos[1]] = 1 #moved to this position\n",
    "#     return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(state, return_action_vals=False):\n",
    "    action_values=[]\n",
    "    for test_action in range(4): #for each action\n",
    "        new_state = transition(state, test_action)\n",
    "        key = key_from_state(new_state)\n",
    "        action_values.append(value_table[key])\n",
    "    policy_action = np.argmax(action_values)\n",
    "    if return_action_vals: return action_values\n",
    "    return policy_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state):\n",
    "    action = env.action_space.sample() if np.random.rand()<epsilon else greedy_policy(state)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    return np.random.randint(0, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_table_viz(value_table):\n",
    "    values = np.zeros((4, 4))\n",
    "    base_st = np.zeros((3, 4, 4), dtype=np.int64)\n",
    "    base_st[0, 3, 3]=1\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            test_st = np.copy(base_st)\n",
    "            test_st[2, i, j] = 1\n",
    "            key = pickle.dumps(test_st)\n",
    "            if key in value_table:\n",
    "                val = value_table[key]\n",
    "            else:\n",
    "                val=0\n",
    "            values[i, j] = val\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_agent(policy, value_table=None, n=5):\n",
    "    try:\n",
    "        for trial_i in range(n):\n",
    "            observation = env.reset()\n",
    "            done=False\n",
    "            t=0\n",
    "            while not done:\n",
    "                if value_table: env.render(value_table_viz(value_table))\n",
    "                else: env.render()\n",
    "                policy_action = policy(observation)\n",
    "                observation, reward, done, info = env.step(policy_action)\n",
    "                time.sleep(0.5)\n",
    "                t+=1\n",
    "            env.render()\n",
    "            time.sleep(1.5)\n",
    "            print(\"Episode {} finished after {} timesteps\".format(trial_i, t))\n",
    "        env.close()\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def greedy_policy(state, return_action_vals=False):\n",
    "    action_values=[]\n",
    "    for test_action in range(4): #for each action\n",
    "        new_state = transition(state, test_action)\n",
    "        print()\n",
    "        action_values.append(value_table[key])\n",
    "    policy_action = np.argmax(action_values)\n",
    "    if return_action_vals: return action_values\n",
    "    return policy_action\n",
    "\n",
    "def train(policy, n_episodes=100):\n",
    "    global epsilon\n",
    "    global value_table\n",
    "    global i_episode\n",
    "    try:\n",
    "        for _ in range(n_episodes):\n",
    "            observation = env.reset()\n",
    "            episode_mem = []\n",
    "            done=False\n",
    "            t=0\n",
    "            while not done:\n",
    "                env.render()\n",
    "                time.sleep(0.05)\n",
    "                action = policy(observation)\n",
    "                print('action:', action)\n",
    "                new_observation, reward, done, info = env.step(action)\n",
    "                episode_mem.append({'observation':observation,\n",
    "                                    'action':action,\n",
    "                                    'reward':reward,\n",
    "                                    'new_observation':new_observation,\n",
    "                                    'done':done})\n",
    "                observation = new_observation\n",
    "                t+=1\n",
    "                epsilon*=0.999\n",
    "            value_table, v_delta = update_value_table(episode_mem, value_table, discount_factor, learning_rate)\n",
    "            i_episode+=1\n",
    "            print(\"Episode {} finished after {} timesteps. Eplislon={}. V_Delta={}\".format(i_episode, t, epsilon, v_delta))#, end='\\r')\n",
    "            #print(value_table_viz(value_table))\n",
    "            #print()\n",
    "            env.render(value_table_viz(value_table))\n",
    "            time.sleep(2)\n",
    "        env.close()\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()\n",
    "        \n",
    "train(greedy_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(epsilon_greedy_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Current estimates of value')\n",
    "value_table_viz(value_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_agent(greedy_policy, value_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook!\n",
    "\n",
    "Next you might want to check out:\n",
    "- [Policy Gradients]()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
