{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to RL \n",
    "\n",
    "## Problem setup: We want to make an AI that is able to complete a simple video game.\n",
    "\n",
    "### What is the game we are going to start with?\n",
    "In this game, we want our agent (character) to move through the 2D world and reach the goal. At each timestep our agent can to either move up, down, left or right. The agent cannot move into obstacles, and when it reaches the goal, the game ends.\n",
    "\n",
    "# insert video of game being played\n",
    "\n",
    "We are going to use an environment that we built, called Griddy, that works in exactly the same way as other environments provided as part of openAI gym. \n",
    "\n",
    "\n",
    "The main ideas are:\n",
    "<ul>\n",
    "<li>we need to create our environment</li>\n",
    "<li>we need to initialise it by calling `env.reset()`</li>\n",
    "<li>we can increment the simulation by one timestep by calling `env.step(action)`</li>\n",
    "</ul>\n",
    "\n",
    "Check out [openAI gym's docs](http://gym.openai.com/docs/) to see how the environments work in general and in more detail.\n",
    "\n",
    "Let's set up our simulation to train our agent in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import gym\n",
    "from griddy_env import GriddyEnvOneHot\n",
    "import numpy as np\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import random\n",
    "\n",
    "# SET UP THE ENVIRONMENT\n",
    "env = GriddyEnvOneHot()    # create the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once we have an agent in the game, what do we do?\n",
    "\n",
    "Our agent has no idea of how to win the game. It simply observes states that change based on it's actions and receives a reward signal for doing so.\n",
    "So the agent has to learn about the game for itself. Just like a baby learns to interact with it's world by playing with it, our agent has to try random actions to figure out when and why it receives negative or positive rewards.\n",
    "\n",
    "A function which tells the agent what to do in a given state is called a **policy**\n",
    "\n",
    "We need our agent to understand what actions might lead it to achieving high rewards, but it doesn't know anything about how to complete the game yet. So let's set up our environment and implement a random policy that takes in a state and returns a random action for the agent to take.\n",
    "\n",
    "# picture maybe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENT A RANDOM POLICY\n",
    "def random_policy(state):\n",
    "    return random.randint(0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\n",
      "Episode finished after 17 timesteps.\n",
      "Episode 1\n"
     ]
    }
   ],
   "source": [
    "# WRITE A LOOP FOR THE AGENT TO TRY RANDOM ACTIONS\n",
    "num_episodes = 3\n",
    "\n",
    "try:\n",
    "    for episode_idx in range(num_episodes):\n",
    "        print('Episode', episode_idx)\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        episode_mem = []\n",
    "        t = 0\n",
    "        while not done:\n",
    "            env.render()\n",
    "            action = random_policy(observation)\n",
    "            observation, _, done, info = env.step(action)\n",
    "            t += 1\n",
    "            time.sleep(0.1)\n",
    "        env.render()\n",
    "        #time.sleep(0.5)\n",
    "        print(f\"Episode finished after {t + 1} timesteps.\")\n",
    "    env.close()\n",
    "except KeyboardInterrupt:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## How do we know if we are doing well?\n",
    "\n",
    "When our agent takes this action and moves into a new state, the environment returns it a reward. The reward when it reaches the goal is +1, and 0 everywhere else. The reward that the agent receives at any point can be considered as what it feels in that moment - like pain or pleasure.\n",
    "\n",
    "**However**, the reward doesn't tell the agent how good that move actually was, only whether it sensed anything, and how good or bad that sensation was.\n",
    "\n",
    "E.g.\n",
    "- Our agent might not receive any reward for stepping toward the goal, even though this might be a good move.\n",
    "- A robot might receive a negative reward as it's battery depletes, but still make good progress towards its goal.\n",
    "- A chess playing agent might receive a positive reward for taking an opponent's piece, but make a bad move in doing so by exposing its king to an attack eventually causing it to lose the game.\n",
    "\n",
    "What we really want to know is not the instantaneous reward, but \"How good is the position I'm in right now?\". The measure of this, is called the *value* of the state. If we had a way to estimate this, then we could look ahead to the state that each action would take us to and take the action which results in us landing in the state with best value. A function that predicts this value is called a **state-value function**.\n",
    "\n",
    "# diagram of following value function\n",
    "\n",
    "### So how good *is* each state?\n",
    "Intuitively, we want our agents to receive as much reward as possible.\n",
    "In general, the goal of reinforcement learning is to maximise this future reward. \n",
    "\n",
    "# goal of RL equation\n",
    "![](./images/objective.png)\n",
    "\n",
    "The value of a state is the total reward that we can expect from this state onwards. \n",
    "This future reward is also known as the return.\n",
    "\n",
    "# return\n",
    "![](./images/return.png)\n",
    "\n",
    "To determine what these values are, we can have our agent play one run-through of the game and then *back-up* through that trajectory, step-by-step, looking forward at what the future reward was from that point.\n",
    "\n",
    "# backup diagram\n",
    "![](./images/backup.png)\n",
    "\n",
    "#### Is getting a reward now as good as getting the same reward later?\n",
    "- What if the reward is removed from the game in the next timestep?\n",
    "- Would you rather be rich now or later?\n",
    "- What if a larger reward is introduced and you don't have enough energy to reach both?\n",
    "- What about inflation?\n",
    "\n",
    "It's better to get rewards sooner.\n",
    "\n",
    "![](./images/decay.png)\n",
    "\n",
    "We can encode this into our goal by using a **discount factor**, $\\gamma \\in [0, 1]$ ($\\gamma$ between 0 and 1). This makes the goal become:\n",
    "\n",
    "![](./images/discounted_obj.png)\n",
    "\n",
    "\n",
    "#### How good is the terminal state?\n",
    "In this initial version of the game we get +1 reward when we reach the goal. So +1 is the value of that state!\n",
    "#### How good is the last state before the game ends?\n",
    "Well we don't get a reward for this state. But we know that the action we took led to the terminal state where we got a reward of +1. So discounting that future reward gives us an estimate of the value of this state at $\\gamma$.\n",
    "\n",
    "This process is recursive, and we can continue to apply it to each preceding state in the trajectory that we took until we arrive back at the initial state, having estimated values for every state that we encountered. Some states may not have been visited and as such won't have had their values updated yet.\n",
    "\n",
    "### The backup algorithm for value iteration\n",
    "\n",
    "# algo\n",
    "![](./images/backup_algo.png)\n",
    "\n",
    "Value iteration is a type of **value based** method. Notice that to learn an optimal policy, we never have to represent it explicitly. There is no function which represents the policy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"body\">\n",
    "<div class=\"title\">\n",
    "Implement the backup algorithm ðŸ”¥\n",
    "</div>\n",
    "\n",
    "Now our agent is exploring the environment, let's implement the backup algorithm to update our estimates of the value of each state.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALISE THE ENVIRONMENT\n",
    "### SAME AS PREVIOUS CELL\n",
    "\n",
    "# LOOP TO RUN EPISODES TO EXPLORE THE ENVIRONMENT\n",
    "### SAME AS PREVIOUS CELL\n",
    "\n",
    "    # FOR EACH TIMESTEP, SELECT AN ACTION USING OUR RANDOM POLICY\n",
    "    ### SAME AS PREVIOUS CELL\n",
    "    \n",
    "    # FOR EACH EPISODE RUN THE BACKUP ALGORITHM TO UPDATE THE VALUES\n",
    "    ### IMPLEMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"body\">\n",
    "<div class=\"title\">\n",
    "How can we use the values that we know to perform well?\n",
    "</div>\n",
    "\n",
    "Now that our agent is capable of exploring and learning about it's environment, we need to make it take advantage of what it knows so that it can perform well.\n",
    "Our random policy has helped us to estimate the values of each state, which means we have some idea of how good each state is. Think about how we could use this knowledge to make our agent perform well before reading the next paragraphs.\n",
    "\n",
    "In this simple version of the game, we know exactly what actions will lead us to what states. That means we have a perfect **model** of the environment. A model is a function that tells us how the state will change when we take certain actions. E.g. we know that if the agent tries to move up into an empty space, then that's where it will end up.\n",
    "\n",
    "Because we know exactly what states we can end up in by taking an action, we can just look at the value of the states and choose the action which leads us to the state with the greatest value. So we just move into the best state that we can reach at any point.\n",
    "A policy that always takes the action that it expects to end up in the best, currently reachable state is called a **greedy policy**.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"body\">\n",
    "<div class=\"title\">\n",
    "Why not just act greedily all the time?\n",
    "</div>\n",
    "\n",
    "If we act greedily all the time then we will move into the state with the best value. But remember that these values are only estimates based on our agent's experience with the game, which means that they might not be correct. So if we want to make sure that our agent will do well by always choosing the next action greedily, we need to make sure that it has good estimates for the values of those states. This brings us to a core challenge in reinforcement learning: **the exploration vs exploitation dilemma**. Our agent can either exploit what it knows by using it's current knowledge to choose the best action, or it can explore more and improve it's knowledge perhaps learning that some actions are even worse than what it does currently.\n",
    "\n",
    "# An epsilon-greedy policy\n",
    "We can combine our random policy and our greedy policy to make an improved policy that both explores its environment and exploits its current knowledge. An $\\epsilon$-greedy (epsilon-greedy) policy is one which exploits what it knows most of the time, but with probability $\\epsilon$ will instead select a random action to try.\n",
    "\n",
    "## Do we need to keep exploring once we are confident in the values of states?\n",
    "\n",
    "As our agent explores more, it becomes more confident in predicting how valuable any state is. Once it knows a lot, it should start to explore less and exploit what it knows more. That means that we should decrease epsilon over time.\n",
    "\n",
    "Let's implement it\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state):\n",
    "    epsilon = 0.05\n",
    "    if random.random() < epsilon:\n",
    "        return random_policy(state)\n",
    "    else:\n",
    "        return greedy_policy(state)\n",
    "\n",
    "num_episodes = 100\n",
    "\n",
    "\n",
    "# INITIALISE THE ENVIRONMENT\n",
    "### COPY FROM PREVIOUS CODE CELL\n",
    "\n",
    "# LOOP TO RUN EPISODES TO EXPLORE THE ENVIRONMENT\n",
    "### COPY FROM PREVIOUS CODE CELL\n",
    "\n",
    "    # FOR EACH TIMESTEP, SELECT AN ACTION USING OUR EPSILON-GREEDY POLICY\n",
    "    ### IMPLEMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"body\">\n",
    "<div class=\"title\">\n",
    "What if we don't have a model?\n",
    "</div>\n",
    "How big is the input space to the action-state value function?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"body\">\n",
    "<div class=\"title\">End of notebook!\n",
    "</div>\n",
    "\n",
    "Next you might want to check out:\n",
    "- [Policy Gradients]()\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
