{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to RL \n",
    "\n",
    "## Problem setup: We want to make an AI that is able to complete a simple video game.\n",
    "\n",
    "### What is the game we are going to start with?\n",
    "In this game, we want our agent (character) to move through the 2D world and reach the goal. At each timestep our agent can to either move up, down, left or right. The agent cannot move into obstacles, and when it reaches the goal, the game ends.\n",
    "\n",
    "# insert video of game being played\n",
    "\n",
    "We are going to use an environment that we built, called Griddy, that works in exactly the same way as other environments provided as part of openAI gym. \n",
    "\n",
    "\n",
    "The main ideas are:\n",
    "<ul>\n",
    "<li>we need to create our environment</li>\n",
    "<li>we need to initialise it by calling `env.reset()`</li>\n",
    "<li>we can increment the simulation by one timestep by calling `env.step(action)`</li>\n",
    "</ul>\n",
    "\n",
    "Check out [openAI gym's docs](http://gym.openai.com/docs/) to see how the environments work in general and in more detail.\n",
    "\n",
    "Let's set up our simulation to train our agent in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from GriddyEnv import GriddyEnv # make sure you: pip3 install GriddyEnv\n",
    "\n",
    "# SET UP THE ENVIRONMENT\n",
    "env = GriddyEnv()    # create the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once we have an agent in the game, what do we do?\n",
    "\n",
    "Our agent has no idea of how to win the game. \n",
    "It simply observes states, and takes actions.\n",
    "As a result of these actions, the agent will see the environment change to a new state and also receive some sensation. This sensation, which may be good or bad, is called a **reward**.\n",
    "and then  that change based on it's actions and receives a reward signal for doing so.\n",
    "\n",
    "This continuous interaction between the agent and our environment sets up the framework for a **reinforcement learning problem**, in an agent-environment loop as shown below.\n",
    "\n",
    "![](images/agent-env-loop.png)\n",
    "\n",
    "So without any prior knowledge, the agent has to learn about the game for itself. Just like a baby learns to interact with it's world by playing with it, our agent has to try random actions in the environment to figure out what causes it to receive negative or positive rewards.\n",
    "\n",
    "A function which tells the agent what actions to take from a given state is called a **policy**.\n",
    "\n",
    "![](./images/policy.png)\n",
    "\n",
    "Policies can be deterministic or stochastic (have randomness).\n",
    "\n",
    "Mathematically, a policy is a probability distribution over actions, conditioned on the state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our RL problem\n",
    "\n",
    "In our case:\n",
    "\n",
    "### Action Space\n",
    "The action space consists of 4 unique actions: 0, 1, 2, 3<br>\n",
    "0 - Move left<br>\n",
    "1 - Move right<br>\n",
    "2 - Move up<br>\n",
    "3 - Move down<br>\n",
    "\n",
    "### Observation Space\n",
    "Has shape (3, 4, 4). Our grid world is 4x4<br>\n",
    "Each of the 3 channels is a binary mask for the location of different objects within the environment.<br>\n",
    "Channel 0 - Goal<br>\n",
    "Channel 1 - Wall<br>\n",
    "Channel 2 - Agent<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop our agent into the environment, implement a random policy, and then watch it act for a few episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualise agent function\n",
    "def visualise_agent(policy, n=5):\n",
    "    try:\n",
    "        for trial_i in range(n):\n",
    "            observation = env.reset()\n",
    "            done=False\n",
    "            t=0\n",
    "            while not done:\n",
    "                env.render()\n",
    "                policy_action = policy(observation)\n",
    "                observation, reward, done, info = env.step(policy_action)\n",
    "                time.sleep(0.1)\n",
    "                t += 1\n",
    "                print('observation:', observation)\n",
    "            env.render()\n",
    "            time.sleep(1.5)\n",
    "            print(\"Episode {} finished after {} timesteps\".format(trial_i, t))\n",
    "        env.close()\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENT A RANDOM POLICY\n",
    "def random_policy(state):\n",
    "    action = np.random.randint(0, 4)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 1 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 1 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 1 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 1 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 1 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 1]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 1 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 1 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 1 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 1 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 1 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 1 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 1 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 1 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 1 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 1 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 1 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 1]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 1]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 1]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 1 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 1 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[1 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[1 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 1 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 1 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 1 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 1 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 1 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 1 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 1 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 1 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 1 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 1 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 1 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 1]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 1]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 1]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 1]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 1]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 1]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 1]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 1 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 1 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 1 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 1 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 1 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [1 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[1 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [1 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[1 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 1 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 1 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 1 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 1 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 1]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 1]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 1]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 1]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 1 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 1 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 1 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 1 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 1 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 1]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 1]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 1 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 1 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 1 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 1 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 1 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 1 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 1 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 1 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 1 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 1 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 1 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 1 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 1 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 1 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 1 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 1 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 1 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 1 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 1 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 1 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[1 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 1 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "observation: [[[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[1 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "visualise_agent(random_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we know if we are doing well?\n",
    "## What should our agent try to do?\n",
    "## What should our agent try to optimise?\n",
    "\n",
    "When our agent takes this action and moves into a new state, the environment returns it a reward. The reward when it reaches the goal is +1, and 0 everywhere else. The reward that the agent receives at any point can be considered as what it feels in that moment - like pain or pleasure.\n",
    "\n",
    "**However**, the reward doesn't tell the agent how good that move actually was, only whether it sensed anything, and how good or bad that sensation was at that particular moment.\n",
    "\n",
    "E.g.\n",
    "- Our agent might not receive any reward for stepping toward the goal, even though this might be a good move.\n",
    "- A robot might receive a negative reward as it's battery depletes, but still make good progress towards its goal.\n",
    "- A chess playing agent might receive a positive reward for taking an opponent's piece, but make a bad move in doing so by exposing its king to an attack eventually causing it to lose the game.\n",
    "\n",
    "What we really want to know is not the instantaneous reward, but \"How good is the position I'm in right now?\", that is, what amount of reward can our agent get from this point onwards.\n",
    "This future reward is also known as the return.\n",
    "\n",
    "![](./images/undiscounted_return.png)\n",
    "\n",
    "#### Is getting a reward now as good as getting the same reward later?\n",
    "- What if the reward is removed from the game in the next timestep?\n",
    "- Would you rather be rich now or later?\n",
    "- What if a larger reward is introduced and you don't have enough energy to reach both?\n",
    "- What about inflation?\n",
    "\n",
    "It's better to get rewards sooner rather than later.\n",
    "\n",
    "![](./images/decay.jpg)\n",
    "\n",
    "We can encode this into our goal by using a **discount factor**, $\\gamma \\in [0, 1]$ ($\\gamma$ between 0 and 1). \n",
    "The discount factor is the coefficient of a reward $t$ timesteps in the future, raised to the power of $t$. Because it is less than 1, raising to the power reduces its value. As such, this coefficient weights rewards further away in the future by a lesser number than those nearby in time.\n",
    "\n",
    "This makes our agent value more immediate rewards more than those which can be reached further in the future.\n",
    "This makes the goal become:\n",
    "\n",
    "![](./images/discounted_return.png)\n",
    "\n",
    "This is called the **discounted return**. From here on, when we say return, we mean discounted return unless otherwise specified, because we rarely use the undiscounted version.\n",
    "\n",
    "Once an agent has played a whole game it can calculate the return for each state it visited by simply adding up the discounted reward it achieved from there on.\n",
    "\n",
    "The value of these return values can also be defined recursively as shown below.\n",
    "\n",
    "![](./images/recursive_return.png)\n",
    " \n",
    "Because of this recursive relationship, we can calculate the experienced returns for each visited state in a single pass through the trajectory of that episode. We do this by working backwards, firstly calculating the  return from the terminal state (always zero), and then recursively calculating the return from the previous timestep by discounting it and adding the reward from that timestep.\n",
    "This process is called ***backup***.\n",
    "\n",
    "The return of a terminal state is always zero, because from there the episode will have terminated and hence the agent will not be able to attain any further reward.\n",
    "\n",
    "![](./images/backup.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What tools might our agent be able to use to perform well?\n",
    "\n",
    "### Value functions - so how good is each state?\n",
    "\n",
    "Now we know how to calculate the return that we experienced during any single episode. \n",
    "But this is just a single sample estimate. \n",
    "In general, the goal of reinforcement learning is to maximise the **expected** future reward. \n",
    "That is, to maximise the average return from a the current state onwards. \n",
    "This quantity is defined as the **state-value** of a state, commonly just referred to as the  or **value** of a state. \n",
    "A function that returns this value given a state is called a value function.\n",
    "\n",
    "![](./images/value_def.jpg)\n",
    "\n",
    "**Note that a value function must correspond to some policy.** If we follow a bad policy then states will have lower values than if we follow a good policy. If we change the policy, then the value function will change\n",
    "\n",
    "\n",
    "### Q-functions - how good is taking a certain action in a certain state?\n",
    "\n",
    "Another useful thing for our agent to know is how good it is to take a particular action, from a particular state.\n",
    "\n",
    "![](./images/q_def.jpg)\n",
    "\n",
    "### The Bellman equations\n",
    "\n",
    "The Bellman equations are a way of expressing state and action value functions recursively.\n",
    "\n",
    "### The Bellman Optimality equations\n",
    "\n",
    "#### If we act optimally (following an optimal policy $\\pi*$), how do the Bellman equations for $V$ and $Q$ relate to each other?\n",
    "\n",
    "#### Recovering the Bellman optimality questions\n",
    "\n",
    "#### What do the Bellman optimality equations optimise?\n",
    "\n",
    "What does it mean to reduce the Bellman error?\n",
    "Until the Bellman optimality equations are solved, we have not found an optimal policy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we use these tools, and improve our agent's performance?\n",
    "\n",
    "### Method 1: Dynamic Programming (DP) methods for computing value functions and improving policies\n",
    "\n",
    "The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect **model** of the environment.\n",
    "\n",
    "A model tells us how the environment will change when we take certain actions. It may be stochastic or deterministic. A model can allow us to **simulate** the progression of the environment, taking any action from any state.\n",
    "\n",
    "# diagram of model\n",
    "\n",
    "**A model will also be referred to as a transition function.**\n",
    "\n",
    "Fortunately (by our design) in this simple version of the game, we do know exactly what actions will lead us to what states. That means we have a perfect **model** of the environment. A model is a function that tells us how the state will change when we take certain actions. E.g. we know that if the agent tries to move up into an empty space, then that's where it will end up.\n",
    "\n",
    "Let's run the cell below to define our transition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSITION FUNCTION/ENVIRONMENT MODEL\n",
    "def old_transition(state, action):\n",
    "    state = np.copy(state)\n",
    "    agent_pos = list(zip(*np.where(state[2] == 1)))[0]\n",
    "    new_agent_pos = np.array(agent_pos)\n",
    "    if action==0:\n",
    "        new_agent_pos[1]-=1\n",
    "    elif action==1:\n",
    "        new_agent_pos[1]+=1\n",
    "    elif action==2:\n",
    "        new_agent_pos[0]-=1\n",
    "    elif action==3:\n",
    "        new_agent_pos[0]+=1    \n",
    "    new_agent_pos = np.clip(new_agent_pos, 0, 3)\n",
    "\n",
    "    state[2, agent_pos[0], agent_pos[1]] = 0 # moved from this position so it is empty\n",
    "    \n",
    "    state[2, new_agent_pos[0], new_agent_pos[1]] = 1 # moved to this position\n",
    "    \n",
    "    goal_pos = list(zip(*np.where(state[0] == 1)))[0]\n",
    "    agent_pos = list(zip(*np.where(state[2] == 1)))[0]\n",
    "    print('goal_pos', goal_pos)\n",
    "    print('agent pos:', agent_pos)\n",
    "    \n",
    "    return state, reward\n",
    "\n",
    "def transition(state, action):\n",
    "    LEFT = 0\n",
    "    DOWN = 1\n",
    "    RIGHT = 2\n",
    "    UP = 3\n",
    "    GOAL = 15\n",
    "    nrow = 4\n",
    "    ncol = 4\n",
    "    row = state // 4\n",
    "    col = state % 4\n",
    "#     print('state:', state, 'row:', row, 'col:', col)\n",
    "    if action == LEFT:\n",
    "        state = (row, max(col-1, 0))\n",
    "    if action == DOWN:\n",
    "        state = (min(row+1, nrow - 1), col)\n",
    "    if action == RIGHT:\n",
    "        state = (row, min(col+1, ncol - 1))\n",
    "    if action == UP:\n",
    "        state = (max(row-1, 0), col)\n",
    "#     new_states = [(row, max(col-1, 0)), (min(row+1, nrow - 1), col), (row, min(col+1, ncol - 1)), (max(row-1, 0), col)] # new (row, col) if action [left, down, right, up] is taken\n",
    "#     state = new_states[action]\n",
    "    state = nrow * state[0] + state[1] # convert back to integer\n",
    "#     print('action:', action)\n",
    "    row = state // 4\n",
    "    col = state % 4\n",
    "#     print('new_state:', state, 'row:', row, 'col:', col)\n",
    "    reward = 0\n",
    "    if state == GOAL:\n",
    "        reward = 1\n",
    "    return state, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a model, we can look ahead to the successor states reachable from our current state.\n",
    "If we also had a way to estimate the value function, then we could look ahead to the state that each action would take us to and take the action which results in the best expected return.\n",
    "\n",
    "Acting greedily with respect to a value function for an optimal policy will produce optimal behaviour.\n",
    "\n",
    "![](./images/follow_values.png)\n",
    "\n",
    "#### Computing value functions using dynamic programming\n",
    "## Algorithm 1: Policy iteration\n",
    "\n",
    "### Step 1: Policy evaluation step\n",
    "\n",
    "Policy evaluation is the process of approximately evaluating the value function for our current policy.\n",
    "\n",
    "How can we do this using our environment model?\n",
    "\n",
    "The Bellman equations define the value of any state recursively, as a function of it successor state.\n",
    "We know that we can use our model to simulate the next states that our agent will move into by taking any given action, and that it defines what rewards we might receive.\n",
    "Given this, along with the fact that our value table already contains estimates\n",
    "\n",
    "Even if we \n",
    "We can't overestimate the value of a state because the transition dynamics are known\n",
    "The best value that \n",
    "\n",
    "### Step 2: Policy improvement step\n",
    "\n",
    "### Does this converge to an optimal policy?\n",
    "\n",
    "For a policy $\\pi'$ to better than some other policy $\\pi$?, it must be such that $v_{\\pi'}(s) \\geq v_{\\pi}(s)$ for all states.\n",
    "\n",
    "Why is $\\pi'$ strictly better than $\\pi$?\n",
    "\n",
    "$\\pi$ is determining the column of the action-value (Q) table that we should take. This is fixed until we do policy improvement. As we perform policy evaluation, the values of the table might change, and the column containing the maximum value for any given state might change\n",
    "\n",
    "### Full policy iteration algorithm\n",
    "\n",
    "The full policy iteration algorithm iterates between policy evaluation and policy improvement. This alternatively improves the policy by making it greedy with respect to the value function, and then improves the value function by minimising the Bellman error.\n",
    "\n",
    "# policy eval algo\n",
    "\n",
    "Local consistency. The values of each state must relate to all their neighbbouring (reachable in 1 step) states according to the Bellman equation for $v(s)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_value_table(num_states=16):\n",
    "    value_table = {} # start off with empty map\n",
    "    for s in range(num_states): # for each state\n",
    "        value_table[s] = np.random.rand()\n",
    "        value_table[s] = 0\n",
    "#         states = np.zeros(3, num_states)\n",
    "#         states[s] = 1\n",
    "#         key = pickle.dumps(s)\n",
    "#         value_table[key] = 0\n",
    "    print(value_table)\n",
    "    return value_table\n",
    "\n",
    "def initialise_policy(num_states=16, num_actions=4):\n",
    "    policy = {} # start off with empty map\n",
    "    for s in range(num_states):\n",
    "        action_probs = np.random.rand((num_actions))\n",
    "        action_probs /= sum(action_probs)\n",
    "        policy[s] = action_probs\n",
    "    return policy\n",
    "\n",
    "\n",
    "def initialise_deterministic_policy(num_states=16, num_actions=4):\n",
    "    policy = {} # start off with empty map\n",
    "    for s in range(num_states):\n",
    "        action_probs = np.random.rand((num_actions))\n",
    "        action_probs /= sum(action_probs)\n",
    "        policy[s] = np.argmax(action_probs) # only line different to above - the function returns the action, rather than the dist over\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTING VALUE FUNCTION USING DYNAMIC PROGRAMMING\n",
    "def policy_evaluation(policy, value_table, discount_factor, error_threshold=0.01, num_states=16):\n",
    "    print(value_table)\n",
    "\n",
    "    new_value_table = {} # init new value table to be filled in and returned\n",
    "    converged = False # initially we have not found a converged value function for this policy\n",
    "    while not converged: # until the value function converges\n",
    "        worst_delta = 0 # difference between previous values and iterated values\n",
    "        for state in range(num_states): # loop over each state\n",
    "            action = policy[state]\n",
    "            new_state, reward = transition(state, action)\n",
    "            new_val = reward + discount_factor * value_table[new_state]\n",
    "#             print(new_val)\n",
    "#             print(value_table[state])\n",
    "            new_value_table[state] = new_val\n",
    "            delta = abs(new_val - value_table[state]) # find the absolute diff between new val and old val for this state\n",
    "#             print('delta:', delta)\n",
    "            if delta > worst_delta: \n",
    "                worst_delta = delta\n",
    "                print('worst_delta:', worst_delta)\n",
    "        print()   \n",
    "        if worst_delta < error_threshold: # once the values stop changing\n",
    "            converged = True # we have found the value function\n",
    "            print('Converged on value function')\n",
    "        value_table = new_value_table # took me ages to realise i was missing this line and debug \n",
    "    return new_value_table # return value table evaluated for this policy\n",
    "        \n",
    "# IMPROVE POLICY\n",
    "def policy_improvement(value_table, discount_factor): # set a greedy policy which will always be better than the previous\n",
    "    new_policy = {}\n",
    "    action_space = range(4) \n",
    "    for state in value_table.keys(): # loop over each state\n",
    "        best_value = -float('inf')\n",
    "        best_action = None\n",
    "        for action in action_space: \n",
    "            new_state, reward = transition(state, action)\n",
    "            value = reward + discount_factor * value_table[new_state]\n",
    "            if value > best_value:\n",
    "                best_value = value\n",
    "                best_action = action\n",
    "        new_policy[state] = best_action\n",
    "    return new_policy\n",
    "\n",
    "def check_stable_policy(old_policy, new_policy):\n",
    "    stable = True\n",
    "    for s in range(16):\n",
    "        old_action = old_policy[s]\n",
    "        new_action = new_policy[s]\n",
    "        if new_action != old_action:\n",
    "            stable = False\n",
    "    return stable\n",
    "\n",
    "# # IMPLEMENT A GREEDY POLICY\n",
    "# def greedy_policy(state, value_table):\n",
    "#     best_expected_return = 0\n",
    "#     best_action = None\n",
    "#     for action in range(0, 4): # for all possible actions\n",
    "#         new_state, reward = transition(state, action)\n",
    "#         expected_return = 0\n",
    "#         for new_state in new_states:\n",
    "#             expected_return += reward + discount_factor * value_table[new_state] # our MODEL is deterministic, if it weren't, we'd need to average over different possible next states\n",
    "#         if expected_return > best_expected_return:\n",
    "#             best_expected_return = expected_return\n",
    "#             best_action = action\n",
    "#     return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0}\n",
      "Evaluating policy  0\n",
      "{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0}\n",
      "worst_delta: 1.0\n",
      "\n",
      "\n",
      "Converged on value function\n",
      "Iterating policy  0\n",
      "Evaluating policy  0\n",
      "{0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.9, 14: 1.0, 15: 0.9}\n",
      "worst_delta: 0.81\n",
      "worst_delta: 0.9\n",
      "worst_delta: 1.81\n",
      "\n",
      "\n",
      "Converged on value function\n",
      "Iterating policy  0\n",
      "Evaluating policy  0\n",
      "{0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.81, 10: 1.629, 11: 2.629, 12: 0.81, 13: 1.629, 14: 2.629, 15: 2.629}\n",
      "worst_delta: 0.7290000000000001\n",
      "worst_delta: 1.4661\n",
      "worst_delta: 2.3661\n",
      "\n",
      "\n",
      "Converged on value function\n",
      "Iterating policy  0\n",
      "Evaluating policy  0\n",
      "{0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 1.31949, 6: 2.12949, 7: 3.02949, 8: 1.31949, 9: 2.12949, 10: 3.02949, 11: 4.02949, 12: 2.12949, 13: 3.02949, 14: 4.02949, 15: 4.02949}\n",
      "worst_delta: 1.1875410000000002\n",
      "worst_delta: 1.916541\n",
      "worst_delta: 2.726541\n",
      "\n",
      "\n",
      "Converged on value function\n",
      "Iterating policy  0\n",
      "Evaluating policy  0\n",
      "{0: 0.0, 1: 1.7248869, 2: 2.4538869, 3: 3.2638869, 4: 1.7248869, 5: 2.4538869, 6: 3.2638869, 7: 4.1638869, 8: 2.4538869, 9: 3.2638869, 10: 4.1638869, 11: 5.1638869, 12: 3.2638869, 13: 4.1638869, 14: 5.1638869, 15: 5.1638869}\n",
      "worst_delta: 1.55239821\n",
      "\n",
      "\n",
      "Converged on value function\n",
      "Iterating policy  0\n",
      "Policy now stable - optimal policy found\n",
      "Optimal policy: {0: 1, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 1, 12: 2, 13: 2, 14: 2, 15: 1}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'dict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-f60faba1e0fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0moptimal_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mvisualise_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimal_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-26c3825d0e90>\u001b[0m in \u001b[0;36mvisualise_agent\u001b[0;34m(policy, n)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mpolicy_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict' object is not callable"
     ]
    }
   ],
   "source": [
    "# POLICY ITERATION ALGORITHM\n",
    "def policy_iteration(discount_factor=0.9):\n",
    "    value_table = initialise_value_table()\n",
    "#     policy = initialise_policy()\n",
    "    policy = initialise_deterministic_policy()\n",
    "    policy_stable = False\n",
    "    policy_idx = 0\n",
    "    while not policy_stable:\n",
    "        print('Evaluating policy ', policy_idx)\n",
    "        value_table = policy_evaluation(policy, value_table, discount_factor) # converge on value function\n",
    "        print('Iterating policy ', policy_idx)\n",
    "        new_policy = policy_improvement(value_table,discount_factor) # set greedy policy using converged value function\n",
    "        if check_stable_policy(policy, new_policy):\n",
    "            policy_stable = True\n",
    "            print('Policy now stable - optimal policy found')\n",
    "            \n",
    "        policy = new_policy\n",
    "    print('Optimal policy:', policy)\n",
    "    return policy\n",
    "        \n",
    "optimal_policy = policy_iteration()\n",
    "\n",
    "optimal_policy = \n",
    "def policy_map_to_func(policy_map, key):\n",
    "    \n",
    "    return policy_map[key]\n",
    "\n",
    "visualise_agent(optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the output policy is optimal, but it is not the only optimal policy. For this map it's easy to see that there could be alternatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 2: Value iteration - forget the explicit policy\n",
    "\n",
    "Value iteration is very similar to policy iteration. We only perform a single sweep over the state space when updating our value function, instead of repeating this until our approximate value function converges for this policy.\n",
    "\n",
    "Every time we perform a sweep, the value function gets closer to the true value function for the current policy. \n",
    "It can be seen that improved policies are found by greedily following \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### implement greedy policy\n",
    "#greedy policy\n",
    "def greedy_policy(state):\n",
    "    return policy_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(policy, n_episodes=100):\n",
    "    global value_table\n",
    "    global i_episode\n",
    "    try:\n",
    "        for _ in range(n_episodes):\n",
    "            observation = env.reset()\n",
    "            episode_mem = []\n",
    "            done=False\n",
    "            t = 0\n",
    "            while not done:\n",
    "                env.render()\n",
    "                time.sleep(0.05)\n",
    "                action = policy(observation)\n",
    "                new_observation, reward, done, info = env.step(action)\n",
    "                episode_mem.append({'observation':observation,\n",
    "                                    'action':action,\n",
    "                                    'reward':reward,\n",
    "                                    'new_observation':new_observation,\n",
    "                                    'done':done})\n",
    "                observation=new_observation\n",
    "                t+=1\n",
    "                epsilon*=0.999\n",
    "            episode_mem = calculate_Gs(episode_mem, discount_factor)\n",
    "            value_table, v_delta = update_value_table(value_table, episode_mem)\n",
    "            i_episode+=1\n",
    "            print(\"Episode {} finished after {} timesteps. Eplsilon={}. V_Delta={}\".format(i_episode, t, epsilon, v_delta))\n",
    "            env.render()\n",
    "            time.sleep(1)\n",
    "        env.close()\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_table = {}\n",
    "\n",
    "train(random_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/update_values.png)\n",
    "\n",
    "Value iteration is a type of **value based** method. Notice that to learn an optimal policy, we never have to represent it explicitly. There is no function which represents the policy. Instead we just look-ahead and choose the action that maximises the value of the next state.\n",
    "\n",
    "Now that our agent is exploring the environment, let's implement value iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1\n",
    "i_episode = 0\n",
    "discount_factor = 0.8\n",
    "value_table = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_Gs(episode_mem, discount_factor=0.95):\n",
    "    for i, mem in reversed(list(enumerate(episode_mem))): #start from terminal state\n",
    "        if i==len(episode_mem)-1: #if terminal state, G=reward\n",
    "            episode_mem[i]['G']= mem['reward'] \n",
    "        else:\n",
    "            G = mem['reward']+discount_factor*episode_mem[i+1]['G']\n",
    "            episode_mem[i]['G'] = G \n",
    "    return episode_mem\n",
    "\n",
    "def update_value_table(value_table, episode_mem):\n",
    "    all_diffs=[]\n",
    "    for mem in episode_mem:\n",
    "        key = pickle.dumps(mem['new_observation'])\n",
    "        if key not in value_table:\n",
    "            value_table[key]=0 #initialize\n",
    "        new_val = max(value_table[key], mem['G'])\n",
    "        diff = abs(value_table[key]-new_val)\n",
    "        all_diffs.append(diff)\n",
    "        value_table[key] = new_val\n",
    "    return value_table, np.mean(all_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(policy, n_episodes=100):\n",
    "    global epsilon\n",
    "    global value_table\n",
    "    global i_episode\n",
    "    try:\n",
    "        for _ in range(n_episodes):\n",
    "            observation = env.reset()\n",
    "            episode_mem = []\n",
    "            done=False\n",
    "            t=0\n",
    "            while not done:\n",
    "                env.render()\n",
    "                time.sleep(0.05)\n",
    "                action = policy(observation)\n",
    "                new_observation, reward, done, info = env.step(action)\n",
    "                episode_mem.append({'observation':observation,\n",
    "                                    'action':action,\n",
    "                                    'reward':reward,\n",
    "                                    'new_observation':new_observation,\n",
    "                                    'done':done})\n",
    "                observation=new_observation\n",
    "                t+=1\n",
    "                epsilon*=0.999\n",
    "            episode_mem = calculate_Gs(episode_mem, discount_factor)\n",
    "            value_table, v_delta = update_value_table(value_table, episode_mem)\n",
    "            i_episode+=1\n",
    "            print(\"Episode {} finished after {} timesteps. Eplsilon={}. V_Delta={}\".format(i_episode, t, epsilon, v_delta))\n",
    "            env.render()\n",
    "            time.sleep(1)\n",
    "        env.close()\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished after 110 timesteps. Eplislon=0.8957848329039134. V_Delta=0.027662717201199977\n",
      "Episode 2 finished after 2 timesteps. Eplislon=0.8939941590229386. V_Delta=0.3999999999478519\n",
      "Episode 3 finished after 20 timesteps. Eplislon=0.8762831198969288. V_Delta=0.023666372686992065\n",
      "Episode 4 finished after 10 timesteps. Eplislon=0.8675596164681794. V_Delta=0.184051036038044\n",
      "Episode 5 finished after 1 timesteps. Eplislon=0.8666920568517111. V_Delta=0.0\n",
      "Episode 6 finished after 36 timesteps. Eplislon=0.8360310213470206. V_Delta=0.01422280383608496\n",
      "Episode 7 finished after 6 timesteps. Eplislon=0.8310273589761736. V_Delta=0.0\n",
      "Episode 8 finished after 89 timesteps. Eplislon=0.7602278474153183. V_Delta=0.002173309250157304\n",
      "Episode 9 finished after 62 timesteps. Eplislon=0.7145029791340722. V_Delta=0.004875561290322581\n",
      "Episode 10 finished after 51 timesteps. Eplislon=0.6789596158607596. V_Delta=0.0031354478431372557\n",
      "Episode 11 finished after 13 timesteps. Eplislon=0.6701859060067403. V_Delta=0.0\n",
      "Episode 12 finished after 28 timesteps. Eplislon=0.6516718490384363. V_Delta=0.0014971028860342862\n",
      "Episode 13 finished after 59 timesteps. Eplislon=0.6143173286381555. V_Delta=0.0\n",
      "Episode 14 finished after 82 timesteps. Eplislon=0.5659301095244186. V_Delta=0.0\n",
      "Episode 15 finished after 13 timesteps. Eplislon=0.5586169991970452. V_Delta=0.02490005031384616\n",
      "Episode 16 finished after 3 timesteps. Eplislon=0.5569428234918348. V_Delta=0.0\n",
      "Episode 17 finished after 20 timesteps. Eplislon=0.5459091539334175. V_Delta=0.0\n",
      "Episode 18 finished after 1 timesteps. Eplislon=0.5453632447794842. V_Delta=0.0\n",
      "Episode 19 finished after 84 timesteps. Eplislon=0.5014029397595902. V_Delta=0.0\n",
      "Episode 20 finished after 143 timesteps. Eplislon=0.4345619451717332. V_Delta=0.0\n",
      "Episode 21 finished after 50 timesteps. Eplislon=0.4133577680476022. V_Delta=0.0008388608000000002\n",
      "Episode 22 finished after 1 timesteps. Eplislon=0.4129444102795546. V_Delta=0.0\n",
      "Episode 23 finished after 2 timesteps. Eplislon=0.41211893440340575. V_Delta=0.0\n",
      "Episode 24 finished after 45 timesteps. Eplislon=0.3939757930361214. V_Delta=0.0\n",
      "Episode 25 finished after 22 timesteps. Eplislon=0.3853987301463841. V_Delta=0.010472727272727272\n",
      "Episode 26 finished after 2 timesteps. Eplislon=0.3846283180848215. V_Delta=0.0\n",
      "Episode 27 finished after 11 timesteps. Eplislon=0.3804184978064605. V_Delta=0.0024403223272727276\n",
      "Episode 28 finished after 84 timesteps. Eplislon=0.349753957504439. V_Delta=0.0\n",
      "Episode 29 finished after 20 timesteps. Eplislon=0.34282493457591334. V_Delta=0.0\n",
      "Episode 30 finished after 32 timesteps. Eplislon=0.33202288968461574. V_Delta=0.0\n",
      "Episode 31 finished after 34 timesteps. Eplislon=0.32091840475743094. V_Delta=0.0\n",
      "Episode 32 finished after 43 timesteps. Eplislon=0.3074047815158953. V_Delta=0.0\n",
      "Episode 33 finished after 63 timesteps. Eplislon=0.2886266154377016. V_Delta=0.0020305757460317465\n",
      "Episode 34 finished after 72 timesteps. Eplislon=0.26856630800641784. V_Delta=0.0009102222222222227\n",
      "Episode 35 finished after 12 timesteps. Eplislon=0.26536117873480936. V_Delta=0.0\n",
      "Episode 36 finished after 12 timesteps. Eplislon=0.26219430017947276. V_Delta=0.0\n",
      "Episode 37 finished after 38 timesteps. Eplislon=0.2524130467175491. V_Delta=0.0\n",
      "Episode 38 finished after 105 timesteps. Eplislon=0.22724171731734197. V_Delta=0.0042832457142857145\n",
      "Episode 39 finished after 8 timesteps. Eplislon=0.22543013363724612. V_Delta=0.0\n",
      "Episode 40 finished after 43 timesteps. Eplislon=0.21593744687294092. V_Delta=0.0\n",
      "Episode 41 finished after 9 timesteps. Eplislon=0.21400176548760727. V_Delta=0.0\n",
      "Episode 42 finished after 23 timesteps. Eplislon=0.20913349021874023. V_Delta=0.0\n",
      "Episode 43 finished after 27 timesteps. Eplislon=0.2035596837759133. V_Delta=0.0\n",
      "Episode 44 finished after 81 timesteps. Eplislon=0.1877138485735047. V_Delta=0.0\n",
      "Episode 45 finished after 2 timesteps. Eplislon=0.1873386085902063. V_Delta=0.0\n",
      "Episode 46 finished after 116 timesteps. Eplislon=0.1668107075597524. V_Delta=0.0\n",
      "Episode 47 finished after 2 timesteps. Eplislon=0.16647725295534047. V_Delta=0.0\n",
      "Episode 48 finished after 13 timesteps. Eplislon=0.16432598639897444. V_Delta=0.0\n",
      "Episode 49 finished after 65 timesteps. Eplislon=0.15397952748707622. V_Delta=0.0\n",
      "Episode 50 finished after 49 timesteps. Eplislon=0.14661280597880508. V_Delta=0.0\n",
      "Episode 51 finished after 33 timesteps. Eplislon=0.14185120098835316. V_Delta=0.0\n",
      "Episode 52 finished after 11 timesteps. Eplislon=0.14029861623483292. V_Delta=0.0\n",
      "Episode 53 finished after 41 timesteps. Eplislon=0.13465993635479728. V_Delta=0.0019980487804878047\n",
      "Episode 54 finished after 5 timesteps. Eplislon=0.13398798192646064. V_Delta=0.0\n",
      "Episode 55 finished after 28 timesteps. Eplislon=0.13028652967534501. V_Delta=0.0\n",
      "Episode 56 finished after 3 timesteps. Eplislon=0.1298960608156215. V_Delta=0.0\n",
      "Episode 57 finished after 8 timesteps. Eplislon=0.12886052215370544. V_Delta=0.0\n",
      "Episode 58 finished after 7 timesteps. Eplislon=0.12796120006398387. V_Delta=0.0\n",
      "Episode 59 finished after 8 timesteps. Eplislon=0.1269410862201967. V_Delta=0.0\n",
      "Episode 60 finished after 29 timesteps. Eplislon=0.12331087195805032. V_Delta=0.0\n",
      "Episode 61 finished after 52 timesteps. Eplislon=0.11705952472752763. V_Delta=0.0\n",
      "Episode 62 finished after 2 timesteps. Eplislon=0.1168255227375973. V_Delta=0.0\n",
      "Episode 63 finished after 4 timesteps. Eplislon=0.11635892113259806. V_Delta=0.0\n",
      "Episode 64 finished after 63 timesteps. Eplislon=0.10925100584600911. V_Delta=0.0\n",
      "Episode 65 finished after 71 timesteps. Eplislon=0.1017595336843962. V_Delta=0.0\n",
      "Episode 66 finished after 101 timesteps. Eplislon=0.09197915574462306. V_Delta=0.0\n",
      "Episode 67 finished after 4 timesteps. Eplislon=0.0916117906287544. V_Delta=0.0\n",
      "Episode 68 finished after 25 timesteps. Eplislon=0.08934876984714372. V_Delta=0.0\n",
      "Episode 69 finished after 7 timesteps. Eplislon=0.0887252016582989. V_Delta=0.0\n",
      "Episode 70 finished after 29 timesteps. Eplislon=0.08618787113701235. V_Delta=0.0\n",
      "Episode 71 finished after 9 timesteps. Eplislon=0.08541527583120778. V_Delta=0.0\n",
      "Episode 72 finished after 30 timesteps. Eplislon=0.08288962874392225. V_Delta=0.0\n",
      "Episode 73 finished after 12 timesteps. Eplislon=0.08190040571973876. V_Delta=0.0\n",
      "Episode 74 finished after 16 timesteps. Eplislon=0.08059978156138373. V_Delta=0.0\n",
      "Episode 75 finished after 79 timesteps. Eplislon=0.0744744723018047. V_Delta=0.0\n",
      "Episode 76 finished after 125 timesteps. Eplislon=0.06571938079777004. V_Delta=0.0\n",
      "Episode 77 finished after 21 timesteps. Eplislon=0.06435298785620469. V_Delta=0.0\n",
      "Episode 78 finished after 9 timesteps. Eplislon=0.06377612227551108. V_Delta=0.0\n",
      "Episode 79 finished after 126 timesteps. Eplislon=0.0562224317803237. V_Delta=0.0\n",
      "Episode 80 finished after 1 timesteps. Eplislon=0.056166209348543376. V_Delta=0.0\n",
      "Episode 81 finished after 11 timesteps. Eplislon=0.05555146093830795. V_Delta=0.0\n",
      "Episode 82 finished after 23 timesteps. Eplislon=0.054287733964751464. V_Delta=0.0\n",
      "Episode 83 finished after 50 timesteps. Eplislon=0.051638798089334734. V_Delta=0.0\n",
      "Episode 84 finished after 142 timesteps. Eplislon=0.04479973586526098. V_Delta=0.0\n",
      "Episode 85 finished after 18 timesteps. Eplislon=0.04400015855939337. V_Delta=0.0\n",
      "Episode 86 finished after 3 timesteps. Eplislon=0.043868290040190716. V_Delta=0.0\n",
      "Episode 87 finished after 41 timesteps. Eplislon=0.04210519892026722. V_Delta=0.0\n",
      "Episode 88 finished after 52 timesteps. Eplislon=0.03997064083563731. V_Delta=0.0\n",
      "Episode 89 finished after 158 timesteps. Eplislon=0.0341262250855682. V_Delta=0.0\n",
      "Episode 90 finished after 1 timesteps. Eplislon=0.03409209886048263. V_Delta=0.0\n",
      "Episode 91 finished after 31 timesteps. Eplislon=0.03305094444473369. V_Delta=0.0\n",
      "Episode 92 finished after 67 timesteps. Eplislon=0.030908048516573555. V_Delta=0.0\n",
      "Episode 93 finished after 20 timesteps. Eplislon=0.030295724989556416. V_Delta=0.0\n",
      "Episode 94 finished after 103 timesteps. Eplislon=0.027329182266073036. V_Delta=0.0\n",
      "Episode 95 finished after 42 timesteps. Eplislon=0.026204576333666588. V_Delta=0.0\n",
      "Episode 96 finished after 15 timesteps. Eplislon=0.02581424728178508. V_Delta=0.0\n",
      "Episode 97 finished after 1 timesteps. Eplislon=0.025788433034503296. V_Delta=0.0\n",
      "Episode 98 finished after 65 timesteps. Eplislon=0.024164715638120806. V_Delta=0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 99 finished after 1 timesteps. Eplislon=0.024140550922482684. V_Delta=0.0\n",
      "Episode 100 finished after 286 timesteps. Eplislon=0.018133298459078098. V_Delta=0.0\n"
     ]
    }
   ],
   "source": [
    "train(random_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Method 2:  Monte-Carlo methods for computing value functions and improving policies\n",
    "\n",
    "Monte-Carlo methods are those based on repeated sampling to estimate a quantity. \n",
    "\n",
    "#### Computing value functions using Monte-Carlo methods\n",
    "\n",
    "For this method, we will use Monte-Carlo sampling to estimate the value of each state by running many episodes, and then doing backup from the terminal state.\n",
    "\n",
    "The equation below shows how we can use Monte-Carlo sampling to compute the value function for any given policy.\n",
    "\n",
    "![](./\n",
    "\n",
    "#### Improving the policy using Monte-Carlo methods\n",
    "\n",
    "As with DP, we can use generalised policy iteration to cyclically improve the value function for the current policy and then improve the current policy based on that value function. Again, policy improvement is done by acting greedily with respect to the current value function. However, in this method, we will use Monte-Carlo sampling to find the value function, alternatively to simulation using a model, as described above.\n",
    "\n",
    "#### Finding an optimal policy using Monte-Carlo methods\n",
    "\n",
    "\n",
    "Values will not be updated for states that aren't visited to during an episode.\n",
    "\n",
    "\n",
    "\n",
    "This process of iteratively updating the value function is called **value iteration**.\n",
    "\n",
    "Let's implement an algorithm to find the value function for our random policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now that our agent is capable of exploring and learning about it's environment, we need to make it take advantage of what it knows so that it can perform well.\n",
    "Our random policy has helped us to estimate the values of each state, which means we have some idea of how good each state is. Think about how we could use this knowledge to make our agent perform well before reading the next paragraphs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old transition func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we know exactly what states we can end up in by taking an action, we can just look at the value of the states and choose the action which leads us to the state with the greatest value. So we just move into the best state that we can reach at any point.\n",
    "A policy that always takes the action that it expects to end up in the best, currently reachable state is called a **greedy policy**.\n",
    "\n",
    "Let's implement a greedy policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why not just act greedily all the time?\n",
    "\n",
    "If we act greedily all the time then we will move into the state with the best value. But remember that these values are only estimates based on our agent's experience with the game, which means that they might not be correct. So if we want to make sure that our agent will do well by always choosing the next action greedily, we need to make sure that it has good estimates for the values of those states. This brings us to a core challenge in reinforcement learning: **the exploration vs exploitation dilemma**. Our agent can either exploit what it knows by using it's current knowledge to choose the best action, or it can explore more and improve it's knowledge perhaps learning that some actions are even worse than what it does currently.\n",
    "\n",
    "## An epsilon-greedy policy\n",
    "We can combine our random policy and our greedy policy to make an improved policy that both explores its environment and exploits its current knowledge. An $\\epsilon$-greedy (epsilon-greedy) policy is one which exploits what it knows most of the time, but with probability $\\epsilon$ will instead select a random action to try.\n",
    "\n",
    "## Do we need to keep exploring once we are confident in the values of states?\n",
    "\n",
    "As our agent explores more, it becomes more confident in predicting how valuable any state is. Once it knows a lot, it should start to explore less and exploit what it knows more. That means that we should decrease epsilon over time.\n",
    "\n",
    "Let's implement it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state):\n",
    "    epsilon = 0.05\n",
    "    if random.random() < epsilon:\n",
    "        return random_policy(state)\n",
    "    else:\n",
    "        return greedy_policy(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we find an optimal policy?\n",
    "\n",
    "An optimal policy would take the best possible action in any state. Because of this, the optimal value function would give the maximum possible values for any state.\n",
    "\n",
    "In the first line below, the maximum state-value of a state is equivalent to the maximum action-value when taking the best action in that state. Following this, we can derive a recursive definition of the optimal value function.\n",
    "\n",
    "In the last step, we even remove the policy from the equation entirely! This means that value iteration never needs to explicitly represent a policy in terms of a function that takes in a state and returns a distribution over actions.\n",
    "Instead, value iteration uses a **model**, $p(s', r | s, a)$, to look one step ahead, and take the action, $a$, that most likely leads it to the next state that has the best state-value function.\n",
    "\n",
    "A **model** defines how the state changes. It is also known as the transition dynamics of the environment. In our case the model is really simple: we are certain that taking the action to move right will move our agent one space to the right as long as there are no obstacles. There is no randomness in our environment (e.g. no wind that might push us into a different cell when we try to move right). That is, our environment is deterministic, not stochastic.\n",
    "\n",
    "![](./images/bellman_op_v.png)\n",
    "\n",
    "![](./images/backup_v.png)\n",
    "\n",
    "![](./images/update_rule_v.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement value iteration to find optimal value function\n",
    "def update_value_table(episode_mem, value_table, discount_factor=0.95, alpha=0.5):\n",
    "    return value_table, v_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Code solution with visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from griddy_env import GriddyEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_from_state(state):\n",
    "    key = pickle.dumps(state)\n",
    "    if key not in value_table:\n",
    "        value_table[key]=0 #initialize\n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_value_table(episode_mem, value_table, discount_factor=0.95, learning_rate=0.1):\n",
    "    all_diffs=[]\n",
    "    for i, mem in reversed(list(enumerate(episode_mem))): #start from terminal state\n",
    "        if i==len(episode_mem)-1: #if terminal state, G=reward\n",
    "            calculated_new_v = episode_mem[i]['reward']\n",
    "        else:\n",
    "            calculated_new_v = mem['reward']+(discount_factor*np.max(greedy_policy(mem['new_observation'], return_action_vals=True)))\n",
    "        key = key_from_state(mem['new_observation'])\n",
    "        diff = abs(value_table[key]-calculated_new_v)\n",
    "        all_diffs.append(diff)\n",
    "        value_table[key] =  value_table[key] + learning_rate*(calculated_new_v-value_table[key])\n",
    "    return value_table, np.mean(all_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the transition model aka our model of the environment. Given state and action it predicts next state\n",
    "def transition(state, action):\n",
    "    state = np.copy(state)\n",
    "    agent_pos = list(zip(*np.where(state[2] == 1)))[0]\n",
    "    new_agent_pos = np.array(agent_pos)\n",
    "    if action==0:\n",
    "        new_agent_pos[1]-=1\n",
    "    elif action==1:\n",
    "        new_agent_pos[1]+=1\n",
    "    elif action==2:\n",
    "        new_agent_pos[0]-=1\n",
    "    elif action==3:\n",
    "        new_agent_pos[0]+=1    \n",
    "    new_agent_pos = np.clip(new_agent_pos, 0, 3)\n",
    "\n",
    "    state[2, agent_pos[0], agent_pos[1]] = 0 #moved from this position so it is empty\n",
    "    state[2, new_agent_pos[0], new_agent_pos[1]] = 1 #moved to this position\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(state, return_action_vals=False):\n",
    "    action_values=[]\n",
    "    for test_action in range(4): #for each action\n",
    "        new_state = transition(state, test_action)\n",
    "        key = key_from_state(new_state)\n",
    "        action_values.append(value_table[key])\n",
    "    policy_action = np.argmax(action_values)\n",
    "    if return_action_vals: return action_values\n",
    "    return policy_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state):\n",
    "    action = env.action_space.sample() if np.random.rand()<epsilon else greedy_policy(state)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    return np.random.randint(0, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_table_viz(value_table):\n",
    "    values = np.zeros((4, 4))\n",
    "    base_st = np.zeros((3, 4, 4), dtype=np.int64)\n",
    "    base_st[0, 3, 3]=1\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            test_st = np.copy(base_st)\n",
    "            test_st[2, i, j] = 1\n",
    "            key = pickle.dumps(test_st)\n",
    "            if key in value_table:\n",
    "                val = value_table[key]\n",
    "            else:\n",
    "                val=0\n",
    "            values[i, j] = val\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_agent(policy, value_table=None, n=5):\n",
    "    try:\n",
    "        for trial_i in range(n):\n",
    "            observation = env.reset()\n",
    "            done=False\n",
    "            t=0\n",
    "            while not done:\n",
    "                if value_table: env.render(value_table_viz(value_table))\n",
    "                else: env.render()\n",
    "                policy_action = policy(observation)\n",
    "                observation, reward, done, info = env.step(policy_action)\n",
    "                time.sleep(0.5)\n",
    "                t+=1\n",
    "            env.render()\n",
    "            time.sleep(1.5)\n",
    "            print(\"Episode {} finished after {} timesteps\".format(trial_i, t))\n",
    "        env.close()\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GriddyEnv(4, 4)\n",
    "epsilon = 1\n",
    "i_episode=0\n",
    "discount_factor=0.9\n",
    "learning_rate=0.3\n",
    "value_table = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(policy, n_episodes=100):\n",
    "    global epsilon\n",
    "    global value_table\n",
    "    global i_episode\n",
    "    try:\n",
    "        for _ in range(n_episodes):\n",
    "            observation = env.reset()\n",
    "            episode_mem = []\n",
    "            done=False\n",
    "            t=0\n",
    "            while not done:\n",
    "                env.render()\n",
    "                time.sleep(0.05)\n",
    "                action = policy(observation)\n",
    "                new_observation, reward, done, info = env.step(action)\n",
    "                episode_mem.append({'observation':observation,\n",
    "                                    'action':action,\n",
    "                                    'reward':reward,\n",
    "                                    'new_observation':new_observation,\n",
    "                                    'done':done})\n",
    "                observation=new_observation\n",
    "                t+=1\n",
    "                epsilon*=0.999\n",
    "            value_table, v_delta = update_value_table(episode_mem, value_table, discount_factor, learning_rate)\n",
    "            i_episode+=1\n",
    "            print(\"Episode {} finished after {} timesteps. Eplislon={}. V_Delta={}\".format(i_episode, t, epsilon, v_delta))#, end='\\r')\n",
    "            #print(value_table_viz(value_table))\n",
    "            #print()\n",
    "            env.render(value_table_viz(value_table))\n",
    "            time.sleep(2)\n",
    "        env.close()\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(epsilon_greedy_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current estimates of value\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.51701049, 0.57998881, 0.62474069, 0.67338781],\n",
       "       [0.58888181, 0.6550619 , 0.72840154, 0.80980877],\n",
       "       [0.65598463, 0.72898751, 0.80999937, 0.8999999 ],\n",
       "       [0.70749033, 0.80901724, 0.89982683, 0.99999997]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Current estimates of value')\n",
    "value_table_viz(value_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_agent(greedy_policy, value_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook!\n",
    "\n",
    "Next you might want to check out:\n",
    "- [Policy Gradients]()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
