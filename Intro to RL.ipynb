{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to RL \n",
    "\n",
    "## Problem setup: We want to make an AI that is able to complete a simple video game.\n",
    "\n",
    "### What is the game we are going to start with?\n",
    "In this game, we want our agent (character) to move through the 2D world and reach the goal. At each timestep our agent can to either move up, down, left or right. The agent cannot move into obstacles, and when it reaches the goal, the game ends.\n",
    "\n",
    "# insert video of game being played\n",
    "\n",
    "We are going to use an environment that we built, called Griddy, that works in exactly the same way as other environments provided as part of openAI gym. \n",
    "\n",
    "\n",
    "The main ideas are:\n",
    "<ul>\n",
    "<li>we need to create our environment</li>\n",
    "<li>we need to initialise it by calling `env.reset()`</li>\n",
    "<li>we can increment the simulation by one timestep by calling `env.step(action)`</li>\n",
    "</ul>\n",
    "\n",
    "Check out [openAI gym's docs](http://gym.openai.com/docs/) to see how the environments work in general and in more detail.\n",
    "\n",
    "Let's set up our simulation to train our agent in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import gym\n",
    "from griddy_env import GriddyEnvOneHot\n",
    "import numpy as np\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import random\n",
    "\n",
    "# SET UP THE ENVIRONMENT\n",
    "env = GriddyEnvOneHot()    # create the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once we have an agent in the game, what do we do?\n",
    "\n",
    "Our agent has no idea of how to win the game. It simply observes states that change based on it's actions and receives a reward signal for doing so.\n",
    "So the agent has to learn about the game for itself. Just like a baby learns to interact with it's world by playing with it, our agent has to try random actions to figure out when and why it receives negative or positive rewards.\n",
    "\n",
    "A function which tells the agent what to do in a given state is called a **policy**\n",
    "\n",
    "We need our agent to understand what actions might lead it to achieving high rewards, but it doesn't know anything about how to complete the game yet. So let's set up our environment and implement a random policy that takes in a state and returns a random action for the agent to take.\n",
    "\n",
    "![](./images/policy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENT A RANDOM POLICY\n",
    "def random_policy(state):\n",
    "    return random.randint(0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\n",
      "Episode finished after 17 timesteps.\n",
      "Episode 1\n"
     ]
    }
   ],
   "source": [
    "# WRITE A LOOP FOR THE AGENT TO TRY RANDOM ACTIONS\n",
    "num_episodes = 3\n",
    "\n",
    "try:\n",
    "    for episode_idx in range(num_episodes):\n",
    "        print('Episode', episode_idx)\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        episode_mem = []\n",
    "        t = 0\n",
    "        while not done:\n",
    "            env.render()\n",
    "            action = random_policy(observation)\n",
    "            observation, _, done, info = env.step(action)\n",
    "            t += 1\n",
    "            time.sleep(0.1)\n",
    "        env.render()\n",
    "        #time.sleep(0.5)\n",
    "        print(f\"Episode finished after {t + 1} timesteps.\")\n",
    "    env.close()\n",
    "except KeyboardInterrupt:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## How do we know if we are doing well?\n",
    "\n",
    "When our agent takes this action and moves into a new state, the environment returns it a reward. The reward when it reaches the goal is +1, and 0 everywhere else. The reward that the agent receives at any point can be considered as what it feels in that moment - like pain or pleasure.\n",
    "\n",
    "**However**, the reward doesn't tell the agent how good that move actually was, only whether it sensed anything, and how good or bad that sensation was.\n",
    "\n",
    "E.g.\n",
    "- Our agent might not receive any reward for stepping toward the goal, even though this might be a good move.\n",
    "- A robot might receive a negative reward as it's battery depletes, but still make good progress towards its goal.\n",
    "- A chess playing agent might receive a positive reward for taking an opponent's piece, but make a bad move in doing so by exposing its king to an attack eventually causing it to lose the game.\n",
    "\n",
    "What we really want to know is not the instantaneous reward, but \"How good is the position I'm in right now?\", that is, what amount of reward can our agent get from this point onwards.\n",
    "This future reward is also known as the return.\n",
    "\n",
    "![](./images/undiscounted_return.png)\n",
    "\n",
    "#### Is getting a reward now as good as getting the same reward later?\n",
    "- What if the reward is removed from the game in the next timestep?\n",
    "- Would you rather be rich now or later?\n",
    "- What if a larger reward is introduced and you don't have enough energy to reach both?\n",
    "- What about inflation?\n",
    "\n",
    "It's better to get rewards sooner rather than later.\n",
    "\n",
    "![](./images/decay.png)\n",
    "\n",
    "We can encode this into our goal by using a **discount factor**, $\\gamma \\in [0, 1]$ ($\\gamma$ between 0 and 1). This makes our agent value more immediate rewards more than those which can be reached further in the future. This makes the goal become:\n",
    "\n",
    "![](./images/discounted_return.png)\n",
    "\n",
    "\n",
    "The value of these return values can be defined recursively as shown below.\n",
    "\n",
    "![](./images/recursive_return.png)\n",
    "\n",
    "Because of this, we can calculate the returns by having our agent play one run-through of the game and then *backing-up* through that trajectory, step-by-step, looking forward at what the future reward was from that point.\n",
    "\n",
    "The back up procedure is a way that we can determine the returns for each state that we visited in an episode. The return of a terminal state is always zero, but the terminal state is not the goal, it is the state which our agent transitions into once the episode has finished. We do backup by looking at the final timestep before our agent went into the terminal state - here it is easy to calculate the return. It is simply the reward that we received for moving into this state, because the expected return from the next state (terminal state) is always zero. Then using the recursive expression of returns (above), we can calculate the return for the timestep before that. This can be done recursively until we reach our initial state. At this point we know what all of the returns were for the whole episode.\n",
    "\n",
    "![](./images/backup.png)\n",
    "\n",
    "### So how good *is* each state?\n",
    "In general, the goal of reinforcement learning is to maximise the **expected** future reward. That is, to maximise the expected return from a the current state onwards. The measure of this, is called the *value* of the state (or the state value). A function that predicts this value is called a **state-value function** or **value function**.\n",
    "\n",
    "![](./images/value_def.png)\n",
    "\n",
    "If we had a way to estimate this, then we could look ahead to the state that each action would take us to and take the action which results in us landing in the state with best value. \n",
    "\n",
    "![](./images/follow_values.png)\n",
    "\n",
    "Values will not be updated for states that aren't visited to during an episode.\n",
    "\n",
    "If we initialise the value for each state as zero, and then average the return for each state over many episodes, that average return will converge to the true value of the state for this policy. This process of iteratively updating the value function is called **value iteration**.\n",
    "\n",
    "![](./images/update_values.png)\n",
    "\n",
    "Value iteration is a type of **value based** method. Notice that to learn an optimal policy, we never have to represent it explicitly. There is no function which represents the policy. Instead we just look-ahead and choose the action that maximises the value of the next state.\n",
    "\n",
    "Now that our agent is exploring the environment, let's implement value iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALISE THE ENVIRONMENT\n",
    "### SAME AS PREVIOUS CELL\n",
    "\n",
    "# LOOP TO RUN EPISODES TO EXPLORE THE ENVIRONMENT\n",
    "### SAME AS PREVIOUS CELL\n",
    "\n",
    "    # FOR EACH TIMESTEP, SELECT AN ACTION USING OUR RANDOM POLICY\n",
    "    ### SAME AS PREVIOUS CELL\n",
    "    \n",
    "    # FOR EACH EPISODE RUN THE BACKUP ALGORITHM TO UPDATE THE VALUES\n",
    "    ### IMPLEMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we use the values that we know to perform well?\n",
    "\n",
    "Now that our agent is capable of exploring and learning about it's environment, we need to make it take advantage of what it knows so that it can perform well.\n",
    "Our random policy has helped us to estimate the values of each state, which means we have some idea of how good each state is. Think about how we could use this knowledge to make our agent perform well before reading the next paragraphs.\n",
    "\n",
    "In this simple version of the game, we know exactly what actions will lead us to what states. That means we have a perfect **model** of the environment. A model is a function that tells us how the state will change when we take certain actions. E.g. we know that if the agent tries to move up into an empty space, then that's where it will end up.\n",
    "\n",
    "Because we know exactly what states we can end up in by taking an action, we can just look at the value of the states and choose the action which leads us to the state with the greatest value. So we just move into the best state that we can reach at any point.\n",
    "A policy that always takes the action that it expects to end up in the best, currently reachable state is called a **greedy policy**.\n",
    "\n",
    "Let's implement a greedy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### implement greedy policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why not just act greedily all the time?\n",
    "\n",
    "If we act greedily all the time then we will move into the state with the best value. But remember that these values are only estimates based on our agent's experience with the game, which means that they might not be correct. So if we want to make sure that our agent will do well by always choosing the next action greedily, we need to make sure that it has good estimates for the values of those states. This brings us to a core challenge in reinforcement learning: **the exploration vs exploitation dilemma**. Our agent can either exploit what it knows by using it's current knowledge to choose the best action, or it can explore more and improve it's knowledge perhaps learning that some actions are even worse than what it does currently.\n",
    "\n",
    "## An epsilon-greedy policy\n",
    "We can combine our random policy and our greedy policy to make an improved policy that both explores its environment and exploits its current knowledge. An $\\epsilon$-greedy (epsilon-greedy) policy is one which exploits what it knows most of the time, but with probability $\\epsilon$ will instead select a random action to try.\n",
    "\n",
    "## Do we need to keep exploring once we are confident in the values of states?\n",
    "\n",
    "As our agent explores more, it becomes more confident in predicting how valuable any state is. Once it knows a lot, it should start to explore less and exploit what it knows more. That means that we should decrease epsilon over time.\n",
    "\n",
    "Let's implement it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state):\n",
    "    epsilon = 0.05\n",
    "    if random.random() < epsilon:\n",
    "        return random_policy(state)\n",
    "    else:\n",
    "        return greedy_policy(state)\n",
    "\n",
    "num_episodes = 100\n",
    "\n",
    "# INITIALISE THE ENVIRONMENT\n",
    "### COPY FROM PREVIOUS CODE CELL\n",
    "\n",
    "# LOOP TO RUN EPISODES TO EXPLORE THE ENVIRONMENT\n",
    "### COPY FROM PREVIOUS CODE CELL\n",
    "\n",
    "    # FOR EACH TIMESTEP, SELECT AN ACTION USING OUR EPSILON-GREEDY POLICY\n",
    "    ### IMPLEMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How can we find an optimal policy?\n",
    "\n",
    "An optimal policy would take the best possible action in any state. Because of this, the optimal value function would give the maximum possible values for any state.\n",
    "\n",
    "In the first line below, the maximum state-value of a state is equivalent to the maximum state-action value when taking the best action in that state. Following this, we can derive a recursive definition of the optimal value function.\n",
    "\n",
    "In the last step, we even remove the policy from the equation entirely! This means that value iteration never needs to explicitly represent a policy in terms of a function that takes in a state and returns a distribution over actions.\n",
    "Instead, value iteration uses a **model**, $p(s', r | s, a)$, to look one step ahead, and take the action, $a$, that most likely leads it to the next state that has the best state-value function.\n",
    "\n",
    "A **model** defines how the state changes. It is also known as the transition dynamics of the environment. In our case the model is really simple: we are certain that taking the action to move right will move our agent one space to the right as long as there are no obstacles. There is no randomness in our environment (e.g. no wind that might push us into a different cell when we try to move right). That is, our environment is deterministic, not stochastic.\n",
    "\n",
    "![](./images/bellman_op_v.png)\n",
    "\n",
    "![](./images/backup_v.png)\n",
    "\n",
    "![](./images/update_rule_v.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement value iteration to find optimal value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we don't have a model?\n",
    "\n",
    "Without a model that tells our agent what states it can reach next, it can't look ahead. Not even one step. So the agent won't be able to use value iteration to find an optimal policy. \n",
    "\n",
    "### We cant find out \"how good each state is\" without a model\n",
    "However, if we can find out \"how good each action in each state is\" without a model. A function that predicts this is called the **state-action value function**:\n",
    "\n",
    "![](./images/q_def.png)\n",
    "\n",
    "![](./images/bellman_op_q.png)\n",
    "\n",
    "![](./images/backup_q.png)\n",
    "\n",
    "![](./images/update_rule_q.png)\n",
    "\n",
    "How big is the input space to the state value function V?\n",
    "\n",
    "How much bigger is the input space to the state-action value function Q?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook!\n",
    "\n",
    "Next you might want to check out:\n",
    "- [Policy Gradients]()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
