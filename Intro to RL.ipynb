{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to RL \n",
    "\n",
    "## Problem setup: We want to make an AI that is able to complete a simple video game.\n",
    "\n",
    "### What is the game we are going to start with?\n",
    "In this game, we want our agent (character) to move through the 2D world and reach the goal. At each timestep our agent can to either move up, down, left or right. The agent cannot move into obstacles, and when it reaches the goal, the game ends.\n",
    "\n",
    "![](./images/griddy.gif)\n",
    "\n",
    "We are going to use an environment that we built, called Griddy, that works in exactly the same way as other environments provided as part of openAI gym. \n",
    "\n",
    "\n",
    "The main ideas are:\n",
    "<ul>\n",
    "<li>we need to create our environment</li>\n",
    "<li>we need to initialise it by calling `env.reset()`</li>\n",
    "<li>we can increment the simulation by one timestep by calling `env.step(action)`</li>\n",
    "</ul>\n",
    "\n",
    "Check out [openAI gym's docs](http://gym.openai.com/docs/) to see how the environments work in general and in more detail.\n",
    "\n",
    "Let's set up our simulation to train our agent in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-c5be1fa42853>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-c5be1fa42853>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    env =     # create the environment\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from GriddyEnv import GriddyEnv # make sure you: pip3 install GriddyEnv\n",
    "\n",
    "# SET UP THE ENVIRONMENT\n",
    "env =     ## create the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once we have an agent in the game, what do we do?\n",
    "\n",
    "Our agent has no idea of how to win the game. \n",
    "It simply observes states, and takes actions.\n",
    "As a result of these actions, the agent will see the environment change to a new state and also receive some sensation. This sensation, which may be good or bad, is called a **reward**.\n",
    "and then  that change based on it's actions and receives a reward signal for doing so.\n",
    "\n",
    "This continuous interaction between the agent and our environment sets up the framework for a **reinforcement learning problem**, in an agent-environment loop as shown below.\n",
    "\n",
    "![](images/agent-env-loop.png)\n",
    "\n",
    "So without any prior knowledge, the agent has to learn about the game for itself. Just like a baby learns to interact with it's world by playing with it, our agent has to try random actions in the environment to figure out what causes it to receive negative or positive rewards.\n",
    "\n",
    "A function which tells the agent what actions to take from a given state is called a **policy**.\n",
    "\n",
    "![](./images/policy.png)\n",
    "\n",
    "Policies can be deterministic or stochastic (have randomness).\n",
    "\n",
    "Mathematically, a policy is a probability distribution over actions, conditioned on the state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our RL problem\n",
    "\n",
    "In our case:\n",
    "\n",
    "### Action Space\n",
    "The action space consists of 4 unique actions: 0, 1, 2, 3<br>\n",
    "0 - Move left<br>\n",
    "1 - Move right<br>\n",
    "2 - Move up<br>\n",
    "3 - Move down<br>\n",
    "\n",
    "### Observation Space\n",
    "Has shape (3, 4, 4). Our grid world is 4x4<br>\n",
    "Each of the 3 channels is a binary mask for the location of different objects within the environment.<br>\n",
    "Channel 0 - Goal<br>\n",
    "Channel 1 - Wall<br>\n",
    "Channel 2 - Agent<br>\n",
    "This is what our environment returns us on a state transition.\n",
    "\n",
    "I've simplified the code by including lines that convert this tensor state representation to an integer. This better represents the size of our environment and makes visualisation easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop our agent into the environment, implement a random policy, and then watch it act for a few episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualise agent function\n",
    "def visualise_agent(policy, n=5):\n",
    "    try:\n",
    "        for trial_i in range(n):\n",
    "            observation = env.reset()\n",
    "            done=False\n",
    "            t = 0\n",
    "            while not done:\n",
    "                env.render()\n",
    "                agent_pos = list(zip(*np.where(observation[2] == 1)))[0]\n",
    "                state = 4 * agent_pos[0] + agent_pos[1]\n",
    "                policy_action = policy(state)\n",
    "#                 print('policy_action:', policy_action)\n",
    "                observation, reward, done, info = env.step(policy_action)\n",
    "                time.sleep(0.1)\n",
    "                t += 1\n",
    "            env.render()\n",
    "            time.sleep(1.5)\n",
    "            print(\"Episode {} finished after {} timesteps\".format(trial_i, t))\n",
    "        env.close()\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENT A RANDOM POLICY\n",
    "def random_policy(state):\n",
    "    action = ##\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_agent(random_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we know if we are doing well?\n",
    "\n",
    "When our agent takes this action and moves into a new state, the environment returns it a reward. The reward when it reaches the goal is +1, and 0 everywhere else. The reward that the agent receives at any point can be considered as what it feels in that moment - like pain or pleasure.\n",
    "\n",
    "**However**, the reward doesn't tell the agent how good that move actually was, only whether it sensed anything, and how good or bad that sensation was at that particular moment.\n",
    "\n",
    "E.g.\n",
    "- Our agent might not receive any reward for stepping toward the goal, even though this might be a good move.\n",
    "- A robot might receive a negative reward as it's battery depletes, but still make good progress towards its goal.\n",
    "- A chess playing agent might receive a positive reward for taking an opponent's piece, but make a bad move in doing so by exposing its king to an attack eventually causing it to lose the game.\n",
    "\n",
    "What we really want to know is not the instantaneous reward, but \"How good is the position I'm in right now?\", that is, what amount of reward can our agent get from this point onwards.\n",
    "This future reward is also known as the return.\n",
    "\n",
    "![](./images/undiscounted_return.jpg)\n",
    "\n",
    "#### Is getting a reward now as good as getting the same reward later?\n",
    "- What if the reward is removed from the game in the next timestep?\n",
    "- Would you rather be rich now or later?\n",
    "- What if a larger reward is introduced and you don't have enough energy to reach both?\n",
    "- What about inflation?\n",
    "\n",
    "It's better to get rewards sooner rather than later.\n",
    "\n",
    "![](./images/decay.jpg)\n",
    "\n",
    "We can encode this into our goal by using a **discount factor**, $\\gamma \\in [0, 1]$ ($\\gamma$ between 0 and 1). \n",
    "The discount factor is the coefficient of a reward $t$ timesteps in the future, raised to the power of $t$. Because it is less than 1, raising to the power reduces its value. As such, this coefficient weights rewards further away in the future by a lesser number than those nearby in time.\n",
    "\n",
    "This makes our agent value more immediate rewards more than those which can be reached further in the future.\n",
    "This makes the goal become:\n",
    "\n",
    "![](./images/discounted_return.png)\n",
    "\n",
    "This is called the **discounted return**. From here on, when we say return, we mean discounted return unless otherwise specified, because we rarely use the undiscounted version.\n",
    "\n",
    "Once an agent has played a whole game it can calculate the return for each state it visited by simply adding up the discounted reward it achieved from there on.\n",
    "\n",
    "The value of these return values can also be defined recursively as shown below.\n",
    "\n",
    "![](./images/recursive_return.png)\n",
    " \n",
    "Because of this recursive relationship, we can calculate the experienced returns for each visited state in a single pass through the trajectory of that episode. We do this by working backwards, firstly calculating the  return from the terminal state (always zero), and then recursively calculating the return from the previous timestep by discounting it and adding the reward from that timestep.\n",
    "This process is called ***backup***.\n",
    "\n",
    "The return of a terminal state is always zero, because from there the episode will have terminated and hence the agent will not be able to attain any further reward.\n",
    "\n",
    "![](./images/backup.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value functions - so how good is each state?\n",
    "\n",
    "Now we know how to calculate the return that we experienced during any single episode. \n",
    "But this is just a single sample estimate. \n",
    "In general, the goal of reinforcement learning is to maximise the **expected** future reward. \n",
    "That is, to maximise the average return from a the current state onwards. \n",
    "This quantity is defined as the **state-value** of a state, commonly just referred to as the  or **value** of a state. \n",
    "A function that returns this value given a state is called a value function.\n",
    "\n",
    "![](./images/value_def.jpg)\n",
    "\n",
    "**Note that a value function must correspond to some policy.** If we follow a bad policy then states will have lower values than if we follow a good policy. If we change the policy, then the value function will change\n",
    "\n",
    "### The Bellman equation for $V(S)$\n",
    "\n",
    "Using the recursive definition of the return, we can express the value function recursively.\n",
    "\n",
    "The Bellman equations are those which express value functions recursively.\n",
    "\n",
    "![](./images/value-bellman.jpg)\n",
    "\n",
    "#### Recovering the Bellman optimality question for $V(S)$\n",
    "\n",
    "![](./images/value-optim-deriv.jpg)\n",
    "\n",
    "\n",
    "### The Bellman Optimality equation for $V(S)$\n",
    "\n",
    "For a value function to be better than some other, it must have values greater than or equal to those for the other value function for all states.\n",
    "\n",
    "If we are acting optimally at any given state, then the action we take should consider all actions and then take the one with the best expected return. This return is calculated using the value for the next state.\n",
    "\n",
    "![](./images/value-optim.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we use these tools, and improve our agent's performance?\n",
    "\n",
    "### Method 1: Dynamic Programming (DP) methods for computing value functions and improving policies\n",
    "\n",
    "The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect **model** of the environment.\n",
    "\n",
    "A model tells us how the environment will change when we take certain actions. It may be stochastic or deterministic. A model can allow us to **simulate** the progression of the environment, taking any action from any state.\n",
    "\n",
    "![](./images/model.jpg)\n",
    "\n",
    "**A model will also be referred to as a transition function.**\n",
    "\n",
    "Fortunately (by our design) in this simple version of the game, we do know exactly what actions will lead us to what states. That means we have a perfect **model** of the environment. A model is a function that tells us how the state will change when we take certain actions. E.g. we know that if the agent tries to move up into an empty space, then that's where it will end up.\n",
    "\n",
    "Let's run the cell below to define our transition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSITION FUNCTION/ENVIRONMENT MODEL\n",
    "def transition(state, action):\n",
    "    LEFT = 0\n",
    "    DOWN = 1\n",
    "    RIGHT = 2\n",
    "    UP = 3\n",
    "    \n",
    "    GOAL = 15\n",
    "    nrow = 4\n",
    "    ncol = 4\n",
    "    \n",
    "    if state == GOAL: #they have already reached the goal and need to reset the environment\n",
    "        reward=0\n",
    "        done=1\n",
    "        return state, reward, done\n",
    "    \n",
    "    row = state // 4 # nearest factor of nrows\n",
    "    col = state % 4 # remainder of ncols\n",
    "    if action == LEFT:\n",
    "        new_state = (row, max(col-1, 0))\n",
    "    if action == DOWN:\n",
    "        new_state = (min(row+1, nrow - 1), col)\n",
    "    if action == RIGHT:\n",
    "        new_state = (row, min(col+1, ncol - 1))\n",
    "    if action == UP:\n",
    "        new_state = (max(row-1, 0), col)\n",
    "    new_state = nrow * new_state[0] + new_state[1] # convert back to integer\n",
    "    reward = 0\n",
    "    done=0\n",
    "    if new_state == GOAL: #they have just reached the goal state\n",
    "        reward = 1\n",
    "        done=1\n",
    "    return new_state, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a model, we can look ahead to the successor states reachable from our current state.\n",
    "If we also had a way to estimate the value function, then we could look ahead to the state that each action would take us to and take the action which results in the best expected return.\n",
    "\n",
    "Acting greedily with respect to a value function for an optimal policy will produce optimal behaviour.\n",
    "\n",
    "![](./images/follow_values.jpg)\n",
    "\n",
    "#### Computing value functions using dynamic programming\n",
    "## Algorithm 1: Policy iteration\n",
    "\n",
    "### Step 1: Policy evaluation step\n",
    "\n",
    "Policy evaluation is the process of approximately evaluating the value function for our current policy.\n",
    "\n",
    "How can we do this using our environment model?\n",
    "\n",
    "The Bellman equations define the value of any state recursively, as a function of it successor state.\n",
    "We know that we can use our model to simulate the next states that our agent will move into by taking any given action, and that it defines what rewards we might receive.\n",
    "Given this, along with the fact that our value table already contains estimates\n",
    "\n",
    "### Step 2: Policy improvement step\n",
    "\n",
    "Policy improvement is done by setting the new policy to be greedy with respect to the value function. This means that the new policy will consider all actions,and then take the action that leads to the greatest expected return, based on our current value function.\n",
    "\n",
    "### Full policy iteration algorithm\n",
    "\n",
    "The full policy iteration algorithm iterates between policy evaluation and policy improvement. This alternatively improves the policy by making it greedy with respect to the value function, and then improves the value function by minimising the Bellman error.\n",
    "\n",
    "Let's put these two steps together to produce the full policy iteration algorithm.\n",
    "\n",
    "![](./images/policy-iteration.jpg)\n",
    "\n",
    "### Does this converge to an optimal policy?\n",
    "\n",
    "For a policy $\\pi'$ to better than some other policy $\\pi$?, it must be such that $v_{\\pi'}(s) \\geq v_{\\pi}(s)$ for all states.\n",
    "\n",
    "![](./images/convergence.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_value_table(num_states=16):\n",
    "    value_table = {} # start off with empty map\n",
    "    for s in range(num_states): # for each state\n",
    "        value_table[s] = np.random.rand()\n",
    "    return value_table\n",
    "\n",
    "def initialise_deterministic_policy(num_states=16, num_actions=4):\n",
    "    policy = {} # start off with empty map\n",
    "    for s in range(num_states):\n",
    "        policy[s] = ##choose random action\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTING VALUE FUNCTION USING DYNAMIC PROGRAMMING\n",
    "def policy_evaluation(policy, value_table, discount_factor, error_threshold=0.01, num_states=16):\n",
    "    print()\n",
    "    print(value_table)\n",
    "    new_value_table = value_table # init new value table to be filled in and returned\n",
    "    converged = False # initially we have not found a converged value function for this policy\n",
    "    k = 0 # sweep index\n",
    "    while not converged: # until the value function converges\n",
    "        print('sweep ', k)\n",
    "        k += 1 # increment sweep counter\n",
    "        \n",
    "        worst_delta = 0 # difference between previous values and iterated values\n",
    "        for state in range(num_states): # loop over each state\n",
    "            action =  ## get the action according to current policy\n",
    "            new_state, reward done, =  ## use model to simulate next state and reward\n",
    "            if not done: \n",
    "                new_val =  ## compute new value for non-terminal state\n",
    "            else:\n",
    "                new_val =  ## compute new value for terminal state\n",
    "            new_value_table[state] =  ## update value table\n",
    "            delta =  ## find the absolute diff between new val and old val for this state\n",
    "\n",
    "            # CHECK WORST VALUE FUNCTION ERROR\n",
    "            if delta > worst_delta: # is this state the one for which our value table is most wrong?\n",
    "                worst_delta = delta # update worst error for this sweep\n",
    "                print('worst_delta:', worst_delta)\n",
    "                \n",
    "        # CHECK CONVERGED\n",
    "        if worst_delta < error_threshold: # once the values stop changing\n",
    "            converged = ## we have found the value function\n",
    "            print('Converged on value function')\n",
    "        value_table =  # update value table (took me ages to realise i was missing this line and debug )\n",
    "    return new_value_table # return converged value table evaluated for this policy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVE POLICY\n",
    "def policy_improvement(value_table, discount_factor): # set a greedy policy which will always be better than the previous\n",
    "    new_policy = {} # initialise empty new policy to be filled and returned\n",
    "    action_space = range(4) # \n",
    "    for  ## loop over each state\n",
    "        best_value =  ## initialise best value as negative infinity\n",
    "        best_action =  ## no best action found yet\n",
    "        for : ## set the policy as greedy with respect to the value function\n",
    "            new_state, reward = ## use model to simulate next state and reward\n",
    "            if not done: \n",
    "                value =  ## compute value for non-terminal state\n",
    "            else:\n",
    "                value =  ## compute value for terminal state\n",
    "            \n",
    "            # CHECK MAX VALUE\n",
    "            if value > best_value: # checking all actions, which gives this state the best value?\n",
    "                best_value = value # update best value\n",
    "                best_action = action # update best action found so far\n",
    "                \n",
    "        # SET GREEDY POLICY\n",
    "        new_policy[state] =  ## update new policy to take best action found when it sees this state\n",
    "    return new_policy\n",
    "\n",
    "def check_stable_policy(old_policy, new_policy):\n",
    "    stable = True\n",
    "    # CHECK STABLE POLICY\n",
    "    return stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# POLICY ITERATION ALGORITHM\n",
    "def policy_iteration(discount_factor=0.9):\n",
    "    value_table = ## initiailise value table\n",
    "    policy = ## initialise deterministic policy\n",
    "    policy_stable = ## initialise false\n",
    "    policy_idx = 0 # outer to check how many policies we've tried\n",
    "    while not policy_stable: # until convergence\n",
    "        \n",
    "        # POLICY ITERATION\n",
    "        print('Evaluating policy ', policy_idx)\n",
    "        value_table =  ## converge on value function\n",
    "        \n",
    "        # POLICY IMPROVEMENT\n",
    "        print('Iterating policy ', policy_idx)\n",
    "        new_policy =  ## get greedy policy using converged value function\n",
    "        \n",
    "        # CHECK CONVERGENCE\n",
    "        if check_stable_policy(policy, new_policy): # compare policies\n",
    "            policy_stable = True # when they have the same greedy action for each state in the state space\n",
    "            print('Policy now stable - optimal policy found')\n",
    "            \n",
    "        policy =  # update policy\n",
    "        \n",
    "    print('Optimal policy:', policy)\n",
    "    return policy, value_table\n",
    "        \n",
    "optimal_policy, optimal_value = policy_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_map_to_func(policy_map): # turn our dict into a function\n",
    "    def policy(k):\n",
    "        return policy_map[k]    \n",
    "    return policy\n",
    "\n",
    "optimal_policy_func = policy_map_to_func(optimal_policy)\n",
    "visualise_agent(optimal_policy_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the output policy is optimal, but it is not the only optimal policy. For this map it's easy to see that there could be alternatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 2: Value iteration - forget the explicit policy\n",
    "\n",
    "Value iteration is very similar to policy iteration - but we only perform a single sweep over the state space when updating our value function, instead of repeating this until our approximate value function converges for the current policy.\n",
    "\n",
    "The policy evaluation phase requires us to sweep over the entire state space repeatedly. If the state space is large, then this can be computationally expensive.\n",
    "\n",
    "It also turns out that it's not necessary.\n",
    "\n",
    "Every time we perform a sweep, the value function gets closer to the true value function for the current policy. \n",
    "It can be seen that improved policies are found by greedily following value functions that result from truncated policy evaluations.\n",
    "\n",
    "Additionally, if we are greedily following the value function rather than querying the policy each timestep then we don't need to represent the policy explicitly. \n",
    " \n",
    "\n",
    "Notice that in this case, to learn an optimal policy, we never have to represent it explicitly. There is no function which represents the policy. Instead we just look-ahead and choose the action that maximises the value of the next state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where does value iteration come from?\n",
    "\n",
    "In the first line below, the maximum state-value of a state is equivalent to the maximum action-value when taking the best action in that state. Following this, we can derive a recursive definition of the optimal value function.\n",
    "\n",
    "In the last step, we even remove the policy from the equation entirely! This means that value iteration never needs to explicitly represent a policy in terms of a function that takes in a state and returns a distribution over actions.\n",
    "Instead, value iteration uses a **model**, $p(s', r | s, a)$, to look one step ahead, and take the action, $a$, that most likely leads it to the next state that has the best state-value function.\n",
    "\n",
    "A **model** defines how the state changes. It is also known as the transition dynamics of the environment. In our case the model is really simple: we are certain that taking the action to move right will move our agent one space to the right as long as there are no obstacles. There is no randomness in our environment (e.g. no wind that might push us into a different cell when we try to move right). That is, our environment is deterministic, not stochastic.\n",
    "\n",
    "![](./images/bellman_op_v.png)\n",
    "\n",
    "![](./images/backup_v.png)\n",
    "\n",
    "![](./images/update_rule_v.png)  \n",
    "\n",
    "Let's now implement value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(num_states=16, num_actions=4, error_threshold=0.01, discount_factor=0.9):\n",
    "    converged = False\n",
    "    value_table = initialise_value_table()\n",
    "    while not converged:\n",
    "        new_value_table = {}\n",
    "        \n",
    "        # POLICY EVALUATION WITH CONSTANT IMPLICIT POLICY UPDATES\n",
    "        worst_delta = 0\n",
    "        for state in range(num_states): # sweep once over state space\n",
    "            \n",
    "            # FIND BEST ACTION FOR THIS STATE BASED ON CURRENT VALUE TABLE\n",
    "            best_action = None\n",
    "            best_value = -float('inf')\n",
    "            for action in range(num_actions): # max over actions\n",
    "                new_state, reward, done = ## simulate next state\n",
    "                if not done: \n",
    "                    new_val =  ## compute new value for non-terminal state\n",
    "                else:\n",
    "                    new_val =  ## compute new value for terminal state\n",
    "                if new_val > best_value:\n",
    "                    best_action = action\n",
    "                    best_value = new_val\n",
    "            \n",
    "            # CHECK ERROR\n",
    "            delta = ## check value change for each state\n",
    "            worst_delta = max(delta, worst_delta)\n",
    "            \n",
    "            new_value_table[state] = best_value # update value table greedily\n",
    "\n",
    "        value_table = new_value_table\n",
    "        \n",
    "        # NOTE THAT THERE IS NO POLICY IMPROVEMENT STEP - IT'S DONE IMPLICITLY BY MAXIMISING VALUES OVER ACTIONS\n",
    "        \n",
    "        # CHECK CONVERGENCE\n",
    "        if worst_delta < : # check if worst delta below threshold\n",
    "            print('Converged')\n",
    "            converged = True\n",
    "    \n",
    "    # RETURN DETERMINISTIC POLICY - at this point, the algorithm has completed its job\n",
    "    # now the value function should be optimal, and hence correspond to an optimal policy which we can extract\n",
    "    output_policy = {}\n",
    "    for state in range(num_states):    \n",
    "        best_action = None\n",
    "        best_value = -float('inf')\n",
    "        for action in range(num_actions): # max over actions\n",
    "            new_state, reward, done = ##\n",
    "            if not done: \n",
    "                new_val =  ## compute new value for non-terminal state\n",
    "            else:\n",
    "                new_val =  ## compute new value for terminal state\n",
    "            if new_val > best_value:\n",
    "                best_action = action\n",
    "                best_value = new_val\n",
    "        output_policy[state] = best_action\n",
    "    return output_policy\n",
    "\n",
    "optimal_policy = value_iteration()\n",
    "optimal_policy_func = policy_map_to_func(optimal_policy)\n",
    "visualise_agent(optimal_policy_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Value iteration and policy iteration are types of **value based** method - they use a value or Q function to find an optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawbacks of Dynamic Programming\n",
    "#### What if we aren't fortunate enough to have a model?\n",
    "We very rarely have access to a transition model for the environment.\n",
    "\n",
    "#### Policy evaluation requires sweeping over entire states\n",
    "\n",
    "Backgammon has over $10^{20}$ states. Performing a sweep over this many states is extremely computationally expensive. \n",
    "Some of these states are very rare. Some might never actually be seen through experience.\n",
    "How can we continue to evaluate policies without needing this sweep?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook!\n",
    "\n",
    "Next you might want to check out:\n",
    "- [Q Learning](https://github.com/AI-Core/Reinforcement-Learning/blob/master/Q%20Learning.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
