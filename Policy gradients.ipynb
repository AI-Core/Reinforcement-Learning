{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients\n",
    "\n",
    "Prerequisites:\n",
    "- [Value based methods](https://theaicore.com/app/training/intro-to-rl)\n",
    "- [Neural Networks](https://theaicore.com/app/training/neural-networks)\n",
    "\n",
    "Previously we looked at value based methods; those which estimated how good certain states were (value function) and how good certain actions were from certain states (action-value or Q function).\n",
    "\n",
    "In this notebook we'll look at policy gradient based methods\n",
    "\n",
    "## What's the goal of reinforcement learning?\n",
    "\n",
    "The goal of a reinforcement learning agent is to maximise expected reward over it's lifetime.\n",
    "What the agent experiences over it's lifetime, including rewards, states and actions defines it's *trajectory*.\n",
    "The trajectories that an agent might experience depend on what actions it takes from any given state, that is, what policy the agent follows.\n",
    "\n",
    "We can formulate this as below.\n",
    "\n",
    "![](./images/policy-gradient-objective.jpg)\n",
    "\n",
    "Where the policy is a function with parameters $\\theta$.\n",
    "\n",
    "What we'd like to do, is to find parameters that maximise this objective, $J$, and hence find an optimal parameterisation for our poilcy.\n",
    "\n",
    "Because the objective if fully differentiable, we can use gradient **ascent** to improve our objective with respect to our parameters.\n",
    "\n",
    "Below we analytically derive the gradient of the objective with respect to the parameters.\n",
    "\n",
    "![](./images/policy-gradient-derivation.jpg)\n",
    "\n",
    "Now we can use this derivative in our gradient ascent update rule to adjust the weights in a direction that should increase the objective. \n",
    "Note the update is in the direction of the gradient because this is gradient ascent, not descent. \n",
    "That's because the objective represents our expected reward which we want to maximise, rather than a loss which we might want to minimise in a different case.\n",
    "\n",
    "![](./images/policy-gradient-update.jpg)\n",
    "\n",
    "This algorithm is called REINFORCE (REward Increment = Nonnegative Factor $\\times$ Offset Reinforcement $\\times$ Characteristic Eligibility). This name describes the structure of the parameter updates. But don't worry about the acronym.\n",
    "\n",
    "Let's build a neural network which will act as our agent's policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, layers, embedding=False, distribution=False):\n",
    "        super().__init__()\n",
    "        l = []\n",
    "        for idx in range(len(layers) - 1):\n",
    "            l.append(torch.nn.Linear(layers[idx], layers[idx+1]))   # add a linear layer\n",
    "            if idx + 1 != len(layers) - 1: # if this is not the last layer ( +1 = zero indexed) (-1 = layer b4 last)\n",
    "                l.append(torch.nn.ReLU())   # activate\n",
    "        if distribution:    # if a probability dist output is required\n",
    "            l.append(torch.nn.Softmax())    # apply softmax to output\n",
    "            \n",
    "        self.layers = torch.nn.Sequential(*l) # unpack layers & turn into a function which applies them sequentially \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this neural network to model the policy which will control our agent in Griddy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from GriddyEnv import GriddyEnv # get griddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 AVG REWARD: -93.03\n",
      "EPOCH: 1 AVG REWARD: -34.27\n",
      "EPOCH: 2 AVG REWARD: -37.60\n",
      "EPOCH: 3 AVG REWARD: -28.40\n",
      "EPOCH: 4 AVG REWARD: -23.20\n",
      "EPOCH: 5 AVG REWARD: -34.77\n",
      "EPOCH: 6 AVG REWARD: -27.90\n",
      "EPOCH: 7 AVG REWARD: -20.43\n",
      "EPOCH: 8 AVG REWARD: -29.10\n",
      "EPOCH: 9 AVG REWARD: -14.50\n",
      "EPOCH: 10 AVG REWARD: -19.63\n",
      "EPOCH: 11 AVG REWARD: -17.03\n",
      "EPOCH: 12 AVG REWARD: -13.20\n",
      "EPOCH: 13 AVG REWARD: -19.77\n",
      "EPOCH: 14 AVG REWARD: -15.37\n",
      "EPOCH: 15 AVG REWARD: -14.67\n",
      "EPOCH: 16 AVG REWARD: -17.23\n",
      "EPOCH: 17 AVG REWARD: -16.83\n",
      "EPOCH: 18 AVG REWARD: -14.13\n",
      "EPOCH: 19 AVG REWARD: -12.13\n",
      "EPOCH: 20 AVG REWARD: -9.50\n",
      "EPOCH: 21 AVG REWARD: -6.37\n",
      "EPOCH: 22 AVG REWARD: -10.67\n",
      "EPOCH: 23 AVG REWARD: -11.27\n",
      "EPOCH: 24 AVG REWARD: -10.17\n",
      "EPOCH: 25 AVG REWARD: -7.27\n",
      "EPOCH: 26 AVG REWARD: -10.57\n",
      "EPOCH: 27 AVG REWARD: -10.13\n",
      "EPOCH: 28 AVG REWARD: -11.37\n",
      "EPOCH: 29 AVG REWARD: -8.27\n"
     ]
    }
   ],
   "source": [
    "def train(env, optimiser, agent_tag, epochs=100, episodes=30, use_baseline=False, use_causality=False):\n",
    "    assert not (use_baseline and use_causality)   # cant implement both simply\n",
    "    baseline = 0\n",
    "    try:\n",
    "        for epoch in range(epochs):\n",
    "            avg_reward = 0\n",
    "            objective = 0\n",
    "            for episode in range(episodes):\n",
    "                done = False\n",
    "                state = env.reset()\n",
    "                log_policy = []\n",
    "\n",
    "                rewards = []\n",
    "\n",
    "                step = 0\n",
    "\n",
    "                # RUN AN EPISODE\n",
    "                while not done:     # while the episode is not terminated\n",
    "                    state = torch.Tensor(state)     # correct data type for passing to model\n",
    "                    # print('STATE:', state)\n",
    "                    state = state.view(np.prod(state.shape))\n",
    "\n",
    "                    action_distribution = policy(state)     # get a distribution over actions from the policy given the state\n",
    "                    # print('ACTION DISTRIBUTION:', action_distribution)\n",
    "\n",
    "                    action = torch.distributions.Categorical(action_distribution).sample()      # sample from that distrbution\n",
    "                    action = int(action)\n",
    "                    # print('ACTION:', action)\n",
    "\n",
    "                    new_state, reward, done, info = env.step(action)    # take timestep\n",
    "\n",
    "                    rewards.append(reward)\n",
    "\n",
    "                    state = new_state\n",
    "                    log_policy.append(torch.log(action_distribution[action]))\n",
    "\n",
    "                    step += 1\n",
    "                    if done:\n",
    "                        break\n",
    "                    if step > 10000000:\n",
    "                        # break\n",
    "                        pass\n",
    "\n",
    "                avg_reward += ( sum(rewards) - avg_reward ) / ( episode + 1 )   # accumulate avg reward\n",
    "                writer.add_scalar(f'{agent_tag}/Reward/Train', avg_reward, epoch*episodes + episode)     # plot the latest reward\n",
    "\n",
    "                # update baseline\n",
    "                if use_baseline:\n",
    "                    baseline += ( sum(rewards) - baseline ) / (epoch*episodes + episode + 1)    # accumulate average return  \n",
    "\n",
    "                for idx in range(len(rewards)):     # for each timestep experienced in the episode\n",
    "                    # add causality\n",
    "                    if use_causality:   \n",
    "                        weight = sum(rewards[idx:])     # only weight the log likelihood of this action by the future rewards, not the total\n",
    "                    else:\n",
    "                        weight = sum(rewards) - baseline           # weight by the total reward from this episode\n",
    "                    objective += log_policy[idx] * weight   # add the weighted log likelihood of this taking action to \n",
    "\n",
    "\n",
    "            objective /= episodes   # average over episodes\n",
    "            objective *= -1     # invert to represent reward rather than cost\n",
    "\n",
    "\n",
    "            # UPDATE POLICY\n",
    "            # print('updating policy')\n",
    "            print('EPOCH:', epoch, f'AVG REWARD: {avg_reward:.2f}')\n",
    "            objective.backward()    # backprop\n",
    "            optimiser.step()    # update params\n",
    "            optimiser.zero_grad()   # reset gradients to zero\n",
    "\n",
    "            # VISUALISE AT END OF EPOCH AFTER UPDATING POLICY\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                env.render()\n",
    "                state = torch.Tensor(state)\n",
    "                state = state.view(np.prod(state.shape))\n",
    "                action_distribution = policy(state)\n",
    "                action = torch.distributions.Categorical(action_distribution).sample()\n",
    "                action = int(action)\n",
    "                state, reward, done, info = env.step(action)\n",
    "                sleep(0.01)\n",
    "    except KeyboardInterrupt:\n",
    "        print('interrupted')\n",
    "        env.close()\n",
    "\n",
    "    env.close()\n",
    "    torch.save(policy.state_dict(), f'trained-agent-{agent_tag}.pt')\n",
    "\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env = GriddyEnv(time_penalty=True)\n",
    "\n",
    "policy = NN([np.prod(env.observation_space.shape), 32, env.action_space.n], distribution=True)\n",
    "\n",
    "lr = 0.001\n",
    "weight_decay = 1\n",
    "optimiser = torch.optim.SGD(policy.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "agent_tag = 'cartpole'\n",
    "\n",
    "train(\n",
    "    env,\n",
    "    optimiser,\n",
    "    agent_tag,\n",
    "    use_baseline=True,\n",
    "    use_causality=False,\n",
    "    epochs=30,\n",
    "    episodes=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we solve a harder challenge?\n",
    "\n",
    "Now let's move onto a more challenging environment called CartPole. The aim is to have a cart move along one dimension and balance an inverted pole vertically. Note that because we set up the size of the policy network programmatically, we can use exactly the same training loop! This is the first time we've used the same learning algorithm to solve different environments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 AVG REWARD: 20.17\n",
      "EPOCH: 1 AVG REWARD: 23.40\n",
      "EPOCH: 2 AVG REWARD: 23.10\n",
      "EPOCH: 3 AVG REWARD: 22.13\n",
      "EPOCH: 4 AVG REWARD: 25.30\n",
      "EPOCH: 5 AVG REWARD: 29.23\n",
      "EPOCH: 6 AVG REWARD: 26.50\n",
      "EPOCH: 7 AVG REWARD: 27.53\n",
      "EPOCH: 8 AVG REWARD: 24.93\n",
      "EPOCH: 9 AVG REWARD: 25.07\n",
      "EPOCH: 10 AVG REWARD: 26.83\n",
      "EPOCH: 11 AVG REWARD: 31.00\n",
      "EPOCH: 12 AVG REWARD: 27.47\n",
      "EPOCH: 13 AVG REWARD: 33.70\n",
      "EPOCH: 14 AVG REWARD: 35.33\n",
      "EPOCH: 15 AVG REWARD: 33.03\n",
      "EPOCH: 16 AVG REWARD: 34.33\n",
      "EPOCH: 17 AVG REWARD: 34.83\n",
      "EPOCH: 18 AVG REWARD: 37.27\n",
      "EPOCH: 19 AVG REWARD: 40.03\n",
      "EPOCH: 20 AVG REWARD: 30.70\n",
      "EPOCH: 21 AVG REWARD: 38.53\n",
      "EPOCH: 22 AVG REWARD: 43.37\n",
      "EPOCH: 23 AVG REWARD: 39.80\n",
      "EPOCH: 24 AVG REWARD: 33.20\n",
      "EPOCH: 25 AVG REWARD: 43.83\n",
      "EPOCH: 26 AVG REWARD: 39.00\n",
      "EPOCH: 27 AVG REWARD: 50.77\n",
      "EPOCH: 28 AVG REWARD: 48.63\n",
      "EPOCH: 29 AVG REWARD: 44.93\n",
      "EPOCH: 30 AVG REWARD: 46.77\n",
      "EPOCH: 31 AVG REWARD: 53.90\n",
      "EPOCH: 32 AVG REWARD: 49.67\n",
      "EPOCH: 33 AVG REWARD: 44.60\n",
      "EPOCH: 34 AVG REWARD: 54.27\n",
      "EPOCH: 35 AVG REWARD: 59.60\n",
      "EPOCH: 36 AVG REWARD: 64.93\n",
      "EPOCH: 37 AVG REWARD: 59.47\n",
      "EPOCH: 38 AVG REWARD: 49.90\n",
      "EPOCH: 39 AVG REWARD: 59.73\n",
      "EPOCH: 40 AVG REWARD: 59.83\n",
      "EPOCH: 41 AVG REWARD: 59.40\n",
      "EPOCH: 42 AVG REWARD: 50.97\n",
      "EPOCH: 43 AVG REWARD: 65.07\n",
      "EPOCH: 44 AVG REWARD: 69.50\n",
      "EPOCH: 45 AVG REWARD: 64.37\n",
      "EPOCH: 46 AVG REWARD: 48.43\n",
      "EPOCH: 47 AVG REWARD: 59.60\n",
      "EPOCH: 48 AVG REWARD: 54.47\n",
      "EPOCH: 49 AVG REWARD: 71.33\n",
      "EPOCH: 50 AVG REWARD: 68.17\n",
      "EPOCH: 51 AVG REWARD: 72.67\n",
      "EPOCH: 52 AVG REWARD: 50.70\n",
      "EPOCH: 53 AVG REWARD: 91.60\n",
      "EPOCH: 54 AVG REWARD: 36.57\n",
      "EPOCH: 55 AVG REWARD: 52.93\n",
      "EPOCH: 56 AVG REWARD: 88.60\n",
      "EPOCH: 57 AVG REWARD: 59.17\n",
      "EPOCH: 58 AVG REWARD: 95.77\n",
      "EPOCH: 59 AVG REWARD: 103.93\n",
      "EPOCH: 60 AVG REWARD: 104.27\n",
      "EPOCH: 61 AVG REWARD: 26.80\n",
      "EPOCH: 62 AVG REWARD: 29.53\n",
      "EPOCH: 63 AVG REWARD: 33.20\n",
      "EPOCH: 64 AVG REWARD: 36.00\n",
      "EPOCH: 65 AVG REWARD: 37.97\n",
      "EPOCH: 66 AVG REWARD: 56.67\n",
      "EPOCH: 67 AVG REWARD: 114.47\n",
      "EPOCH: 68 AVG REWARD: 54.17\n",
      "EPOCH: 69 AVG REWARD: 115.60\n",
      "EPOCH: 70 AVG REWARD: 41.80\n",
      "EPOCH: 71 AVG REWARD: 62.70\n",
      "EPOCH: 72 AVG REWARD: 114.57\n",
      "EPOCH: 73 AVG REWARD: 55.27\n",
      "EPOCH: 74 AVG REWARD: 117.27\n",
      "EPOCH: 75 AVG REWARD: 60.83\n",
      "EPOCH: 76 AVG REWARD: 145.07\n",
      "EPOCH: 77 AVG REWARD: 149.87\n",
      "EPOCH: 78 AVG REWARD: 34.93\n",
      "EPOCH: 79 AVG REWARD: 37.00\n",
      "EPOCH: 80 AVG REWARD: 54.63\n",
      "EPOCH: 81 AVG REWARD: 102.37\n",
      "EPOCH: 82 AVG REWARD: 168.80\n",
      "EPOCH: 83 AVG REWARD: 84.63\n",
      "EPOCH: 84 AVG REWARD: 119.30\n",
      "EPOCH: 85 AVG REWARD: 164.97\n",
      "EPOCH: 86 AVG REWARD: 154.37\n",
      "EPOCH: 87 AVG REWARD: 192.03\n",
      "EPOCH: 88 AVG REWARD: 83.20\n",
      "EPOCH: 89 AVG REWARD: 157.53\n",
      "interrupted\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter() # create new tensorboard writer\n",
    "\n",
    "env = gym.make('CartPole-v0') # make cartpole environment\n",
    "\n",
    "policy = NN([np.prod(env.observation_space.shape), 32, env.action_space.n], distribution=True) \n",
    "\n",
    "lr = 0.001\n",
    "weight_decay = 1\n",
    "optimiser = torch.optim.SGD(policy.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "train(\n",
    "    env,\n",
    "    optimiser,\n",
    "    use_baseline=True,\n",
    "    use_causality=False,\n",
    "    epochs=300,\n",
    "    episodes=30\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we improve this learning algorithm?\n",
    "\n",
    "What we just implemented is the vanilla policy gradient algorithm.\n",
    "\n",
    "Notice anything that you think could be improved?\n",
    "\n",
    "What are we weighting the likelihood of taking each trajectory by?\n",
    "\n",
    "### Baselines\n",
    "\n",
    "What if all of the trajectories receive a similar total reward (e.g. \"*bad*\" trajectories give 99 total reward and \"good\" trajectories give 100 total reward)?\n",
    "In this case the likelihood of all trajectories will be increased.\n",
    "\n",
    "What if the total reward over every trajectory is negative (e.g. negative reward every timestep and zero upon reaching a terminal state)?\n",
    "In any of these cases the log probability of ANY trajectory taken will be reduced by the updates.\n",
    "\n",
    "We don't want any of these things to happen, so we can introduce baselines. Baselines help you to see the reward from the current trajectory in the context of the reward from each of the others, giving you a relative measure of how good they were, not just an absolute measure.\n",
    "\n",
    "![](./images/policy-gradient-baseline.jpg)\n",
    "\n",
    "A pretty standard baseline to use is the average baseline. But there are others.\n",
    "\n",
    "![](./images/policy-gradient-average-baseline.jpg)\n",
    "\n",
    "Even though we adjust the objective, it remains unbiased in expectation because the expectation of the baseline is zero\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causality\n",
    "What rewards can an action take responsibilty for?\n",
    "Surely an reward received before an action taken later in the trajectory shouldn't indicate that the later action was good. Whatever the action taken later in time was, this reward was received before then.\n",
    "\n",
    "To account for this, we should only weight how good the action was by the rewards which it led the agent to receive from that point in time onwards. The rewards attained before the action was taken are not eligible for making that action more likely; that action was taken after the reward was received, so it can't be accountable for it.\n",
    "\n",
    "![](./images/policy-gradient-causality.jpg)\n",
    "### Can we combine both?\n",
    "To combine both, we would need to have a baseline for each point in time; how much reward can I expect to get from timestep t? For any games without a fixed episode length, or even worse games with an infinite horizon, we'll need to compute a baseline for each timestep. This makes combining causality and baselines difficult, but we'll see how to achieve this in a later notebook, combining policy gradients with some things we've learnt in previous notebooks. (hint: what function represents the same thing as a lookup table for baselines?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now add options to our training function for our algorithm to use baselines and causality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 AVG REWARD: 18.23\n",
      "EPOCH: 1 AVG REWARD: 20.83\n",
      "EPOCH: 2 AVG REWARD: 23.97\n",
      "EPOCH: 3 AVG REWARD: 23.43\n",
      "EPOCH: 4 AVG REWARD: 28.77\n",
      "EPOCH: 5 AVG REWARD: 23.80\n",
      "EPOCH: 6 AVG REWARD: 29.03\n",
      "EPOCH: 7 AVG REWARD: 25.03\n",
      "EPOCH: 8 AVG REWARD: 22.03\n",
      "EPOCH: 9 AVG REWARD: 33.87\n",
      "EPOCH: 10 AVG REWARD: 25.60\n",
      "EPOCH: 11 AVG REWARD: 34.30\n",
      "EPOCH: 12 AVG REWARD: 32.47\n",
      "EPOCH: 13 AVG REWARD: 39.87\n",
      "EPOCH: 14 AVG REWARD: 33.80\n",
      "EPOCH: 15 AVG REWARD: 35.47\n",
      "EPOCH: 16 AVG REWARD: 35.70\n",
      "EPOCH: 17 AVG REWARD: 39.47\n",
      "EPOCH: 18 AVG REWARD: 38.33\n",
      "EPOCH: 19 AVG REWARD: 36.50\n",
      "EPOCH: 20 AVG REWARD: 46.73\n",
      "EPOCH: 21 AVG REWARD: 48.77\n",
      "EPOCH: 22 AVG REWARD: 50.90\n",
      "EPOCH: 23 AVG REWARD: 44.70\n",
      "EPOCH: 24 AVG REWARD: 54.93\n",
      "EPOCH: 25 AVG REWARD: 52.53\n",
      "EPOCH: 26 AVG REWARD: 57.03\n",
      "EPOCH: 27 AVG REWARD: 58.17\n",
      "EPOCH: 28 AVG REWARD: 43.47\n",
      "EPOCH: 29 AVG REWARD: 68.50\n"
     ]
    }
   ],
   "source": [
    "def train(env, optimiser, agent_tag, epochs=100, episodes=30, use_baseline=False, use_causality=False):\n",
    "    assert not (use_baseline and use_causality)   # cant implement both simply\n",
    "    baseline = 0\n",
    "    try:\n",
    "        for epoch in range(epochs):\n",
    "            avg_reward = 0\n",
    "            objective = 0\n",
    "            for episode in range(episodes):\n",
    "                done = False\n",
    "                state = env.reset()\n",
    "                log_policy = []\n",
    "\n",
    "                rewards = []\n",
    "\n",
    "                step = 0\n",
    "\n",
    "                # RUN AN EPISODE\n",
    "                while not done:     # while the episode is not terminated\n",
    "                    state = torch.Tensor(state)     # correct data type for passing to model\n",
    "                    # print('STATE:', state)\n",
    "                    state = state.view(np.prod(state.shape))\n",
    "\n",
    "                    action_distribution = policy(state)     # get a distribution over actions from the policy given the state\n",
    "                    # print('ACTION DISTRIBUTION:', action_distribution)\n",
    "\n",
    "                    action = torch.distributions.Categorical(action_distribution).sample()      # sample from that distrbution\n",
    "                    action = int(action)\n",
    "                    # print('ACTION:', action)\n",
    "\n",
    "                    new_state, reward, done, info = env.step(action)    # take timestep\n",
    "\n",
    "                    rewards.append(reward)\n",
    "\n",
    "                    state = new_state\n",
    "                    log_policy.append(torch.log(action_distribution[action]))\n",
    "\n",
    "                    step += 1\n",
    "                    if done:\n",
    "                        break\n",
    "                    if step > 10000000:\n",
    "                        # break\n",
    "                        pass\n",
    "\n",
    "                avg_reward += ( sum(rewards) - avg_reward ) / ( episode + 1 )   # accumulate avg reward\n",
    "                writer.add_scalar(f'{agent_tag}/Reward/Train', avg_reward, epoch*episodes + episode)     # plot the latest reward\n",
    "\n",
    "                # update baseline\n",
    "                if use_baseline:\n",
    "                    baseline += ( sum(rewards) - baseline ) / (epoch*episodes + episode + 1)    # accumulate average return  \n",
    "\n",
    "                for idx in range(len(rewards)):     # for each timestep experienced in the episode\n",
    "                    # add causality\n",
    "                    if use_causality:   \n",
    "                        weight = sum(rewards[idx:])     # only weight the log likelihood of this action by the future rewards, not the total\n",
    "                    else:\n",
    "                        weight = sum(rewards) - baseline           # weight by the total reward from this episode\n",
    "                    objective += log_policy[idx] * weight   # add the weighted log likelihood of this taking action to \n",
    "\n",
    "\n",
    "            objective /= episodes   # average over episodes\n",
    "            objective *= -1     # invert to represent reward rather than cost\n",
    "\n",
    "\n",
    "            # UPDATE POLICY\n",
    "            # print('updating policy')\n",
    "            print('EPOCH:', epoch, f'AVG REWARD: {avg_reward:.2f}')\n",
    "            objective.backward()    # backprop\n",
    "            optimiser.step()    # update params\n",
    "            optimiser.zero_grad()   # reset gradients to zero\n",
    "\n",
    "            # VISUALISE AT END OF EPOCH AFTER UPDATING POLICY\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                env.render()\n",
    "                state = torch.Tensor(state)\n",
    "                state = state.view(np.prod(state.shape))\n",
    "                action_distribution = policy(state)\n",
    "                action = torch.distributions.Categorical(action_distribution).sample()\n",
    "                action = int(action)\n",
    "                state, reward, done, info = env.step(action)\n",
    "                sleep(0.01)\n",
    "    except KeyboardInterrupt:\n",
    "        print('interrupted')\n",
    "        env.close()\n",
    "\n",
    "    env.close()\n",
    "    torch.save(policy.state_dict(), f'trained-agent-{agent_tag}.pt')\n",
    "\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "policy = NN([np.prod(env.observation_space.shape), 32, env.action_space.n], distribution=True)\n",
    "\n",
    "lr = 0.001\n",
    "weight_decay = 1\n",
    "optimiser = torch.optim.SGD(policy.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "agent_tag = 'cartpole-REINFORCE'\n",
    "\n",
    "train(\n",
    "    env,\n",
    "    optimiser,\n",
    "    agent_tag,\n",
    "    use_baseline=True,\n",
    "    use_causality=False,\n",
    "    epochs=30,\n",
    "    episodes=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 AVG REWARD: -165.90\n",
      "EPOCH: 1 AVG REWARD: -147.29\n",
      "EPOCH: 2 AVG REWARD: -151.91\n",
      "EPOCH: 3 AVG REWARD: -145.13\n",
      "EPOCH: 4 AVG REWARD: -170.91\n",
      "EPOCH: 5 AVG REWARD: -139.21\n",
      "EPOCH: 6 AVG REWARD: -110.02\n",
      "EPOCH: 7 AVG REWARD: -138.35\n",
      "EPOCH: 8 AVG REWARD: -146.17\n",
      "EPOCH: 9 AVG REWARD: -143.49\n",
      "EPOCH: 10 AVG REWARD: -122.32\n",
      "EPOCH: 11 AVG REWARD: -111.05\n",
      "EPOCH: 12 AVG REWARD: -117.63\n",
      "EPOCH: 13 AVG REWARD: -104.58\n",
      "EPOCH: 14 AVG REWARD: -129.34\n",
      "EPOCH: 15 AVG REWARD: -125.12\n",
      "EPOCH: 16 AVG REWARD: -123.64\n",
      "EPOCH: 17 AVG REWARD: -113.54\n",
      "EPOCH: 18 AVG REWARD: -109.63\n",
      "EPOCH: 19 AVG REWARD: -106.91\n",
      "EPOCH: 20 AVG REWARD: -128.06\n",
      "EPOCH: 21 AVG REWARD: -123.71\n",
      "EPOCH: 22 AVG REWARD: -132.36\n",
      "EPOCH: 23 AVG REWARD: -108.37\n",
      "EPOCH: 24 AVG REWARD: -112.74\n",
      "EPOCH: 25 AVG REWARD: -96.60\n",
      "EPOCH: 26 AVG REWARD: -106.75\n",
      "EPOCH: 27 AVG REWARD: -156.71\n",
      "EPOCH: 28 AVG REWARD: -148.22\n",
      "EPOCH: 29 AVG REWARD: -120.53\n",
      "EPOCH: 30 AVG REWARD: -106.09\n",
      "EPOCH: 31 AVG REWARD: -100.33\n",
      "EPOCH: 32 AVG REWARD: -105.94\n",
      "EPOCH: 33 AVG REWARD: -106.12\n",
      "EPOCH: 34 AVG REWARD: -112.88\n",
      "EPOCH: 35 AVG REWARD: -102.76\n",
      "EPOCH: 36 AVG REWARD: -104.82\n",
      "EPOCH: 37 AVG REWARD: -103.83\n",
      "EPOCH: 38 AVG REWARD: -106.33\n",
      "EPOCH: 39 AVG REWARD: -106.79\n",
      "EPOCH: 40 AVG REWARD: -99.67\n",
      "EPOCH: 41 AVG REWARD: -93.92\n",
      "EPOCH: 42 AVG REWARD: -97.44\n",
      "EPOCH: 43 AVG REWARD: -89.25\n",
      "EPOCH: 44 AVG REWARD: -92.53\n",
      "EPOCH: 45 AVG REWARD: -102.58\n",
      "EPOCH: 46 AVG REWARD: -91.25\n",
      "EPOCH: 47 AVG REWARD: -80.74\n",
      "EPOCH: 48 AVG REWARD: -102.30\n",
      "EPOCH: 49 AVG REWARD: -90.52\n",
      "EPOCH: 50 AVG REWARD: -93.92\n",
      "EPOCH: 51 AVG REWARD: -84.06\n",
      "EPOCH: 52 AVG REWARD: -68.98\n",
      "EPOCH: 53 AVG REWARD: -76.38\n",
      "EPOCH: 54 AVG REWARD: -79.73\n",
      "EPOCH: 55 AVG REWARD: -72.92\n",
      "EPOCH: 56 AVG REWARD: -75.13\n",
      "EPOCH: 57 AVG REWARD: -59.24\n",
      "EPOCH: 58 AVG REWARD: -77.64\n",
      "EPOCH: 59 AVG REWARD: -70.24\n",
      "EPOCH: 60 AVG REWARD: -71.12\n",
      "EPOCH: 61 AVG REWARD: -80.92\n",
      "EPOCH: 62 AVG REWARD: -50.50\n",
      "EPOCH: 63 AVG REWARD: -85.80\n",
      "EPOCH: 64 AVG REWARD: -54.40\n",
      "EPOCH: 65 AVG REWARD: -51.28\n",
      "EPOCH: 66 AVG REWARD: -119.56\n",
      "EPOCH: 67 AVG REWARD: -117.96\n",
      "EPOCH: 68 AVG REWARD: -85.65\n",
      "EPOCH: 69 AVG REWARD: -86.39\n",
      "EPOCH: 70 AVG REWARD: -75.59\n",
      "EPOCH: 71 AVG REWARD: -86.63\n",
      "EPOCH: 72 AVG REWARD: -84.19\n",
      "EPOCH: 73 AVG REWARD: -82.06\n",
      "EPOCH: 74 AVG REWARD: -79.10\n",
      "EPOCH: 75 AVG REWARD: -76.35\n",
      "EPOCH: 76 AVG REWARD: -88.51\n",
      "EPOCH: 77 AVG REWARD: -63.76\n",
      "EPOCH: 78 AVG REWARD: -61.51\n",
      "EPOCH: 79 AVG REWARD: -61.12\n",
      "EPOCH: 80 AVG REWARD: -86.97\n",
      "EPOCH: 81 AVG REWARD: -70.08\n",
      "EPOCH: 82 AVG REWARD: -54.52\n",
      "EPOCH: 83 AVG REWARD: -55.80\n",
      "EPOCH: 84 AVG REWARD: -56.35\n",
      "EPOCH: 85 AVG REWARD: -43.88\n",
      "EPOCH: 86 AVG REWARD: -58.45\n",
      "EPOCH: 87 AVG REWARD: -42.38\n",
      "EPOCH: 88 AVG REWARD: -74.18\n",
      "EPOCH: 89 AVG REWARD: -42.23\n",
      "EPOCH: 90 AVG REWARD: -56.72\n",
      "EPOCH: 91 AVG REWARD: -24.16\n",
      "EPOCH: 92 AVG REWARD: -50.86\n",
      "EPOCH: 93 AVG REWARD: -33.01\n",
      "EPOCH: 94 AVG REWARD: -89.20\n",
      "EPOCH: 95 AVG REWARD: -74.54\n",
      "EPOCH: 96 AVG REWARD: -62.95\n",
      "EPOCH: 97 AVG REWARD: -106.97\n",
      "EPOCH: 98 AVG REWARD: -59.91\n",
      "EPOCH: 99 AVG REWARD: -46.59\n",
      "EPOCH: 100 AVG REWARD: -43.07\n",
      "EPOCH: 101 AVG REWARD: -49.57\n",
      "EPOCH: 102 AVG REWARD: -57.25\n",
      "EPOCH: 103 AVG REWARD: -46.78\n",
      "EPOCH: 104 AVG REWARD: -45.67\n",
      "EPOCH: 105 AVG REWARD: -50.33\n",
      "EPOCH: 106 AVG REWARD: -52.31\n",
      "EPOCH: 107 AVG REWARD: -69.33\n",
      "EPOCH: 108 AVG REWARD: -64.45\n",
      "EPOCH: 109 AVG REWARD: -58.75\n",
      "EPOCH: 110 AVG REWARD: -59.90\n",
      "EPOCH: 111 AVG REWARD: -46.63\n",
      "EPOCH: 112 AVG REWARD: -36.89\n",
      "EPOCH: 113 AVG REWARD: -30.30\n",
      "EPOCH: 114 AVG REWARD: -47.76\n",
      "EPOCH: 115 AVG REWARD: -42.11\n",
      "EPOCH: 116 AVG REWARD: -37.65\n",
      "EPOCH: 117 AVG REWARD: -32.96\n",
      "EPOCH: 118 AVG REWARD: -149.43\n",
      "EPOCH: 119 AVG REWARD: -85.50\n",
      "EPOCH: 120 AVG REWARD: -88.52\n",
      "EPOCH: 121 AVG REWARD: -74.03\n",
      "EPOCH: 122 AVG REWARD: -78.90\n",
      "EPOCH: 123 AVG REWARD: -73.30\n",
      "EPOCH: 124 AVG REWARD: -99.09\n",
      "EPOCH: 125 AVG REWARD: -80.84\n",
      "EPOCH: 126 AVG REWARD: -71.26\n",
      "EPOCH: 127 AVG REWARD: -161.90\n",
      "EPOCH: 128 AVG REWARD: -75.57\n",
      "EPOCH: 129 AVG REWARD: -69.72\n",
      "EPOCH: 130 AVG REWARD: -61.20\n",
      "EPOCH: 131 AVG REWARD: -61.96\n",
      "EPOCH: 132 AVG REWARD: -45.58\n",
      "EPOCH: 133 AVG REWARD: -76.39\n",
      "EPOCH: 134 AVG REWARD: -66.39\n",
      "EPOCH: 135 AVG REWARD: -65.15\n",
      "EPOCH: 136 AVG REWARD: -61.26\n",
      "EPOCH: 137 AVG REWARD: -62.11\n",
      "EPOCH: 138 AVG REWARD: -55.14\n",
      "EPOCH: 139 AVG REWARD: -57.19\n",
      "EPOCH: 140 AVG REWARD: -57.51\n",
      "EPOCH: 141 AVG REWARD: -43.61\n",
      "EPOCH: 142 AVG REWARD: -64.61\n",
      "EPOCH: 143 AVG REWARD: -47.98\n",
      "EPOCH: 144 AVG REWARD: -44.29\n",
      "EPOCH: 145 AVG REWARD: -47.23\n",
      "EPOCH: 146 AVG REWARD: -49.81\n",
      "EPOCH: 147 AVG REWARD: -55.17\n",
      "EPOCH: 148 AVG REWARD: -58.61\n",
      "EPOCH: 149 AVG REWARD: -39.60\n",
      "EPOCH: 150 AVG REWARD: -57.42\n",
      "EPOCH: 151 AVG REWARD: -36.31\n",
      "EPOCH: 152 AVG REWARD: -44.11\n",
      "EPOCH: 153 AVG REWARD: -5.85\n",
      "EPOCH: 154 AVG REWARD: -45.79\n",
      "EPOCH: 155 AVG REWARD: -31.04\n",
      "EPOCH: 156 AVG REWARD: -76.45\n",
      "EPOCH: 157 AVG REWARD: -159.76\n",
      "EPOCH: 158 AVG REWARD: -181.05\n",
      "EPOCH: 159 AVG REWARD: -92.20\n",
      "EPOCH: 160 AVG REWARD: -34.67\n",
      "EPOCH: 161 AVG REWARD: -49.10\n",
      "EPOCH: 162 AVG REWARD: -29.43\n",
      "EPOCH: 163 AVG REWARD: -5.69\n",
      "EPOCH: 164 AVG REWARD: -39.22\n",
      "EPOCH: 165 AVG REWARD: -16.38\n",
      "EPOCH: 166 AVG REWARD: -36.39\n",
      "EPOCH: 167 AVG REWARD: -40.30\n",
      "EPOCH: 168 AVG REWARD: -13.34\n",
      "EPOCH: 169 AVG REWARD: -0.46\n",
      "EPOCH: 170 AVG REWARD: -26.42\n",
      "EPOCH: 171 AVG REWARD: 14.27\n",
      "EPOCH: 172 AVG REWARD: -157.79\n",
      "EPOCH: 173 AVG REWARD: -206.46\n",
      "EPOCH: 174 AVG REWARD: -310.96\n",
      "EPOCH: 175 AVG REWARD: -163.70\n",
      "EPOCH: 176 AVG REWARD: -307.62\n",
      "EPOCH: 177 AVG REWARD: -122.10\n",
      "EPOCH: 178 AVG REWARD: -288.49\n",
      "EPOCH: 179 AVG REWARD: -233.46\n",
      "EPOCH: 180 AVG REWARD: -231.02\n",
      "EPOCH: 181 AVG REWARD: -247.15\n",
      "EPOCH: 182 AVG REWARD: -151.32\n",
      "EPOCH: 183 AVG REWARD: -190.50\n",
      "EPOCH: 184 AVG REWARD: -363.34\n",
      "EPOCH: 185 AVG REWARD: -120.13\n",
      "EPOCH: 186 AVG REWARD: -101.63\n",
      "EPOCH: 187 AVG REWARD: -102.27\n",
      "EPOCH: 188 AVG REWARD: -45.61\n",
      "EPOCH: 189 AVG REWARD: -8.21\n",
      "EPOCH: 190 AVG REWARD: 10.19\n",
      "EPOCH: 191 AVG REWARD: -65.12\n",
      "EPOCH: 192 AVG REWARD: -58.85\n",
      "EPOCH: 193 AVG REWARD: -69.82\n",
      "EPOCH: 194 AVG REWARD: -60.31\n",
      "EPOCH: 195 AVG REWARD: -45.49\n",
      "EPOCH: 196 AVG REWARD: -45.55\n",
      "EPOCH: 197 AVG REWARD: -32.27\n",
      "EPOCH: 198 AVG REWARD: -51.59\n",
      "EPOCH: 199 AVG REWARD: -22.53\n",
      "EPOCH: 200 AVG REWARD: -37.47\n",
      "EPOCH: 201 AVG REWARD: 4.91\n",
      "EPOCH: 202 AVG REWARD: -41.41\n",
      "EPOCH: 203 AVG REWARD: -91.91\n",
      "EPOCH: 204 AVG REWARD: -77.35\n",
      "EPOCH: 205 AVG REWARD: -49.51\n",
      "EPOCH: 206 AVG REWARD: -65.89\n",
      "EPOCH: 207 AVG REWARD: -84.16\n",
      "EPOCH: 208 AVG REWARD: -61.47\n",
      "EPOCH: 209 AVG REWARD: -49.96\n",
      "EPOCH: 210 AVG REWARD: -62.83\n",
      "EPOCH: 211 AVG REWARD: -29.59\n",
      "EPOCH: 212 AVG REWARD: 9.57\n",
      "EPOCH: 213 AVG REWARD: 5.62\n",
      "EPOCH: 214 AVG REWARD: -61.74\n",
      "EPOCH: 215 AVG REWARD: -4.48\n",
      "EPOCH: 216 AVG REWARD: 9.62\n",
      "EPOCH: 217 AVG REWARD: -63.88\n",
      "EPOCH: 218 AVG REWARD: -57.33\n",
      "EPOCH: 219 AVG REWARD: -5.05\n",
      "EPOCH: 220 AVG REWARD: -8.14\n",
      "EPOCH: 221 AVG REWARD: 0.38\n",
      "EPOCH: 222 AVG REWARD: -38.29\n",
      "EPOCH: 223 AVG REWARD: -37.55\n",
      "EPOCH: 224 AVG REWARD: 8.30\n",
      "EPOCH: 225 AVG REWARD: 11.20\n",
      "EPOCH: 226 AVG REWARD: -8.01\n",
      "EPOCH: 227 AVG REWARD: -38.07\n",
      "EPOCH: 228 AVG REWARD: -8.55\n",
      "EPOCH: 229 AVG REWARD: -22.30\n",
      "EPOCH: 230 AVG REWARD: -24.04\n",
      "EPOCH: 231 AVG REWARD: -10.18\n",
      "EPOCH: 232 AVG REWARD: -4.15\n",
      "EPOCH: 233 AVG REWARD: -3.39\n",
      "EPOCH: 234 AVG REWARD: -65.17\n",
      "EPOCH: 235 AVG REWARD: -49.02\n",
      "EPOCH: 236 AVG REWARD: -43.97\n",
      "EPOCH: 237 AVG REWARD: -55.77\n",
      "EPOCH: 238 AVG REWARD: -46.59\n",
      "EPOCH: 239 AVG REWARD: -27.91\n",
      "EPOCH: 240 AVG REWARD: -33.08\n",
      "EPOCH: 241 AVG REWARD: -0.15\n",
      "EPOCH: 242 AVG REWARD: -9.06\n",
      "EPOCH: 243 AVG REWARD: 0.44\n",
      "EPOCH: 244 AVG REWARD: -45.96\n",
      "EPOCH: 245 AVG REWARD: -34.23\n",
      "EPOCH: 246 AVG REWARD: -40.99\n",
      "EPOCH: 247 AVG REWARD: -27.06\n",
      "EPOCH: 248 AVG REWARD: -37.45\n",
      "EPOCH: 249 AVG REWARD: -44.14\n",
      "EPOCH: 250 AVG REWARD: -44.64\n",
      "EPOCH: 251 AVG REWARD: -36.68\n",
      "EPOCH: 252 AVG REWARD: -48.33\n",
      "EPOCH: 253 AVG REWARD: -109.34\n",
      "EPOCH: 254 AVG REWARD: -139.72\n",
      "EPOCH: 255 AVG REWARD: -113.40\n",
      "EPOCH: 256 AVG REWARD: -154.76\n",
      "EPOCH: 257 AVG REWARD: -25.70\n",
      "EPOCH: 258 AVG REWARD: 5.22\n",
      "EPOCH: 259 AVG REWARD: -20.29\n",
      "EPOCH: 260 AVG REWARD: -15.43\n",
      "EPOCH: 261 AVG REWARD: -2.47\n",
      "EPOCH: 262 AVG REWARD: -73.51\n",
      "EPOCH: 263 AVG REWARD: -35.95\n",
      "EPOCH: 264 AVG REWARD: -8.04\n",
      "EPOCH: 265 AVG REWARD: -55.12\n",
      "EPOCH: 266 AVG REWARD: -21.66\n",
      "EPOCH: 267 AVG REWARD: 6.36\n",
      "EPOCH: 268 AVG REWARD: 6.31\n",
      "EPOCH: 269 AVG REWARD: -6.43\n",
      "EPOCH: 270 AVG REWARD: 10.78\n",
      "EPOCH: 271 AVG REWARD: -17.35\n",
      "EPOCH: 272 AVG REWARD: -5.80\n",
      "EPOCH: 273 AVG REWARD: -28.68\n",
      "EPOCH: 274 AVG REWARD: 7.99\n",
      "EPOCH: 275 AVG REWARD: -4.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 276 AVG REWARD: 18.56\n",
      "EPOCH: 277 AVG REWARD: -96.14\n",
      "EPOCH: 278 AVG REWARD: -101.76\n",
      "EPOCH: 279 AVG REWARD: -70.00\n",
      "EPOCH: 280 AVG REWARD: -55.67\n",
      "EPOCH: 281 AVG REWARD: -29.84\n",
      "EPOCH: 282 AVG REWARD: -63.33\n",
      "EPOCH: 283 AVG REWARD: -19.72\n",
      "EPOCH: 284 AVG REWARD: -1.95\n",
      "EPOCH: 285 AVG REWARD: -69.34\n",
      "EPOCH: 286 AVG REWARD: -108.99\n",
      "EPOCH: 287 AVG REWARD: -82.61\n",
      "EPOCH: 288 AVG REWARD: -73.58\n",
      "EPOCH: 289 AVG REWARD: -119.46\n",
      "EPOCH: 290 AVG REWARD: -87.62\n",
      "EPOCH: 291 AVG REWARD: -8.59\n",
      "EPOCH: 292 AVG REWARD: -18.75\n",
      "EPOCH: 293 AVG REWARD: 2.22\n",
      "EPOCH: 294 AVG REWARD: 5.01\n",
      "EPOCH: 295 AVG REWARD: -7.48\n",
      "EPOCH: 296 AVG REWARD: -15.38\n",
      "EPOCH: 297 AVG REWARD: 21.74\n",
      "EPOCH: 298 AVG REWARD: 14.95\n",
      "EPOCH: 299 AVG REWARD: -11.35\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter() # create new tensorboard writer\n",
    "\n",
    "env = gym.make('LunarLander-v2') # make cartpole environment\n",
    "\n",
    "policy = NN([np.prod(env.observation_space.shape), 32, env.action_space.n], distribution=True) \n",
    "\n",
    "lr = 0.001\n",
    "weight_decay = 1\n",
    "optimiser = torch.optim.SGD(policy.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "agent_tag = 'lunar-lander'\n",
    "\n",
    "train(\n",
    "    env,\n",
    "    optimiser,\n",
    "    agent_tag,\n",
    "    use_baseline=True,\n",
    "    use_causality=False,\n",
    "    epochs=300,\n",
    "    episodes=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE Algorithm summary\n",
    "\n",
    "### Online or offline?\n",
    "There is a distinction between collecting experience and updating the policy. So REINFORCE is offline.\n",
    "\n",
    "### Model based or model free?\n",
    "There's no mention of a transition function in this algorithm, so it's model free!\n",
    "\n",
    "### On-policy or off-policy?\n",
    "The gradient signal comes from rewards obtained on a trajectory that was produced by following the current policy. So REINFORCE is on-policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying our trained agents\n",
    "\n",
    "The goal of all of this has been to produce agents that are ready to go and do things autonomously. So let's write a function that does that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy(env, state_dict):\n",
    "    \n",
    "    policy = torch.load(state_dict) # load in our pre-trained model\n",
    "    \n",
    "    while True: # keep demonstrating your skills\n",
    "        try:\n",
    "            done = False # not done yet\n",
    "            observation = env.reset() # initialise the environemt\n",
    "            while not done: # until the episode is over\n",
    "                observation = torch.Tensor(observation) # turn observation to tensor\n",
    "                observation = observation.view(np.prod(observation.shape)) # view observation as vector\n",
    "                action_distribution = policy(observation) # infer what actions to take with what probability\n",
    "                action = torch.distributions.Categorical(action_distribution).sample() # sample an action from that distribution\n",
    "                action = int(action) # make it an int not a float\n",
    "                observation, reward, done, info = env.step(action) # take an action and transition the environment\n",
    "                env.render() # show us the environment\n",
    "        except KeyboardInterrupt:\n",
    "            env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Any final words?\n",
    "\n",
    "Great, so policy gradient methods can at least do as well as the previous algorithms that we've seen.\n",
    "\n",
    "Policy gradient based methods depend on the objective being differentiable with respect to the policy parameters.\n",
    "\n",
    "For any RL agent, the objective (total expected reward attained) is produced as a result of following some policy. If the policy is better, then this objective will be larger.\n",
    "\n",
    "In policy and value iteration (value based techniques), the policy was produced as a result of taking the action with the max state-value. This max operation is not differentiable, and so neither was the policy. For policy gradients to be followed, we must have a differentiable policy.\n",
    "\n",
    "Policy gradient methods will also work with a partially observable environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "- [Trust Region Policy Optimisation (TRPO)]()\n",
    "- [Upside Down RL (UDRL)]()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
