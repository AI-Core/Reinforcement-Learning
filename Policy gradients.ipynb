{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients\n",
    "\n",
    "Prerequisites:\n",
    "- [Value based methods](https://theaicore.com/app/training/intro-to-rl)\n",
    "- [Neural Networks](https://theaicore.com/app/training/neural-networks)\n",
    "\n",
    "Previously we looked at value based methods; those which estimated how good certain states were (value function) and how good certain actions were from certain states (action-value or Q function).\n",
    "\n",
    "In this notebook we'll look at policy gradient based methods\n",
    "\n",
    "## What's the goal of reinforcement learning?\n",
    "\n",
    "The goal of a reinforcement learning agent is to maximise expected reward over it's lifetime.\n",
    "What the agent experiences over it's lifetime, including rewards, states and actions defines it's *trajectory*.\n",
    "The trajectories that an agent might experience depend on what actions it takes from any given state, that is, what policy the agent follows.\n",
    "\n",
    "We can formulate this as below.\n",
    "\n",
    "# J\n",
    "\n",
    "Where the policy is a function with parameters $\\theta$.\n",
    "\n",
    "What we'd like to do, is to find parameters that maximise this objective, $J$, and hence find an optimal parameterisation for our poilcy.\n",
    "\n",
    "Because the objective if fully differentiable, we can use gradient **ascent** to improve our objective with respect to our parameters.\n",
    "\n",
    "Below we analytically derive the gradient of the objective with respect to the parameters.\n",
    "\n",
    "# del J\n",
    "\n",
    "Now we can use this derivative in our gradient ascent update rule. \n",
    "Note the update is in the direction of the gradient because this is gradient ascent, not descent. \n",
    "That's because it represents our expected reward which we want to maximise, rather than a loss which we might want to minimise in a different case.\n",
    "\n",
    "# update rule\n",
    "\n",
    "What does this say?\n",
    "\n",
    "# verbose update\n",
    "\n",
    "So let's build a neural network which will act as our agent's policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, layers, embedding=False, distribution=False):\n",
    "        super().__init__()\n",
    "        l = []\n",
    "        for idx in range(len(layers) - 1):\n",
    "            if idx == 0 and embedding:\n",
    "                l.append(torch.nn.Embedding(layers[idx], layers[idx+1]))\n",
    "                continue\n",
    "            l.append(torch.nn.Linear(layers[idx], layers[idx+1]))   # add a linear layer\n",
    "            if idx != len(layers) - 2: # if this is not the last layer\n",
    "                l.append(torch.nn.ReLU())   # activate\n",
    "        if distribution:    # if a probability dist output is required\n",
    "            l.append(torch.nn.Softmax())    # apply softmax to output\n",
    "            \n",
    "        self.layers = torch.nn.Sequential(*l)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this neural network to model the policy which will control our agent in Griddy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from GriddyEnv import GriddyEnv # get griddy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "writer = SummaryWriter()\n",
    "# def train(env, optimiser, epochs=100, episodes=30, use_baseline=False, use_causality=False):\n",
    "# #     try:\n",
    "#     for epoch in range(epochs):\n",
    "#         avg_reward = 0\n",
    "#         objective = 0\n",
    "#         for episode in range(episodes):\n",
    "#             done = False\n",
    "#             state = env.reset()\n",
    "#             log_policy = []\n",
    "\n",
    "#             rewards = []\n",
    "\n",
    "#             step = 0\n",
    "\n",
    "#             # RUN AN EPISODE\n",
    "#             while not done:     # while the episode is not terminated\n",
    "#                 state = torch.Tensor(state)     # correct data type for passing to model\n",
    "#                 print('STATE:', state)\n",
    "#                 state = state.view(np.prod(state.shape))\n",
    "#                 print('FLATTENED STATE:', state)\n",
    "\n",
    "#                 action_distribution = policy(state)     # get a distribution over actions from the policy given the state\n",
    "#                 print('ACTION DISTRIBUTION:', action_distribution)\n",
    "\n",
    "#                 action = torch.distributions.Categorical(action_distribution).sample()      # sample from that distrbution\n",
    "#                 print(action)\n",
    "#                 action = int(action)\n",
    "#                 # print('ACTION:', action)\n",
    "\n",
    "#                 new_state, reward, done, info = env.step(action)    # take timestep\n",
    "\n",
    "#                 rewards.append(reward)\n",
    "\n",
    "#                 state = new_state\n",
    "#                 log_policy.append(torch.log(action_distribution[action]))\n",
    "\n",
    "#                 step += 1\n",
    "#                 if done:\n",
    "#                     break\n",
    "#                 if step > 10000000:\n",
    "#                     # break\n",
    "#                     pass\n",
    "\n",
    "#             avg_reward += ( sum(rewards) - avg_reward ) / ( episode + 1 )   # accumulate avg reward\n",
    "#             writer.add_scalar('Reward/Train', avg_reward, epoch*episodes + episode)     # plot the latest reward\n",
    "\n",
    "#             for idx in range(len(rewards)):     # for each timestep experienced in the episode\n",
    "#                 weight = sum(rewards)           # weight by the total reward from this episode\n",
    "#                 objective += log_policy[idx] * weight   # add the weighted log likelihood of this taking action to \n",
    "\n",
    "\n",
    "#         objective /= episodes   # average over episodes\n",
    "#         objective *= -1     # invert to represent reward rather than cost\n",
    "\n",
    "\n",
    "#         # UPDATE POLICY\n",
    "#         # print('updating policy')\n",
    "#         print('EPOCH:', epoch, f'AVG REWARD: {avg_reward:.2f}')\n",
    "#         objective.backward()    # backprop\n",
    "#         optimiser.step()    # update params\n",
    "#         optimiser.zero_grad()   # reset gradients to zero\n",
    "\n",
    "#         # VISUALISE AT END OF EPOCH AFTER UPDATING POLICY\n",
    "#         state = env.reset()\n",
    "#         done = False\n",
    "#         while not done:\n",
    "#             env.render()\n",
    "#             state = torch.Tensor(state)\n",
    "#             action_distribution = policy(state)\n",
    "#             action = torch.distributions.Categorical(action_distribution).sample()\n",
    "#             action = int(action)\n",
    "#             state, reward, done, info = env.step(action)\n",
    "#             sleep(0.01)\n",
    "\n",
    "#     env.close()\n",
    "# #     except KeyboardInterrupt:\n",
    "# #         env.close()\n",
    "# #     except RuntimeError:\n",
    "# #         env.close()\n",
    "\n",
    "\n",
    "# env = GriddyEnv()\n",
    "# # env = gym.make('CartPole-v0')\n",
    "\n",
    "# policy = NN([np.prod(env.observation_space.shape), 32, env.action_space.n], distribution=True)\n",
    "\n",
    "# lr = 0.001\n",
    "# weight_decay = 1\n",
    "# optimiser = torch.optim.SGD(policy.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# train(\n",
    "#     env,\n",
    "#     optimiser,\n",
    "#     epochs=400,\n",
    "#     episodes=30\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's move onto a more challenging environment called CartPole. The aim is to have a cart move along one dimension and balance an inverted pole vertically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we improve this learning algorithm?\n",
    "\n",
    "What we just implemented is the vanilla policy gradient algorithm.\n",
    "\n",
    "Notice anything that you think could be improved?\n",
    "\n",
    "### Baselines\n",
    "\n",
    "What if all of the trajectories receive a similar reward (e.g. \n",
    "what if the total reward over every trajectory is negative (e.g. negative reward every timestep and zero upon reaching a terminal state)?\n",
    "\n",
    "### Causality\n",
    "What rewards can an action take responsibilty for?\n",
    "Surely an reward received before an action taken later in the trajectory shouldn't indicate that the later action was good. Whatever the action taken later in time was, this reward was received before then.\n",
    "\n",
    "To account for this, we should only weight how good the action was by the rewards which it led the agent to receive from that point in time onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 AVG REWARD: 23.97\n",
      "EPOCH: 1 AVG REWARD: 21.90\n",
      "EPOCH: 2 AVG REWARD: 31.47\n",
      "EPOCH: 3 AVG REWARD: 26.23\n",
      "EPOCH: 4 AVG REWARD: 24.00\n",
      "EPOCH: 5 AVG REWARD: 31.57\n",
      "interrupted\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train(env, optimiser, epochs=100, episodes=30, use_baseline=False, use_causality=False):\n",
    "    assert not (use_baseline and use_causality)   # cant implement both simply\n",
    "    baseline = 0\n",
    "    try:\n",
    "        for epoch in range(epochs):\n",
    "            avg_reward = 0\n",
    "            objective = 0\n",
    "            for episode in range(episodes):\n",
    "                done = False\n",
    "                state = env.reset()\n",
    "                log_policy = []\n",
    "\n",
    "                rewards = []\n",
    "\n",
    "                step = 0\n",
    "\n",
    "                # RUN AN EPISODE\n",
    "                while not done:     # while the episode is not terminated\n",
    "                    state = torch.Tensor(state)     # correct data type for passing to model\n",
    "                    # print('STATE:', state)\n",
    "                    action_distribution = policy(state)     # get a distribution over actions from the policy given the state\n",
    "                    # print('ACTION DISTRIBUTION:', action_distribution)\n",
    "\n",
    "                    action = torch.distributions.Categorical(action_distribution).sample()      # sample from that distrbution\n",
    "                    action = int(action)\n",
    "                    # print('ACTION:', action)\n",
    "\n",
    "                    new_state, reward, done, info = env.step(action)    # take timestep\n",
    "\n",
    "                    rewards.append(reward)\n",
    "\n",
    "                    state = new_state\n",
    "                    log_policy.append(torch.log(action_distribution[action]))\n",
    "\n",
    "                    step += 1\n",
    "                    if done:\n",
    "                        break\n",
    "                    if step > 10000000:\n",
    "                        # break\n",
    "                        pass\n",
    "\n",
    "                avg_reward += ( sum(rewards) - avg_reward ) / ( episode + 1 )   # accumulate avg reward\n",
    "                writer.add_scalar('Reward/Train', avg_reward, epoch*episodes + episode)     # plot the latest reward\n",
    "\n",
    "                # update baseline\n",
    "                if use_baseline:\n",
    "                    baseline += ( sum(rewards) - baseline ) / (epoch*episodes + episode + 1)    # accumulate average return  \n",
    "\n",
    "                for idx in range(len(rewards)):     # for each timestep experienced in the episode\n",
    "                    # add causality\n",
    "                    if use_causality:   \n",
    "                        weight = sum(rewards[idx:])     # only weight the log likelihood of this action by the future rewards, not the total\n",
    "                    else:\n",
    "                        weight = sum(rewards) - baseline           # weight by the total reward from this episode\n",
    "                    objective += log_policy[idx] * weight   # add the weighted log likelihood of this taking action to \n",
    "\n",
    "\n",
    "            objective /= episodes   # average over episodes\n",
    "            objective *= -1     # invert to represent reward rather than cost\n",
    "\n",
    "\n",
    "            # UPDATE POLICY\n",
    "            # print('updating policy')\n",
    "            print('EPOCH:', epoch, f'AVG REWARD: {avg_reward:.2f}')\n",
    "            objective.backward()    # backprop\n",
    "            optimiser.step()    # update params\n",
    "            optimiser.zero_grad()   # reset gradients to zero\n",
    "\n",
    "            # VISUALISE AT END OF EPOCH AFTER UPDATING POLICY\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                env.render()\n",
    "                state = torch.Tensor(state)\n",
    "                state = state.view(np.prod(state.shape))\n",
    "                action_distribution = policy(state)\n",
    "                action = torch.distributions.Categorical(action_distribution).sample()\n",
    "                action = int(action)\n",
    "                state, reward, done, info = env.step(action)\n",
    "                sleep(0.01)\n",
    "    except KeyboardInterrupt:\n",
    "        print('interrupted')\n",
    "        env.close()\n",
    "\n",
    "    env.close()\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "policy = NN([np.prod(env.observation_space.shape), 32, env.action_space.n], distribution=True)\n",
    "\n",
    "lr = 0.001\n",
    "weight_decay = 1\n",
    "optimiser = torch.optim.SGD(policy.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "train(\n",
    "    env,\n",
    "    optimiser,\n",
    "    use_baseline=True,\n",
    "    use_causality=False,\n",
    "    epochs=30,\n",
    "    episodes=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The improved learning algorithm that we just implemented is called REINFORCE (REward Increment = Nonnegative Factor $\\times$ Offset Reinforcement $\\times$ Characteristic Eligibility). This name describes the structure of the parameter updates.\n",
    "\n",
    "## REINFORCE Algorithm summary\n",
    "\n",
    "### Online or offline?\n",
    "There is a distinction between collecting experience and updating the policy. So REINFORCE is offline.\n",
    "\n",
    "### Model based or model free?\n",
    "There's no mention of a transition function in this algorithm, so it's model free!\n",
    "\n",
    "### On-policy or off-policy?\n",
    "The gradient signal comes from rewards obtained on a trajectory that was produced by following the current policy. So REINFORCE is on-policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so policy gradient methods can at least do as well as the previous algorithms that we've seen.\n",
    "\n",
    "Policy gradient based methods depend on the objective being differentiable with respect to the policy parameters.\n",
    "\n",
    "For any RL agent, the objective (total expected reward attained) is produced as a result of following some policy. If the policy is better, then this objective will be larger.\n",
    "\n",
    "In policy and value iteration (value based techniques), the policy was produced as a result of taking the action with the max state-value. This max operation is not differentiable. As such, policy gradients could not be used.\n",
    "\n",
    "Like fitted value iteration and q-learning, we can use a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
