{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning\n",
    "\n",
    "### Monte-Carlo (MC) methods for computing action-value functions and improving policies\n",
    "\n",
    "\n",
    "Monte-Carlo methods are those based on repeated sampling to estimate a quantity. \n",
    "\n",
    "Similarly to what we did using DP, we can do MC based implementations of policy and value iteration by sampling experienced values of states, rather than simulating them with a model.\n",
    "\n",
    "The goal of using MC methods is to avoing the need for a model - if we don't have to look ahead from each state, then we can remove the model.\n",
    "\n",
    "However, even if we find an optimal value function, we will still need to use a model to extract an optimal policy to understand what states are reachable from others. It's no good having a chess piece next you your opponent's King if you don't know how that piece can move.\n",
    "\n",
    "The way we chose actions greedily when we had a model, was by using it to consider all possible actions and then taking the one that gave us the best expected return.\n",
    "\n",
    "As such, what would be more useful would be to use MC methods to estimate the action-value (Q) function. This tells us how good any particular action is from a certain state. If we know how good each action is, then we don't need a model - as long as we know all possible actions, we can just plug them into our q function and then take the one which for which our q function returns the largest value.\n",
    "\n",
    "### Q-functions - how good is taking a certain action in a certain state?\n",
    "![](images/q_def.jpg)\n",
    "\n",
    "![](images/q_optimality_derivation.JPG)\n",
    "\n",
    "### Computing action-value functions using Monte-Carlo methods\n",
    "\n",
    "For this method, we will use Monte-Carlo sampling to estimate the q-value of each state by running many episodes, and then doing backup from the terminal state.\n",
    "\n",
    "The q-value is the <strong>mean</strong> expected future reward following an action from a given state.\n",
    "Rather than storing all of our experience and taking the mean over them, we can use each experience to update an exponentially weighted average forget that exprience.\n",
    "\n",
    "![](./images/exp-avg.jpg)\n",
    "\n",
    "![](images/q_learning_algorithm.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_action_value_table(num_states=16, num_actions=4):\n",
    "    q_table = {}\n",
    "    for s in range(num_states):\n",
    "        q_table[s] = {}\n",
    "        for a in range(num_actions):\n",
    "            q_table[s][a] = 0\n",
    "    return q_table\n",
    "\n",
    "def get_state_idx_from_observation(observation):\n",
    "    return np.argmax(observation[2].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GriddyEnv(4, 4)\n",
    "epsilon = 1\n",
    "i_episode=0\n",
    "discount_factor=0.9\n",
    "learning_rate=0.3\n",
    "\n",
    "def return_optimal_policy_from_q(q_table):\n",
    "    optimal_policy = {}\n",
    "    for state, actions in q_table.items():\n",
    "        optimal_policy[state] = np.argmax(actions.values())\n",
    "    return optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(episode_mem, q_table, discount_factor=0.9, learning_rate=0.1):\n",
    "    for idx, mem in reversed(list(enumerate(episode_mem))):\n",
    "        state = get_state_idx_from_observation(mem['observation'])\n",
    "        new_state = get_state_idx_from_observation(mem['new_observation'])\n",
    "        action = mem['action']\n",
    "        if idx == len(episode_mem) - 1: # if terminal state, G=reward\n",
    "            _return = episode_mem[idx]['reward']\n",
    "        else:\n",
    "            _return = mem['reward'] + (discount_factor * _return)\n",
    "        q_table[state][action] =  q_table[state][action] + learning_rate * (_return - q_table[state][action]) # take exponential average\n",
    "        q_table[state][action] = np.round(q_table[state][action], 2)\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_policy_evaluation(policy, n_episodes=100):\n",
    "    q_table = initialise_action_value_table()\n",
    "    try:\n",
    "        \n",
    "        # SAMPLE SOME EPISODES\n",
    "        for episode_idx in range(n_episodes):\n",
    "            observation = env.reset()\n",
    "            episode_mem = []\n",
    "            done=False\n",
    "            t = 0\n",
    "            while not done:\n",
    "                env.render()\n",
    "                time.sleep(0.05)\n",
    "                action = policy(observation)\n",
    "                new_observation, reward, done, info = env.step(action)\n",
    "                episode_mem.append({'observation':observation,\n",
    "                                    'action':action,\n",
    "                                    'reward':reward,\n",
    "                                    'new_observation':new_observation,\n",
    "                                    'done':done})\n",
    "                observation = new_observation\n",
    "                t += 1\n",
    "                \n",
    "            q_table = update_q_table(episode_mem, q_table, discount_factor, learning_rate) # this is the implicit policy update\n",
    "            print(\"Episode {} finished after {} timesteps. Eplsilon={}.\".format(episode_idx, t, epsilon))#, end='\\r')\n",
    "            print('q table:', q_table)\n",
    "            time.sleep(2)\n",
    "\n",
    "\n",
    "        env.close()\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()\n",
    "        \n",
    "    return q_table\n",
    "\n",
    "MC_policy_evaluation(random_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(state, q_table):\n",
    "    return np.argmax(q_table[state].values()) # return action with best value for this state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_MC_control():\n",
    "    greedy_policy = policy_map_to_func(q_table)\n",
    "    while True:\n",
    "        \n",
    "        # POLICY EVALUATION\n",
    "        q_table = MC_policy_evaluation(greedy_policy)\n",
    "        \n",
    "        # POLICY IMPROVEMENT\n",
    "        \n",
    "        \n",
    "        # CHECK CONVERGENCE\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why not just act greedily all the time?\n",
    "\n",
    "If we act greedily all the time then we will move into the state with the best value. But remember that these values are only estimates based on our agent's experience with the game, which means that they might not be correct. So if we want to make sure that our agent will do well by always choosing the next action greedily, we need to make sure that it has good estimates for the values of those states. This brings us to a core challenge in reinforcement learning: **the exploration vs exploitation dilemma**. Our agent can either exploit what it knows by using it's current knowledge to choose the best action, or it can explore more and improve it's knowledge perhaps learning that some actions are even worse than what it does currently.\n",
    "\n",
    "Because we aren't using a model here, we aren't considering all possible actions. Instead we are just acting based on experience. \n",
    "If we experience a lower return from taking one action in a state rather than another, then we are not going to try the one that gave us the low return again.\n",
    "This would be more problematic in a stochastic environment - where environment transitions can vary randomly.\n",
    "When we act greedily we stop exploring, only exploiting the experience we have, even though we might have not experienced the best states.\n",
    "\n",
    "## An epsilon-greedy policy\n",
    "We can combine our random policy and our greedy policy to make an improved policy that both explores its environment and exploits its current knowledge. An $\\epsilon$-greedy (epsilon-greedy) policy is one which exploits what it knows most of the time, but with probability $\\epsilon$ will instead select a random action to try.\n",
    "\n",
    "## Do we need to keep exploring once we are confident in the values of states?\n",
    "\n",
    "As our agent explores more, it becomes more confident in predicting how valuable any state is. Once it knows a lot, it should start to explore less and exploit what it knows more. That means that we should decrease epsilon over time.\n",
    "\n",
    "Let's implement it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state):\n",
    "    epsilon = 0.05\n",
    "    if random.random() < epsilon:\n",
    "        return random_policy(state)\n",
    "    else:\n",
    "        return greedy_policy(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from GriddyEnv import GriddyEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_agent(policy, n=5):\n",
    "    try:\n",
    "        for trial_i in range(n):\n",
    "            observation = env.reset()\n",
    "            done=False\n",
    "            t=0\n",
    "            while not done:\n",
    "                env.render()\n",
    "                policy_action = policy(observation)\n",
    "                observation, reward, done, info = env.step(policy_action)\n",
    "                time.sleep(0.5)\n",
    "                t+=1\n",
    "            env.render()\n",
    "            time.sleep(1.5)\n",
    "            print(\"Episode {} finished after {} timesteps\".format(trial_i, t))\n",
    "        env.close()\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualise_agent(random_policy, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(state, return_action_val=False):\n",
    "    action_values=[] #store the value of each action from this state\n",
    "    for test_action in range(4): #for each action\n",
    "        key = pickle.dumps(np.array((*np.copy(state).flatten(), test_action))) #calculate the key for our dictionary\n",
    "        if key not in q_table: q_table[key] = 0 #if unseen state-action pair, initialize q value to 0\n",
    "        action_values.append(q_table[key]) #append the value of this action to a list\n",
    "    policy_action = np.argmax(action_values) #get an action by performing argmax operation\n",
    "    if return_action_val: return policy_action, action_values[policy_action] #if flag, return value of action aswell\n",
    "    return policy_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_epsilon_greedy_policy(policy):\n",
    "    def epsilon_greedy_policy(state):\n",
    "        action = env.action_space.sample() if np.random.rand()<epsilon else policy(state) #epsilon greedy policy\n",
    "        return action\n",
    "    return epsilon_greedy_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(episode_mem, q_table, discount_factor=0.95, alpha=0.2):\n",
    "    all_diffs=[] #store difference between new and old q values\n",
    "    for i, mem in reversed(list(enumerate(episode_mem))): #iterate over the memories in reverse chronological order\n",
    "        if i==len(episode_mem)-1: #if terminal state\n",
    "            calculated_q = mem['reward'] #set q = the reward in that memory\n",
    "        else:#if non-terminal state\n",
    "            _, next_obs_q = greedy_policy(mem['new_observation'], return_action_val=True) #get q value of next state\n",
    "            calculated_q = mem['reward']+discount_factor*next_obs_q #calculate new q value estimate for this state-action pair\n",
    "        \n",
    "        key = pickle.dumps(np.array((*mem['observation'].flatten(), mem['action']))) #get key of current state-action pair\n",
    "        if key not in q_table: q_table[key]=0 #if unseen state-action pair, initialize q value to 0\n",
    "        new_val = q_table[key] + alpha*(calculated_q-q_table[key]) #update q with a step of size alpha to new q value\n",
    "        diff = abs(q_table[key]-new_val) #calculate difference between old and new q values estimate\n",
    "        all_diffs.append(diff)\n",
    "        q_table[key] = new_val\n",
    "    return q_table, np.mean(all_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_episode=0\n",
    "epsilon = 1 #initialize epsilon\n",
    "q_table = {} #initialize q table\n",
    "discount_factor=0.95\n",
    "alpha=0.1\n",
    "\n",
    "env = GriddyEnv()\n",
    "epsilon_greedy_policy = create_epsilon_greedy_policy(greedy_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(policy, n_episodes=100):\n",
    "    global epsilon\n",
    "    global q_table\n",
    "    global i_episode\n",
    "    try:\n",
    "        for _ in range(n_episodes):\n",
    "            observation = env.reset()\n",
    "            episode_mem = []\n",
    "            done=False\n",
    "            t=0\n",
    "            while not done:\n",
    "                action = policy(observation)\n",
    "                new_observation, reward, done, info = env.step(action)\n",
    "                episode_mem.append({'observation':observation,\n",
    "                                    'action':action,\n",
    "                                    'reward':reward,\n",
    "                                    'new_observation':new_observation,\n",
    "                                    'done':done})\n",
    "                observation=new_observation\n",
    "                t+=1\n",
    "            epsilon*=0.995 #decay epsilon\n",
    "            q_table, q_delta = update_q_table(episode_mem, q_table, discount_factor, alpha) #update our q table using the current episode memory\n",
    "            i_episode+=1\n",
    "            print(\"Episode {} finished after {} timesteps. Epsilon={}. Q_Delta={}\".format(i_episode, t, epsilon, q_delta))#, end='\\r')\n",
    "        env.close()\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(epsilon_greedy_policy, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished after 5 timesteps\n"
     ]
    }
   ],
   "source": [
    "visualise_agent(greedy_policy, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Learning\n",
    "\n",
    "Instead of using a table to store our q values for each state, which becomes computationally inefficient when we have a large state space, we can use a neural network. This is not a problem for NNs as we don't need to store the value for each action-state pair explicity. We also improve the performance of our agent in unseen states. This is because, in the tabular method, we assume a q value of 0 for unseen action-state pairs while our network will make a guess based on similar state-action pairs it has seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(np.prod(env.observation_space.shape), 32) #input layer\n",
    "        self.fc2 = torch.nn.Linear(32, 32)\n",
    "        self.fc3 = torch.nn.Linear(32, env.action_space.n) #output layer\n",
    "    def forward(self, obs):\n",
    "        obs = obs.view(-1, np.prod(env.observation_space.shape)) #flatten input\n",
    "        x = F.relu(self.fc1(obs))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    def create_optimizer(self, lr=0.001):\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_greedy_policy(q_network):\n",
    "    def greedy_policy(state, return_action_val=False):\n",
    "        action_values = q_network(torch.tensor(state).double()).detach().numpy()\n",
    "        policy_action = np.argmax(action_values)\n",
    "        if return_action_val: return policy_action, action_values[0][policy_action]\n",
    "        return policy_action\n",
    "    return greedy_policy\n",
    "\n",
    "def create_stochastic_policy(q_network):\n",
    "    def stochastic_policy(state, return_action_val=False):\n",
    "        action_values = q_network(torch.tensor(state).double()).detach().numpy()\n",
    "        action_probs = F.softmax(torch.tensor(action_values), dim=-1)\n",
    "        policy_action = torch.distributions.Categorical(action_probs).sample().item()\n",
    "        if return_action_val: return policy_action, action_values[0][policy_action]\n",
    "        return policy_action\n",
    "    return stochastic_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_network(episode_mem, q_network, discount_factor=0.95, alpha=0.2):\n",
    "    all_diffs=[] #store difference between new and old q values\n",
    "    for i, mem in reversed(list(enumerate(episode_mem))): #iterate over the memories in reverse chronological order\n",
    "        if i==len(episode_mem)-1: #if terminal state\n",
    "            calculated_q = mem['reward'] #set q = the reward in that memory\n",
    "        else:#if non-terminal state\n",
    "            _, next_obs_q = greedy_policy(mem['new_observation'], return_action_val=True) #get q value of next state\n",
    "            calculated_q = mem['reward']+discount_factor*next_obs_q #calculate new q value estimate for this state-action pair\n",
    "            \n",
    "        predicted_old_q = q_network(torch.tensor(mem['observation']).double())[0, mem['action']] #what does our network predict for the current state-value pair\n",
    "        label_new_q = predicted_old_q.item() + alpha*(calculated_new_q-predicted_old_q.item()) #what should our label be for the network given our new prediction of q\n",
    "        cost = F.mse_loss(predicted_old_q, torch.tensor(label_new_q).double()) #calculate cost\n",
    "        cost.backward() #calculate gradients\n",
    "        q_network.optimizer.step() #update weights\n",
    "        q_network.optimizer.zero_grad() #reset gradients\n",
    "        all_diffs.append(abs(calculated_new_q-predicted_old_q.item()))\n",
    "    return np.mean(all_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPER-PARAMS\n",
    "epsilon = 1\n",
    "i_episode=0\n",
    "discount_factor=0.95\n",
    "alpha=0.1\n",
    "lr = 0.001\n",
    "\n",
    "env = GriddyEnv(4, 4)\n",
    "q_network = QNetwork().double()\n",
    "q_network.create_optimizer(lr)\n",
    "greedy_policy = create_greedy_policy(q_network)\n",
    "stochastic_policy = create_stochastic_policy(q_network)\n",
    "epsilon_greedy_policy = create_epsilon_greedy_policy(greedy_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deep(policy, n_episodes=100):\n",
    "    global epsilon\n",
    "    global q_network\n",
    "    global i_episode\n",
    "    try:\n",
    "        for _ in range(n_episodes):\n",
    "            observation = env.reset()\n",
    "            episode_mem = []\n",
    "            done=False\n",
    "            t=0\n",
    "            while not done:\n",
    "                action = policy(observation)\n",
    "                new_observation, reward, done, info = env.step(action)\n",
    "                episode_mem.append({'observation':observation,\n",
    "                                    'action':action,\n",
    "                                    'reward':reward,\n",
    "                                    'new_observation':new_observation,\n",
    "                                    'done':done})\n",
    "                observation=new_observation\n",
    "                t+=1\n",
    "            epsilon*=0.995\n",
    "            q_delta = update_q_network(episode_mem, q_network, discount_factor, alpha) #update our q network using the current episode memory\n",
    "            i_episode+=1\n",
    "            print(\"Episode {} finished after {} timesteps. Epsilon={}. Q_Delta={}\".format(i_episode, t, epsilon, q_delta))#, end='\\r')\n",
    "            #print(value_table_viz(value_table))\n",
    "            #print()\n",
    "            #env.render(value_table_viz(value_network, observation))\n",
    "        env.close()\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_deep(epsilon_greedy_policy, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished after 3 timesteps\n",
      "Episode 1 finished after 1 timesteps\n",
      "Episode 2 finished after 4 timesteps\n"
     ]
    }
   ],
   "source": [
    "visualise_agent(greedy_policy, n=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
