{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients\n",
    "\n",
    "Prerequisites:\n",
    "- [Value based methods](https://theaicore.com/app/training/intro-to-rl)\n",
    "- [Neural Networks](https://theaicore.com/app/training/neural-networks)\n",
    "\n",
    "Previously we looked at value based methods; those which estimated how good certain states were (value function) and how good certain actions were from certain states (action-value or Q function).\n",
    "\n",
    "In this notebook we'll look at policy gradient based methods\n",
    "\n",
    "## What's the goal of reinforcement learning?\n",
    "\n",
    "The goal of a reinforcement learning agent is to maximise expected reward over it's lifetime.\n",
    "What the agent experiences over it's lifetime, including rewards, states and actions defines it's *trajectory*.\n",
    "The trajectories that an agent might experience depend on what actions it takes from any given state, that is, what policy the agent follows.\n",
    "\n",
    "We can formulate this as below.\n",
    "\n",
    "![](./images/policy-gradient-objective.jpg)\n",
    "\n",
    "Where the policy is a function with parameters $\\theta$.\n",
    "\n",
    "What we'd like to do, is to find parameters that maximise this objective, $J$, and hence find an optimal parameterisation for our poilcy.\n",
    "\n",
    "Because the objective if fully differentiable, we can use gradient **ascent** to improve our objective with respect to our parameters.\n",
    "\n",
    "Below we analytically derive the gradient of the objective with respect to the parameters.\n",
    "\n",
    "![](./images/policy-gradient-derivation.jpg)\n",
    "\n",
    "Now we can use this derivative in our gradient ascent update rule to adjust the weights in a direction that should increase the objective. \n",
    "Note the update is in the direction of the gradient because this is gradient ascent, not descent. \n",
    "That's because the objective represents our expected reward which we want to maximise, rather than a loss which we might want to minimise in a different case.\n",
    "\n",
    "![](./images/policy-gradient-update.jpg)\n",
    "\n",
    "This algorithm is called REINFORCE (REward Increment = Nonnegative Factor $\\times$ Offset Reinforcement $\\times$ Characteristic Eligibility). This name describes the structure of the parameter updates. But don't worry about the acronym.\n",
    "\n",
    "Let's build a neural network which will act as our agent's policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, layers, embedding=False, distribution=False):\n",
    "        super().__init__()\n",
    "        l = []\n",
    "        for idx in range(len(layers) - 1):\n",
    "            l.append(torch.nn.Linear(layers[idx], layers[idx+1]))   # add a linear layer\n",
    "            if idx + 1 != len(layers) - 1: # if this is not the last layer ( +1 = zero indexed) (-1 = layer b4 last)\n",
    "                l.append(torch.nn.ReLU())   # activate\n",
    "        if distribution:    # if a probability dist output is required\n",
    "            l.append(torch.nn.Softmax())    # apply softmax to output\n",
    "            \n",
    "        self.layers = torch.nn.Sequential(*l) # unpack layers & turn into a function which applies them sequentially \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this neural network to model the policy which will control our agent in Griddy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from GriddyEnv import GriddyEnv # get griddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 AVG REWARD: -59.43\n",
      "EPOCH: 1 AVG REWARD: -24.87\n",
      "EPOCH: 2 AVG REWARD: -16.47\n",
      "EPOCH: 3 AVG REWARD: -15.67\n",
      "EPOCH: 4 AVG REWARD: -18.03\n",
      "EPOCH: 5 AVG REWARD: -9.80\n",
      "EPOCH: 6 AVG REWARD: -14.90\n",
      "EPOCH: 7 AVG REWARD: -13.97\n",
      "EPOCH: 8 AVG REWARD: -16.97\n",
      "EPOCH: 9 AVG REWARD: -11.63\n",
      "EPOCH: 10 AVG REWARD: -12.10\n",
      "EPOCH: 11 AVG REWARD: -12.50\n",
      "EPOCH: 12 AVG REWARD: -12.57\n",
      "EPOCH: 13 AVG REWARD: -13.97\n",
      "EPOCH: 14 AVG REWARD: -8.73\n",
      "EPOCH: 15 AVG REWARD: -8.33\n",
      "EPOCH: 16 AVG REWARD: -6.90\n",
      "EPOCH: 17 AVG REWARD: -8.50\n",
      "EPOCH: 18 AVG REWARD: -13.33\n",
      "EPOCH: 19 AVG REWARD: -9.03\n",
      "EPOCH: 20 AVG REWARD: -8.83\n",
      "EPOCH: 21 AVG REWARD: -8.60\n",
      "EPOCH: 22 AVG REWARD: -10.07\n",
      "EPOCH: 23 AVG REWARD: -8.13\n",
      "EPOCH: 24 AVG REWARD: -9.80\n",
      "EPOCH: 25 AVG REWARD: -7.53\n",
      "EPOCH: 26 AVG REWARD: -7.13\n",
      "EPOCH: 27 AVG REWARD: -7.27\n",
      "EPOCH: 28 AVG REWARD: -7.20\n",
      "EPOCH: 29 AVG REWARD: -5.70\n"
     ]
    }
   ],
   "source": [
    "def train(env, optimiser, agent_tag, epochs=100, episodes=30, use_baseline=False, use_causality=False):\n",
    "    assert not (use_baseline and use_causality)   # cant implement both simply\n",
    "    baseline = 0\n",
    "    try:\n",
    "        for epoch in range(epochs):\n",
    "            avg_reward = 0\n",
    "            objective = 0\n",
    "            for episode in range(episodes):\n",
    "                done = False\n",
    "                state = env.reset()\n",
    "                log_policy = []\n",
    "\n",
    "                rewards = []\n",
    "\n",
    "                step = 0\n",
    "\n",
    "                # RUN AN EPISODE\n",
    "                while not done:     # while the episode is not terminated\n",
    "                    state = torch.Tensor(state)     # correct data type for passing to model\n",
    "                    # print('STATE:', state)\n",
    "                    state = state.view(np.prod(state.shape))\n",
    "\n",
    "                    action_distribution = policy(state)     # get a distribution over actions from the policy given the state\n",
    "                    # print('ACTION DISTRIBUTION:', action_distribution)\n",
    "\n",
    "                    action = torch.distributions.Categorical(action_distribution).sample()      # sample from that distrbution\n",
    "                    action = int(action)\n",
    "                    # print('ACTION:', action)\n",
    "\n",
    "                    new_state, reward, done, info = env.step(action)    # take timestep\n",
    "\n",
    "                    rewards.append(reward)\n",
    "\n",
    "                    state = new_state\n",
    "                    log_policy.append(torch.log(action_distribution[action]))\n",
    "\n",
    "                    step += 1\n",
    "                    if done:\n",
    "                        break\n",
    "                    if step > 10000000:\n",
    "                        # break\n",
    "                        pass\n",
    "\n",
    "                avg_reward += ( sum(rewards) - avg_reward ) / ( episode + 1 )   # accumulate avg reward\n",
    "                writer.add_scalar(f'{agent_tag}/Reward/Train', avg_reward, epoch*episodes + episode)     # plot the latest reward\n",
    "\n",
    "                # update baseline\n",
    "                if use_baseline:\n",
    "                    baseline += ( sum(rewards) - baseline ) / (epoch*episodes + episode + 1)    # accumulate average return  \n",
    "\n",
    "                for idx in range(len(rewards)):     # for each timestep experienced in the episode\n",
    "                    # add causality\n",
    "                    if use_causality:   \n",
    "                        weight = sum(rewards[idx:])     # only weight the log likelihood of this action by the future rewards, not the total\n",
    "                    else:\n",
    "                        weight = sum(rewards) - baseline           # weight by the total reward from this episode\n",
    "                    objective += log_policy[idx] * weight   # add the weighted log likelihood of this taking action to \n",
    "\n",
    "\n",
    "            objective /= episodes   # average over episodes\n",
    "            objective *= -1     # invert to represent reward rather than cost\n",
    "\n",
    "\n",
    "            # UPDATE POLICY\n",
    "            # print('updating policy')\n",
    "            print('EPOCH:', epoch, f'AVG REWARD: {avg_reward:.2f}')\n",
    "            objective.backward()    # backprop\n",
    "            optimiser.step()    # update params\n",
    "            optimiser.zero_grad()   # reset gradients to zero\n",
    "\n",
    "            # VISUALISE AT END OF EPOCH AFTER UPDATING POLICY\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                env.render()\n",
    "                state = torch.Tensor(state)\n",
    "                state = state.view(np.prod(state.shape))\n",
    "                action_distribution = policy(state)\n",
    "                action = torch.distributions.Categorical(action_distribution).sample()\n",
    "                action = int(action)\n",
    "                state, reward, done, info = env.step(action)\n",
    "                sleep(0.01)\n",
    "    except KeyboardInterrupt:\n",
    "        print('interrupted')\n",
    "        env.close()\n",
    "\n",
    "    env.close()\n",
    "    checkpoint = {\n",
    "        'model': policy,\n",
    "        'state_dict': policy.state_dict() \n",
    "    }\n",
    "    torch.save(checkpoint, f'agents/trained-agent-{agent_tag}.pt')\n",
    "\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "env = GriddyEnv(time_penalty=True)\n",
    "\n",
    "policy = NN([np.prod(env.observation_space.shape), 32, env.action_space.n], distribution=True)\n",
    "\n",
    "lr = 0.001\n",
    "weight_decay = 1\n",
    "optimiser = torch.optim.SGD(policy.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "agent_tag = 'griddy'\n",
    "\n",
    "train(\n",
    "    env,\n",
    "    optimiser,\n",
    "    agent_tag,\n",
    "    use_baseline=True,\n",
    "    use_causality=False,\n",
    "    epochs=30,\n",
    "    episodes=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we solve a harder challenge?\n",
    "\n",
    "Now let's move onto a more challenging environment called CartPole. The aim is to have a cart move along one dimension and balance an inverted pole vertically. Note that because we set up the size of the policy network programmatically, we can use exactly the same training loop! This is the first time we've used the same learning algorithm to solve different environments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 AVG REWARD: 22.77\n",
      "EPOCH: 1 AVG REWARD: 19.73\n",
      "EPOCH: 2 AVG REWARD: 21.67\n",
      "EPOCH: 3 AVG REWARD: 27.17\n",
      "EPOCH: 4 AVG REWARD: 25.23\n",
      "EPOCH: 5 AVG REWARD: 24.13\n",
      "EPOCH: 6 AVG REWARD: 25.97\n",
      "EPOCH: 7 AVG REWARD: 26.87\n",
      "EPOCH: 8 AVG REWARD: 26.53\n",
      "EPOCH: 9 AVG REWARD: 27.90\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter() # create new tensorboard writer\n",
    "\n",
    "env = gym.make('CartPole-v0') # make cartpole environment\n",
    "\n",
    "policy = NN([np.prod(env.observation_space.shape), 32, env.action_space.n], distribution=True) \n",
    "\n",
    "lr = 0.001\n",
    "weight_decay = 1\n",
    "optimiser = torch.optim.SGD(policy.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "agent_tag = 'cartpole'\n",
    "\n",
    "train(\n",
    "    env,\n",
    "    optimiser,\n",
    "    agent_tag,\n",
    "    use_baseline=True,\n",
    "    use_causality=False,\n",
    "    epochs=10,\n",
    "    episodes=30\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we improve this learning algorithm?\n",
    "\n",
    "What we just implemented is the vanilla policy gradient algorithm.\n",
    "\n",
    "Notice anything that you think could be improved?\n",
    "\n",
    "What are we weighting the likelihood of taking each trajectory by?\n",
    "\n",
    "### Baselines\n",
    "\n",
    "What if all of the trajectories receive a similar total reward (e.g. \"*bad*\" trajectories give 99 total reward and \"good\" trajectories give 100 total reward)?\n",
    "In this case the likelihood of all trajectories will be increased.\n",
    "\n",
    "What if the total reward over every trajectory is negative (e.g. negative reward every timestep and zero upon reaching a terminal state)?\n",
    "In any of these cases the log probability of ANY trajectory taken will be reduced by the updates.\n",
    "\n",
    "We don't want any of these things to happen, so we can introduce baselines. Baselines help you to see the reward from the current trajectory in the context of the reward from each of the others, giving you a relative measure of how good they were, not just an absolute measure.\n",
    "\n",
    "![](./images/policy-gradient-baseline.jpg)\n",
    "\n",
    "A pretty standard baseline to use is the average baseline. But there are others.\n",
    "\n",
    "![](./images/policy-gradient-average-baseline.jpg)\n",
    "\n",
    "Even though we adjust the objective, it remains unbiased in expectation because the expectation of the baseline is zero\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causality\n",
    "What rewards can an action take responsibilty for?\n",
    "Surely an reward received before an action taken later in the trajectory shouldn't indicate that the later action was good. Whatever the action taken later in time was, this reward was received before then.\n",
    "\n",
    "To account for this, we should only weight how good the action was by the rewards which it led the agent to receive from that point in time onwards. The rewards attained before the action was taken are not eligible for making that action more likely; that action was taken after the reward was received, so it can't be accountable for it.\n",
    "\n",
    "![](./images/policy-gradient-causality.jpg)\n",
    "### Can we combine both?\n",
    "To combine both, we would need to have a baseline for each point in time; how much reward can I expect to get from timestep t? For any games without a fixed episode length, or even worse games with an infinite horizon, we'll need to compute a baseline for each timestep. This makes combining causality and baselines difficult, but we'll see how to achieve this in a later notebook, combining policy gradients with some things we've learnt in previous notebooks. (hint: what function represents the same thing as a lookup table for baselines?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now add options to our training function for our algorithm to use baselines and causality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 AVG REWARD: 19.57\n",
      "EPOCH: 1 AVG REWARD: 21.50\n",
      "EPOCH: 2 AVG REWARD: 21.37\n",
      "EPOCH: 3 AVG REWARD: 19.23\n",
      "EPOCH: 4 AVG REWARD: 24.43\n",
      "EPOCH: 5 AVG REWARD: 22.63\n",
      "EPOCH: 6 AVG REWARD: 21.40\n",
      "EPOCH: 7 AVG REWARD: 23.53\n",
      "EPOCH: 8 AVG REWARD: 20.53\n",
      "EPOCH: 9 AVG REWARD: 21.83\n",
      "EPOCH: 10 AVG REWARD: 25.43\n",
      "EPOCH: 11 AVG REWARD: 21.90\n",
      "EPOCH: 12 AVG REWARD: 24.47\n",
      "EPOCH: 13 AVG REWARD: 20.33\n",
      "EPOCH: 14 AVG REWARD: 21.93\n",
      "EPOCH: 15 AVG REWARD: 23.20\n",
      "EPOCH: 16 AVG REWARD: 27.63\n",
      "EPOCH: 17 AVG REWARD: 25.70\n",
      "EPOCH: 18 AVG REWARD: 24.17\n",
      "EPOCH: 19 AVG REWARD: 29.67\n",
      "EPOCH: 20 AVG REWARD: 27.80\n",
      "EPOCH: 21 AVG REWARD: 30.17\n",
      "EPOCH: 22 AVG REWARD: 30.27\n",
      "EPOCH: 23 AVG REWARD: 36.73\n",
      "EPOCH: 24 AVG REWARD: 33.40\n",
      "EPOCH: 25 AVG REWARD: 30.27\n",
      "EPOCH: 26 AVG REWARD: 33.27\n",
      "EPOCH: 27 AVG REWARD: 37.60\n",
      "EPOCH: 28 AVG REWARD: 41.77\n",
      "EPOCH: 29 AVG REWARD: 41.60\n"
     ]
    }
   ],
   "source": [
    "def train(env, optimiser, agent_tag, epochs=100, episodes=30, use_baseline=False, use_causality=False):\n",
    "    assert not (use_baseline and use_causality)   # cant implement both simply\n",
    "    baseline = 0\n",
    "    try:\n",
    "        for epoch in range(epochs):\n",
    "            avg_reward = 0\n",
    "            objective = 0\n",
    "            for episode in range(episodes):\n",
    "                done = False\n",
    "                state = env.reset()\n",
    "                log_policy = []\n",
    "\n",
    "                rewards = []\n",
    "\n",
    "                step = 0\n",
    "\n",
    "                # RUN AN EPISODE\n",
    "                while not done:     # while the episode is not terminated\n",
    "                    state = torch.Tensor(state)     # correct data type for passing to model\n",
    "                    # print('STATE:', state)\n",
    "                    state = state.view(np.prod(state.shape))\n",
    "\n",
    "                    action_distribution = policy(state)     # get a distribution over actions from the policy given the state\n",
    "                    # print('ACTION DISTRIBUTION:', action_distribution)\n",
    "\n",
    "                    action = torch.distributions.Categorical(action_distribution).sample()      # sample from that distrbution\n",
    "                    action = int(action)\n",
    "                    # print('ACTION:', action)\n",
    "\n",
    "                    new_state, reward, done, info = env.step(action)    # take timestep\n",
    "\n",
    "                    rewards.append(reward)\n",
    "\n",
    "                    state = new_state\n",
    "                    log_policy.append(torch.log(action_distribution[action]))\n",
    "\n",
    "                    step += 1\n",
    "                    if done:\n",
    "                        break\n",
    "                    if step > 10000000:\n",
    "                        # break\n",
    "                        pass\n",
    "\n",
    "                avg_reward += ( sum(rewards) - avg_reward ) / ( episode + 1 )   # accumulate avg reward\n",
    "                writer.add_scalar(f'{agent_tag}/Reward/Train', avg_reward, epoch*episodes + episode)     # plot the latest reward\n",
    "\n",
    "                # update baseline\n",
    "                if use_baseline:\n",
    "                    baseline += ( sum(rewards) - baseline ) / (epoch*episodes + episode + 1)    # accumulate average return  \n",
    "\n",
    "                for idx in range(len(rewards)):     # for each timestep experienced in the episode\n",
    "                    # add causality\n",
    "                    if use_causality:   \n",
    "                        weight = sum(rewards[idx:])     # only weight the log likelihood of this action by the future rewards, not the total\n",
    "                    else:\n",
    "                        weight = sum(rewards) - baseline           # weight by the total reward from this episode\n",
    "                    objective += log_policy[idx] * weight   # add the weighted log likelihood of this taking action to \n",
    "\n",
    "\n",
    "            objective /= episodes   # average over episodes\n",
    "            objective *= -1     # invert to represent reward rather than cost\n",
    "\n",
    "\n",
    "            # UPDATE POLICY\n",
    "            # print('updating policy')\n",
    "            print('EPOCH:', epoch, f'AVG REWARD: {avg_reward:.2f}')\n",
    "            objective.backward()    # backprop\n",
    "            optimiser.step()    # update params\n",
    "            optimiser.zero_grad()   # reset gradients to zero\n",
    "\n",
    "            # VISUALISE AT END OF EPOCH AFTER UPDATING POLICY\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                env.render()\n",
    "                state = torch.Tensor(state)\n",
    "                state = state.view(np.prod(state.shape))\n",
    "                action_distribution = policy(state)\n",
    "                action = torch.distributions.Categorical(action_distribution).sample()\n",
    "                action = int(action)\n",
    "                state, reward, done, info = env.step(action)\n",
    "                sleep(0.01)\n",
    "    except KeyboardInterrupt:\n",
    "        print('interrupted')\n",
    "        env.close()\n",
    "\n",
    "    env.close()\n",
    "    checkpoint = {\n",
    "        'model': policy,\n",
    "        'state_dict': policy.state_dict() \n",
    "    }\n",
    "    torch.save(checkpoint, f'agents/trained-agent-{agent_tag}.pt')\n",
    "\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "policy = NN([np.prod(env.observation_space.shape), 32, env.action_space.n], distribution=True)\n",
    "\n",
    "lr = 0.001\n",
    "weight_decay = 1\n",
    "optimiser = torch.optim.SGD(policy.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "agent_tag = 'cartpole-improved'\n",
    "\n",
    "train(\n",
    "    env,\n",
    "    optimiser,\n",
    "    agent_tag,\n",
    "    use_baseline=True,\n",
    "    use_causality=False,\n",
    "    epochs=30,\n",
    "    episodes=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How far can we push this algorithm?\n",
    "\n",
    "Let's try yet another environment, where our agent has to control a spacecraft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 AVG REWARD: -219.82\n",
      "EPOCH: 1 AVG REWARD: -211.72\n",
      "EPOCH: 2 AVG REWARD: -273.54\n",
      "EPOCH: 3 AVG REWARD: -154.07\n",
      "EPOCH: 4 AVG REWARD: -114.12\n",
      "EPOCH: 5 AVG REWARD: -147.29\n",
      "EPOCH: 6 AVG REWARD: -136.77\n",
      "EPOCH: 7 AVG REWARD: -143.92\n",
      "EPOCH: 8 AVG REWARD: -130.39\n",
      "EPOCH: 9 AVG REWARD: -134.38\n",
      "EPOCH: 10 AVG REWARD: -120.44\n",
      "EPOCH: 11 AVG REWARD: -114.12\n",
      "EPOCH: 12 AVG REWARD: -127.83\n",
      "EPOCH: 13 AVG REWARD: -131.89\n",
      "EPOCH: 14 AVG REWARD: -124.84\n",
      "EPOCH: 15 AVG REWARD: -129.16\n",
      "EPOCH: 16 AVG REWARD: -123.58\n",
      "EPOCH: 17 AVG REWARD: -132.09\n",
      "EPOCH: 18 AVG REWARD: -126.34\n",
      "EPOCH: 19 AVG REWARD: -136.73\n",
      "EPOCH: 20 AVG REWARD: -134.17\n",
      "EPOCH: 21 AVG REWARD: -125.20\n",
      "EPOCH: 22 AVG REWARD: -140.15\n",
      "EPOCH: 23 AVG REWARD: -130.41\n",
      "EPOCH: 24 AVG REWARD: -134.21\n",
      "EPOCH: 25 AVG REWARD: -130.60\n",
      "EPOCH: 26 AVG REWARD: -120.91\n",
      "EPOCH: 27 AVG REWARD: -122.27\n",
      "EPOCH: 28 AVG REWARD: -125.91\n",
      "EPOCH: 29 AVG REWARD: -132.34\n",
      "EPOCH: 30 AVG REWARD: -128.91\n",
      "EPOCH: 31 AVG REWARD: -120.35\n",
      "EPOCH: 32 AVG REWARD: -120.66\n",
      "EPOCH: 33 AVG REWARD: -116.15\n",
      "EPOCH: 34 AVG REWARD: -128.25\n",
      "EPOCH: 35 AVG REWARD: -128.12\n",
      "EPOCH: 36 AVG REWARD: -133.33\n",
      "EPOCH: 37 AVG REWARD: -132.48\n",
      "EPOCH: 38 AVG REWARD: -138.03\n",
      "EPOCH: 39 AVG REWARD: -127.47\n",
      "EPOCH: 40 AVG REWARD: -124.43\n",
      "EPOCH: 41 AVG REWARD: -136.72\n",
      "EPOCH: 42 AVG REWARD: -121.62\n",
      "interrupted\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter() # create new tensorboard writer\n",
    "\n",
    "env = gym.make('LunarLander-v2') # make cartpole environment\n",
    "\n",
    "policy = NN([np.prod(env.observation_space.shape), 32, env.action_space.n], distribution=True) \n",
    "\n",
    "lr = 0.001\n",
    "weight_decay = 1\n",
    "optimiser = torch.optim.SGD(policy.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "agent_tag = 'lunar-lander'\n",
    "\n",
    "train(\n",
    "    env,\n",
    "    optimiser,\n",
    "    agent_tag,\n",
    "    use_baseline=True,\n",
    "    use_causality=False,\n",
    "    epochs=100,\n",
    "    episodes=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE Algorithm summary\n",
    "\n",
    "### Online or offline?\n",
    "There is a distinction between collecting experience and updating the policy. So REINFORCE is offline.\n",
    "\n",
    "### Model based or model free?\n",
    "There's no mention of a transition function in this algorithm, so it's model free!\n",
    "\n",
    "### On-policy or off-policy?\n",
    "The gradient signal comes from rewards obtained on a trajectory that was produced by following the current policy. So REINFORCE is on-policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying our trained agents\n",
    "\n",
    "The goal of all of this has been to produce agents that are ready to go and do things autonomously. So let's write a function that does that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "DeprecatedEnv",
     "evalue": "Env LunarLander-v0 not found (valid versions include ['LunarLander-v2'])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mspec\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'LunarLander-v0'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDeprecatedEnv\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-f705f1af4422>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mlunar_lander_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LunarLander-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mlunar_lander_agent_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'agents/trained-agent-lunar-lander.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mdeploy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlunar_lander_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlunar_lander_agent_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Making new env: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m# We used to have people override _reset/_step rather than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mspec\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    138\u001b[0m                              if env_name == valid_env_spec._env_name]\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmatching_envs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeprecatedEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Env {} not found (valid versions include {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatching_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnregisteredEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No registered env with id: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDeprecatedEnv\u001b[0m: Env LunarLander-v0 not found (valid versions include ['LunarLander-v2'])"
     ]
    }
   ],
   "source": [
    "def deploy(env, saved_model):\n",
    "    \n",
    "#     policy = NN([np.prod(env.observation_space.shape), 32, env.action_space.n], distribution=True) # we must remember the architecture\n",
    "    policy = saved_model['model']\n",
    "    policy.load_state_dict(saved_model['state_dict']) # load in our pre-trained model\n",
    "    policy.eval() # put our model in evaluation mode\n",
    "    try:\n",
    "        for episode in range(100): # keep demonstrating your skills\n",
    "                done = False # not done yet\n",
    "                observation = env.reset() # initialise the environemt\n",
    "                while not done: # until the episode is over\n",
    "                    observation = torch.Tensor(observation) # turn observation to tensor\n",
    "                    observation = observation.view(np.prod(observation.shape)) # view observation as vector\n",
    "                    action_distribution = policy(observation) # infer what actions to take with what probability\n",
    "                    action = torch.distributions.Categorical(action_distribution).sample() # sample an action from that distribution\n",
    "                    action = int(action) # make it an int not a float\n",
    "                    observation, reward, done, info = env.step(action) # take an action and transition the environment\n",
    "                    env.render() # show us the environment\n",
    "                    sleep(0.01)\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()\n",
    "       \n",
    "griddy_env = GriddyEnv()\n",
    "griddy_agent_params = torch.load('agents/trained-agent-griddy.pt')\n",
    "deploy(griddy_env, griddy_agent_params)\n",
    "\n",
    "cartpole_env = gym.make('CartPole-v0')\n",
    "cartpole_agent_params = torch.load('agents/trained-agent-cartpole.pt')\n",
    "deploy(cartpole_env, cartpole_agent_params)\n",
    "\n",
    "\n",
    "lunar_lander_env = gym.make('LunarLander-v2')\n",
    "lunar_lander_agent_params = torch.load('agents/trained-agent-lunar-lander.pt')\n",
    "deploy(lunar_lander_env, lunar_lander_agent_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Any final words?\n",
    "\n",
    "Great, so policy gradient methods can at least do as well as the previous algorithms that we've seen.\n",
    "\n",
    "Policy gradient based methods depend on the objective being differentiable with respect to the policy parameters.\n",
    "\n",
    "For any RL agent, the objective (total expected reward attained) is produced as a result of following some policy. If the policy is better, then this objective will be larger.\n",
    "\n",
    "In policy and value iteration (value based techniques), the policy was produced as a result of taking the action with the max state-value. This max operation is not differentiable, and so neither was the policy. For policy gradients to be followed, we must have a differentiable policy.\n",
    "\n",
    "Policy gradient methods will also work with a partially observable environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "- [Trust Region Policy Optimisation (TRPO)]()\n",
    "- [Upside Down RL (UDRL)]()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
