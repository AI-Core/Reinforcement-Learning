{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is ⅂ᴚ?\n",
    "\n",
    "So far, we have modelled the policy, value and q functions. In this session, we will be using an approach to RL called Upside Down RL where we model the behaviour function.\n",
    "\n",
    "This was outlined in Schmidhuber's December 2019 paper [Reinforcement Learning Upside Down:\n",
    "Don’t Predict Rewards - Just Map Them to Actions](https://arxiv.org/pdf/1912.02875.pdf). <br>\n",
    "The specific implementation we are following is outlined in the following paper: [Training Agents using Upside-Down Reinforcement Learning](https://arxiv.org/pdf/1912.02877.pdf).\n",
    "\n",
    "### The Behaviour Function\n",
    "The behaviour function takes as input the current state and a command, and is trained to output a probability distribution over the actions which lead to that command being fulfilled. The command in this implementation takes the form of two scalars - a desired reward to achieve and a time horizon over which to achieve that desired reward.\n",
    "\n",
    "![](images/udrl_q_vs_b.jpg)\n",
    "\n",
    "![](images/udrl_training.jpg)\n",
    "\n",
    "![](images/udrl_optimal_b.jpg)\n",
    "\n",
    "![](images/udrl_algorithm1.jpg)\n",
    "\n",
    "![](images/udrl_algorithm2.jpg)\n",
    "\n",
    "### Implementation details mentioned in paper\n",
    "\n",
    "#### Behaviour Function\n",
    "There is a specific architectural choice used in the paper. The input state and command are transformed by a linear layer and activated using tanh and sigmoid respectively. Then they are multiplied element-wise before being passed on to the next layer in the network.\n",
    "\n",
    "Rupesh, one of the authors commented that \"This is a simple form of gating used in LSTMs (or more broadly, Fast Weights) because we want contextual processing of the state\"\n",
    "\n",
    "We also multiply the desired reward and horizon used for the command by a hyper-parameter called *command_scale* to scale them down.\n",
    "\n",
    "#### Replay Buffer\n",
    "Instead of using a normal replay buffer of the past *replay_size* episodes, we store *replay_size* episodes with the highest returns seen so far. Where *replay_size* is a hyper-parameter and represents the size of our replay buffer.\n",
    "\n",
    "#### Training\n",
    "When sampling values of t1 and t2 for calculating the cost and performing gradient descent on our behaviour functon, we randomly sample t1 but set t2 = T where T is the final time step.\n",
    "\n",
    "#### Sampling exploratory commands\n",
    "When sampling exploratory commands to generate episodes for training, the following produre is used:\n",
    "1. *last_few* episodes from the end of the replay buffer (i.e, with the highest returns) are selected. *last_few* is a hyper-parameter and remains fixed during training.\n",
    "2. The desired horizon is set to the mean lengths of the selected episodes\n",
    "3. The desired return is sampled from the uniform distribution U[M, M+S] where M is the mean and S is the standard deviation of the selected epidoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from copy import deepcopy\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#command takes form [derired reward, desired horizon]\n",
    "def random_policy(obs, command):\n",
    "    return np.random.randint(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualise agent function\n",
    "def visualise_agent(policy, command, n=5):\n",
    "    try:\n",
    "        for trial_i in range(n):\n",
    "            current_command = deepcopy(command)\n",
    "            observation = env.reset()\n",
    "            done=False\n",
    "            t=0\n",
    "            episode_return=0\n",
    "            while not done:\n",
    "                env.render()\n",
    "                action = policy(torch.tensor([observation]).double(), torch.tensor([command]).double())\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                episode_return+=reward\n",
    "                current_command[0]-= reward\n",
    "                current_command[1] = max(1, current_command[1]-1)\n",
    "                t+=1\n",
    "            env.render()\n",
    "            time.sleep(1.5)\n",
    "            print(\"Episode {} finished after {} timesteps. Return = {}\".format(trial_i, t, episode_return))\n",
    "        env.close()\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished after 9 timesteps. Return = 9.0\n",
      "Episode 1 finished after 14 timesteps. Return = 14.0\n",
      "Episode 2 finished after 20 timesteps. Return = 20.0\n"
     ]
    }
   ],
   "source": [
    "visualise_agent(random_policy, command=[500, 500], n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNN_AGENT(torch.nn.Module):\n",
    "    def __init__(self, command_scale):\n",
    "        super().__init__()\n",
    "        embedding_size=32\n",
    "        self.command_scale=command_scale\n",
    "        self.observation_embedding = torch.nn.Sequential(\n",
    "            torch.nn.Linear(np.prod(env.observation_space.shape), embedding_size), #linear transformation to embedding_size\n",
    "            torch.nn.Tanh() #tanh activation\n",
    "        )\n",
    "        self.command_embedding = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2, embedding_size), #linear transformation to embedding_size\n",
    "            torch.nn.Sigmoid() #sigmoid activation\n",
    "        )\n",
    "        self.to_output = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embedding_size, 64), #hidden architecture\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, env.action_space.n) #linear layer to compute output probability logits\n",
    "        )\n",
    "    \n",
    "    def forward(self, observation, command): #takes in observation and command\n",
    "        obs_emebdding = self.observation_embedding(observation) #compute observation embedding\n",
    "        cmd_embedding = self.command_embedding(command*self.command_scale) #computer command embedding\n",
    "        embedding = torch.mul(obs_emebdding, cmd_embedding) #compute element-wise multiplication of observation and command embedding \n",
    "        action_prob_logits = self.to_output(embedding) #compute output from embedding\n",
    "        return action_prob_logits\n",
    "    \n",
    "    def create_optimizer(self, lr):\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr) #create an optimizer object for this network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algorithm 2\n",
    "def collect_experience(policy, replay_buffer, replay_size, last_few, n_episodes=100, log_to_tensorboard=True):\n",
    "    global i_episode\n",
    "    init_replay_buffer = deepcopy(replay_buffer) #make copy of initial replay buffer so we can use it to sample commands\n",
    "    try:\n",
    "        for _ in range(n_episodes):\n",
    "            command = sample_command(init_replay_buffer, last_few) #sample exploratory command\n",
    "            writer.add_scalar('Command desired reward/Episode', command[0], i_episode)    # write desired reward to a graph\n",
    "            writer.add_scalar('Command horizon/Episode', command[1], i_episode)    # write desired horizon to a graph\n",
    "            observation = env.reset()\n",
    "            episode_mem = {'observation':[],\n",
    "                           'action':[],\n",
    "                           'reward':[]} #initialize episode memory\n",
    "            done=False\n",
    "            while not done:\n",
    "                action = policy(torch.tensor([observation]).double(), torch.tensor([command]).double()) #get action from policy\n",
    "                new_observation, reward, done, info = env.step(action) #step in environment\n",
    "                \n",
    "                #append transition to episode memory\n",
    "                episode_mem['observation'].append(observation)\n",
    "                episode_mem['action'].append(action)\n",
    "                episode_mem['reward'].append(reward)\n",
    "                \n",
    "                \n",
    "                observation=new_observation\n",
    "                command[0]-= reward #reduce command reward by reward recieved\n",
    "                command[1] = max(1, command[1]-1) #rewuce command horizon by one as we just took a step\n",
    "            episode_mem['return']=sum(episode_mem['reward']) #store return for current episode to make sorting easier\n",
    "            episode_mem['episode_len']=len(episode_mem['observation']) #store length of memory\n",
    "            replay_buffer.append(episode_mem) #add memory to replay buffer\n",
    "            i_episode+=1\n",
    "            if log_to_tensorboard: writer.add_scalar('Return/Episode', sum(episode_mem['reward']), i_episode)    # write loss to a graph\n",
    "            print(\"Episode {} finished after {} timesteps. Return = {}\".format(i_episode, len(episode_mem['observation']), sum(episode_mem['reward'])))\n",
    "        env.close()\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()\n",
    "    replay_buffer = sorted(replay_buffer, key=lambda x:x['return'])[-replay_size:] #sort replay_buffer by return and truncate to replay_size\n",
    "    return replay_buffer\n",
    "\n",
    "def sample_command(replay_buffer, last_few):\n",
    "    if len(replay_buffer)==0:\n",
    "        return [1, 1]\n",
    "    else:\n",
    "        command_samples = replay_buffer[-last_few:] #select the last_few memories\n",
    "        lengths = [mem['episode_len'] for mem in command_samples] #get lengths of selected episodes\n",
    "        returns = [mem['return'] for mem in command_samples] #get returns of selected episodes\n",
    "        mean_return, std_return = np.mean(returns), np.std(returns) #calculate mean and standard deviation of returns\n",
    "        command_horizon = np.mean(lengths) #calulate mean length of episodes\n",
    "        desired_reward = np.random.uniform(mean_return, mean_return+std_return) #sample desired reward from uniform distribution\n",
    "        return [desired_reward, command_horizon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(behaviour_func, replay_buffer, n_updates=100, batch_size=64, log_to_tensorboard=True):\n",
    "    global i_updates\n",
    "    all_costs = []\n",
    "    for i in range(n_updates): #for each update, we need\n",
    "        batch_observations = np.zeros((batch_size, np.prod(env.observation_space.shape))) #create empty input observations tensor of the correct shape\n",
    "        batch_commands = np.zeros((batch_size, 2)) #create emply input commands tensor of the correct shape\n",
    "        batch_label = np.zeros((batch_size)) ##create emply labels tensor of the correct shape\n",
    "        for b in range(batch_size): #add items to the batch sampled from replay buffer\n",
    "            sample_episode = np.random.randint(0, len(replay_buffer)) #sample episode index\n",
    "            sample_t1 = np.random.randint(0, replay_buffer[sample_episode]['episode_len']) #sample t1\n",
    "            sample_t2 = replay_buffer[sample_episode]['episode_len'] #set t2 = length of the episode\n",
    "            sample_horizon = sample_t2-sample_t1 #calculate horizon from t1 and t2\n",
    "            sample_obs = replay_buffer[sample_episode]['observation'][sample_t1] #sample observation\n",
    "            sample_desired_return = sum(replay_buffer[sample_episode]['reward'][sample_t1:sample_t2]) #sample desired return\n",
    "            label = replay_buffer[sample_episode]['action'][sample_t1] #get label (action)\n",
    "            batch_observations[b] = sample_obs #set the bth batch item observation to the sampled observation\n",
    "            batch_commands[b] = [sample_desired_return, sample_horizon] #set the bth batch item command to the sampled command\n",
    "            batch_label[b] = label #set the bth batch item label to the sampled label\n",
    "        batch_observations = torch.tensor(batch_observations).double() #convert sampled batch observation to double tensor\n",
    "        batch_commands = torch.tensor(batch_commands).double() #convert sampled batch commands to double tensor\n",
    "        batch_label = torch.tensor(batch_label).long() #convert sampled batch label to long tensor\n",
    "        pred = behaviour_func(batch_observations, batch_commands) #make prediction over action distribution using behaviour function\n",
    "        cost = F.cross_entropy(pred, batch_label) #calculate cross entropy loss\n",
    "        if log_to_tensorboard: writer.add_scalar('Cost/NN update', cost.item() , i_updates)    # write loss to a graph\n",
    "        all_costs.append(cost.item()) #append current cost to all_costs\n",
    "        cost.backward() #calculate gradient of cost wrt to weights\n",
    "        behaviour_func.optimizer.step() #take gradient step to update weights\n",
    "        behaviour_func.optimizer.zero_grad() #reset stored gradient to zero\n",
    "        i_updates+=1\n",
    "    return np.mean(all_costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_greedy_policy(behaviour_func):\n",
    "    def policy(obs, command):\n",
    "        action_logits = behaviour_func(obs, command) #get action logits from network\n",
    "        action = np.argmax(action_logits.detach().numpy()) #choose action with highest probability\n",
    "        return action\n",
    "    return policy\n",
    "\n",
    "def create_stochastic_policy(behaviour_func):\n",
    "    def policy(obs, command):\n",
    "        action_logits = behaviour_func(obs, command) #get action logits from network\n",
    "        action_probs = F.softmax(action_logits, dim=-1) #perform softmax on logics to get action probabilities\n",
    "        action = torch.distributions.Categorical(action_probs).sample().item() #sample from our action distribution\n",
    "        return action\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_episode=0\n",
    "i_updates=0 #number of parameter updates to the neural network\n",
    "replay_buffer = [] #initialize replay buffer\n",
    "log_to_tensorboard = True \n",
    "\n",
    "replay_size = 600 #replay buffer size\n",
    "last_few = 75 #last_few to use when sampling exploratory commands\n",
    "batch_size = 32 #batch size when training network\n",
    "n_warm_up_episodes = 50 #number of warm up episodes\n",
    "n_episodes_per_iter = 50 #numbers of episodes per iteration of algorithm 1\n",
    "n_updates_per_iter = 300 #number of gradient updates per iteration of algorithm 1\n",
    "command_scale = 0.01 #number to multiply command by to scale it down\n",
    "lr = 0.001 #learning rate for neural network\n",
    "\n",
    "behaviour_func = FCNN_AGENT(command_scale).double() #initialize behaviour function\n",
    "behaviour_func.create_optimizer(lr) #create behaviour function optimizer\n",
    "\n",
    "stochastic_policy = create_stochastic_policy(behaviour_func) #create stochastic policy\n",
    "greedy_policy = create_greedy_policy(behaviour_func) #create greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET UP TRAINING VISUALISATION\n",
    "# SET UP TRAINING VISUALISATION\n",
    "if log_to_tensorboard: from torch.utils.tensorboard import SummaryWriter\n",
    "if log_to_tensorboard: writer = SummaryWriter() # we will use this to show our models performance on a graph using tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished after 15 timesteps. Return = 15.0\n",
      "Episode 2 finished after 12 timesteps. Return = 12.0\n",
      "Episode 3 finished after 56 timesteps. Return = 56.0\n",
      "Episode 4 finished after 27 timesteps. Return = 27.0\n",
      "Episode 5 finished after 18 timesteps. Return = 18.0\n",
      "Episode 6 finished after 14 timesteps. Return = 14.0\n",
      "Episode 7 finished after 17 timesteps. Return = 17.0\n",
      "Episode 8 finished after 25 timesteps. Return = 25.0\n",
      "Episode 9 finished after 11 timesteps. Return = 11.0\n",
      "Episode 10 finished after 38 timesteps. Return = 38.0\n",
      "Episode 11 finished after 64 timesteps. Return = 64.0\n",
      "Episode 12 finished after 32 timesteps. Return = 32.0\n",
      "Episode 13 finished after 22 timesteps. Return = 22.0\n",
      "Episode 14 finished after 17 timesteps. Return = 17.0\n",
      "Episode 15 finished after 20 timesteps. Return = 20.0\n",
      "Episode 16 finished after 42 timesteps. Return = 42.0\n",
      "Episode 17 finished after 15 timesteps. Return = 15.0\n",
      "Episode 18 finished after 36 timesteps. Return = 36.0\n",
      "Episode 19 finished after 64 timesteps. Return = 64.0\n",
      "Episode 20 finished after 21 timesteps. Return = 21.0\n",
      "Episode 21 finished after 16 timesteps. Return = 16.0\n",
      "Episode 22 finished after 9 timesteps. Return = 9.0\n",
      "Episode 23 finished after 49 timesteps. Return = 49.0\n",
      "Episode 24 finished after 31 timesteps. Return = 31.0\n",
      "Episode 25 finished after 11 timesteps. Return = 11.0\n",
      "Episode 26 finished after 24 timesteps. Return = 24.0\n",
      "Episode 27 finished after 13 timesteps. Return = 13.0\n",
      "Episode 28 finished after 10 timesteps. Return = 10.0\n",
      "Episode 29 finished after 13 timesteps. Return = 13.0\n",
      "Episode 30 finished after 13 timesteps. Return = 13.0\n",
      "Episode 31 finished after 14 timesteps. Return = 14.0\n",
      "Episode 32 finished after 24 timesteps. Return = 24.0\n",
      "Episode 33 finished after 20 timesteps. Return = 20.0\n",
      "Episode 34 finished after 42 timesteps. Return = 42.0\n",
      "Episode 35 finished after 40 timesteps. Return = 40.0\n",
      "Episode 36 finished after 17 timesteps. Return = 17.0\n",
      "Episode 37 finished after 26 timesteps. Return = 26.0\n",
      "Episode 38 finished after 11 timesteps. Return = 11.0\n",
      "Episode 39 finished after 42 timesteps. Return = 42.0\n",
      "Episode 40 finished after 11 timesteps. Return = 11.0\n",
      "Episode 41 finished after 14 timesteps. Return = 14.0\n",
      "Episode 42 finished after 15 timesteps. Return = 15.0\n",
      "Episode 43 finished after 10 timesteps. Return = 10.0\n",
      "Episode 44 finished after 12 timesteps. Return = 12.0\n",
      "Episode 45 finished after 17 timesteps. Return = 17.0\n",
      "Episode 46 finished after 17 timesteps. Return = 17.0\n",
      "Episode 47 finished after 14 timesteps. Return = 14.0\n",
      "Episode 48 finished after 19 timesteps. Return = 19.0\n",
      "Episode 49 finished after 26 timesteps. Return = 26.0\n",
      "Episode 50 finished after 9 timesteps. Return = 9.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6888335075134364"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer = collect_experience(random_policy, replay_buffer, replay_size, last_few, n_warm_up_episodes, log_to_tensorboard)#collect experience from warm up episodes with a random policy\n",
    "train_net(behaviour_func, replay_buffer, n_updates_per_iter, batch_size, log_to_tensorboard) #train the network with the warm up episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters=1000 #number of iterations\n",
    "for i in range(n_iters):\n",
    "    replay_buffer = collect_experience(stochastic_policy, replay_buffer, replay_size, last_few, n_episodes_per_iter, log_to_tensorboard) #collect expeirence using behaviour function policy\n",
    "    train_net(behaviour_func, replay_buffer, n_updates_per_iter, batch_size, log_to_tensorboard) #train the network with the collected experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(agent.state_dict(), 'checkpoints/lunar_lander_64x64_checkpoint_0.pt')\n",
    "#agent.load_state_dict(torch.load('checkpoints/lunar_lander_32x32_checkpoint_0.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished after 200 timesteps. Return = 200.0\n",
      "Episode 1 finished after 200 timesteps. Return = 200.0\n",
      "Episode 2 finished after 200 timesteps. Return = 200.0\n",
      "Episode 3 finished after 200 timesteps. Return = 200.0\n",
      "Episode 4 finished after 200 timesteps. Return = 200.0\n"
     ]
    }
   ],
   "source": [
    "visualise_agent(greedy_policy, command=[200, 200], n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished after 101 timesteps. Return = -131.8027246567254\n"
     ]
    }
   ],
   "source": [
    "visualise_agent(stochastic_policy, command=[150, 400], n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
