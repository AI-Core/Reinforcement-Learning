{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning\n",
    "\n",
    "### Monte-Carlo (MC) methods for computing action-value functions and improving policies\n",
    "\n",
    "\n",
    "Monte-Carlo methods are those based on repeated sampling to estimate a quantity. \n",
    "\n",
    "Similarly to what we did using DP, we can do MC based implementations of policy and value iteration by sampling experienced values of states, rather than simulating them with a model.\n",
    "\n",
    "The goal of using MC methods is to avoing the need for a model - if we don't have to look ahead from each state, then we can remove the model.\n",
    "\n",
    "However, even if we find an optimal value function, we will still need to use a model to extract an optimal policy to understand what states are reachable from others. It's no good having a chess piece next you your opponent's King if you don't know how that piece can move.\n",
    "\n",
    "The way we chose actions greedily when we had a model, was by using it to consider all possible actions and then taking the one that gave us the best expected return.\n",
    "\n",
    "As such, what would be more useful would be to use MC methods to estimate the action-value (Q) function. This tells us how good any particular action is from a certain state. If we know how good each action is, then we don't need a model - as long as we know all possible actions, we can just plug them into our q function and then take the one which for which our q function returns the largest value.\n",
    "\n",
    "### Q-functions - how good is taking a certain action in a certain state?\n",
    "![](images/q_def.jpg)\n",
    "\n",
    "![](images/q_optimality_derivation.JPG)\n",
    "\n",
    "### Computing action-value functions using Monte-Carlo methods\n",
    "\n",
    "For this method, we will use Monte-Carlo sampling to estimate the q-value of each state by running many episodes, and then doing backup from the terminal state.\n",
    "\n",
    "The q-value is the <strong>mean</strong> expected future reward following an action from a given state.\n",
    "Rather than storing all of our experience and taking the mean over them, we can use each experience to update an exponentially weighted average forget that exprience.\n",
    "\n",
    "![](./images/exp-avg.jpg)\n",
    "\n",
    "![](images/q_learning_algorithm.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from GriddyEnv import GriddyEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_agent(policy, n=5):\n",
    "    try:\n",
    "        for trial_i in range(n):\n",
    "            observation = env.reset()\n",
    "            done=False\n",
    "            t=0\n",
    "            while not done:\n",
    "                env.render()\n",
    "                policy_action = policy(observation)\n",
    "                observation, reward, done, info = env.step(policy_action)\n",
    "                time.sleep(0.5)\n",
    "                t+=1\n",
    "            env.render()\n",
    "            time.sleep(1.5)\n",
    "            print(\"Episode {} finished after {} timesteps\".format(trial_i, t))\n",
    "        env.close()\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualise_agent(random_policy, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(state, return_action_val=False):\n",
    "    action_values=[] #store the value of each action from this state\n",
    "    for test_action in range(4): #for each action\n",
    "        key = pickle.dumps(np.array((*np.copy(state).flatten(), test_action))) #calculate the key for our dictionary\n",
    "        if key not in q_table: q_table[key] = 0 #if unseen state-action pair, initialize q value to 0\n",
    "        action_values.append(q_table[key]) #append the value of this action to a list\n",
    "    policy_action = #get an action by performing argmax operation\n",
    "    if return_action_val: return policy_action, action_values[policy_action] #if flag, return value of action aswell\n",
    "    return policy_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_epsilon_greedy_policy(policy):\n",
    "    def epsilon_greedy_policy(state):\n",
    "        action = env.action_space.sample() if np.random.rand()<epsilon else policy(state) #epsilon greedy policy\n",
    "        return action\n",
    "    return epsilon_greedy_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(episode_mem, q_table, discount_factor=0.95, alpha=0.2):\n",
    "    all_diffs=[] #store difference between new and old q values\n",
    "    for i, mem in reversed(list(enumerate(episode_mem))): #iterate over the memories in reverse chronological order\n",
    "        if i==len(episode_mem)-1: #if terminal state\n",
    "            calculated_q = #set q = the reward in that memory\n",
    "        else:#if non-terminal state\n",
    "            _, next_obs_q = greedy_policy(***) #get q value of next state\n",
    "            calculated_q =  #calculate new q value estimate for this state-action pair\n",
    "        \n",
    "        key = pickle.dumps(np.array((*mem['observation'].flatten(), mem['action']))) #get key of current state-action pair\n",
    "        if key not in q_table: q_table[key]=0 #if unseen state-action pair, initialize q value to 0\n",
    "        new_val =  #update q with a step of size alpha to new q value\n",
    "        diff = #calculate difference between old and new q values estimate\n",
    "        q_table[key] = new_val\n",
    "        all_diffs.append(diff)\n",
    "    return q_table, np.mean(all_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_episode=0\n",
    "epsilon = #initialize epsilon\n",
    "q_table =  #initialize q table\n",
    "discount_factor=0.95\n",
    "alpha=0.1\n",
    "\n",
    "env = GriddyEnv()\n",
    "epsilon_greedy_policy = create_epsilon_greedy_policy(greedy_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(policy, n_episodes=100):\n",
    "    global epsilon\n",
    "    global q_table\n",
    "    global i_episode\n",
    "    try:\n",
    "        for _ in range(n_episodes):\n",
    "            observation = env.reset()\n",
    "            episode_mem = []\n",
    "            done=False\n",
    "            t=0\n",
    "            while not done:\n",
    "                action = policy(observation)\n",
    "                new_observation, reward, done, info = env.step(action)\n",
    "                episode_mem.append({'observation':observation,\n",
    "                                    'action':action,\n",
    "                                    'reward':reward,\n",
    "                                    'new_observation':new_observation,\n",
    "                                    'done':done})\n",
    "                observation=new_observation\n",
    "                t+=1\n",
    "            epsilon*= #decay epsilon\n",
    "            q_table, q_delta = #update our q table using the current episode memory\n",
    "            i_episode+=1\n",
    "            print(\"Episode {} finished after {} timesteps. Epsilon={}. Q_Delta={}\".format(i_episode, t, epsilon, q_delta))#, end='\\r')\n",
    "        env.close()\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(epsilon_greedy_policy, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished after 5 timesteps\n"
     ]
    }
   ],
   "source": [
    "visualise_agent(greedy_policy, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Learning\n",
    "\n",
    "Instead of using a table to store our q values for each state, which becomes computationally inefficient when we have a large state space, we can use a neural network. This is not a problem for NNs as we don't need to store the value for each action-state pair explicity. We also improve the performance of our agent in unseen states. This is because, in the tabular method, we assume a q value of 0 for unseen action-state pairs while our network will make a guess based on similar state-action pairs it has seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(****) #input layer\n",
    "        self.fc2 = torch.nn.Linear(32, 32)\n",
    "        self.fc3 = torch.nn.Linear(***) #output layer\n",
    "    def forward(self, obs):\n",
    "        obs = obs.view(****) #flatten input\n",
    "        x = F.relu(self.fc1(obs))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    def create_optimizer(self, lr=0.001):\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_greedy_policy(q_network):\n",
    "    def greedy_policy(state, return_action_val=False):\n",
    "        action_values = q_network(torch.tensor(state).double()).detach().numpy()\n",
    "        policy_action = np.argmax(action_values)\n",
    "        if return_action_val: return policy_action, action_values[0][policy_action]\n",
    "        return policy_action\n",
    "    return greedy_policy\n",
    "\n",
    "def create_stochastic_policy(q_network):\n",
    "    def stochastic_policy(state, return_action_val=False):\n",
    "        action_values = q_network(torch.tensor(state).double()).detach().numpy()\n",
    "        action_probs = F.softmax(torch.tensor(action_values), dim=-1)\n",
    "        policy_action = torch.distributions.Categorical(action_probs).sample().item()\n",
    "        if return_action_val: return policy_action, action_values[0][policy_action]\n",
    "        return policy_action\n",
    "    return stochastic_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_network(episode_mem, q_network, discount_factor=0.95, alpha=0.2):\n",
    "    all_diffs=[] #store difference between new and old q values\n",
    "    for i, mem in reversed(list(enumerate(episode_mem))): #iterate over the memories in reverse chronological order\n",
    "        if i==len(episode_mem)-1: #if terminal state\n",
    "            calculated_q = mem['reward'] #set q = the reward in that memory\n",
    "        else:#if non-terminal state\n",
    "            _, next_obs_q = greedy_policy(mem['new_observation'], return_action_val=True) #get q value of next state\n",
    "            calculated_q = mem['reward']+discount_factor*next_obs_q #calculate new q value estimate for this state-action pair\n",
    "            \n",
    "        predicted_old_q = #what does our network predict for the current state-value pair\n",
    "        label_new_q = #what should our label be for the network given our new prediction of q\n",
    "        cost =  #calculate cost\n",
    "        cost.backward() #calculate gradients\n",
    "        q_network.optimizer.step() #update weights\n",
    "        q_network.optimizer.zero_grad() #reset gradients\n",
    "        all_diffs.append(abs(calculated_new_q-predicted_old_q.item()))\n",
    "    return np.mean(all_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPER-PARAMS\n",
    "epsilon = 1\n",
    "i_episode=0\n",
    "discount_factor=0.95\n",
    "alpha=0.1\n",
    "lr = 0.001\n",
    "\n",
    "env = GriddyEnv(4, 4)\n",
    "q_network = QNetwork().double()\n",
    "q_network.create_optimizer(lr)\n",
    "greedy_policy = create_greedy_policy(q_network)\n",
    "stochastic_policy = create_stochastic_policy(q_network)\n",
    "epsilon_greedy_policy = create_epsilon_greedy_policy(greedy_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deep(policy, n_episodes=100):\n",
    "    global epsilon\n",
    "    global q_network\n",
    "    global i_episode\n",
    "    try:\n",
    "        for _ in range(n_episodes):\n",
    "            observation = env.reset()\n",
    "            episode_mem = []\n",
    "            done=False\n",
    "            t=0\n",
    "            while not done:\n",
    "                action = policy(observation)\n",
    "                new_observation, reward, done, info = env.step(action)\n",
    "                episode_mem.append({'observation':observation,\n",
    "                                    'action':action,\n",
    "                                    'reward':reward,\n",
    "                                    'new_observation':new_observation,\n",
    "                                    'done':done})\n",
    "                observation=new_observation\n",
    "                t+=1\n",
    "            epsilon*=0.995\n",
    "            q_delta = #update our q network using the current episode memory\n",
    "            i_episode+=1\n",
    "            print(\"Episode {} finished after {} timesteps. Epsilon={}. Q_Delta={}\".format(i_episode, t, epsilon, q_delta))#, end='\\r')\n",
    "            #print(value_table_viz(value_table))\n",
    "            #print()\n",
    "            #env.render(value_table_viz(value_network, observation))\n",
    "        env.close()\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_deep(epsilon_greedy_policy, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished after 3 timesteps\n",
      "Episode 1 finished after 1 timesteps\n",
      "Episode 2 finished after 4 timesteps\n"
     ]
    }
   ],
   "source": [
    "visualise_agent(greedy_policy, n=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
